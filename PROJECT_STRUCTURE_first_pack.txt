–ü–û–õ–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê
–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: D:\Edu\CTCF
====================================================================================================

–î–†–ï–í–û–í–ò–î–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê:
--------------------------------------------------
üìÅ CTCF/
‚îú‚îÄ‚îÄ üìÅ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ IXI/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ datasets.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ train_TransMorph_Dwin_bspl.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ train_UTSRMorph_ixi.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ OASIS/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ datasets.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ inference.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ plot_bars.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ train_CTCF.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ train_TM-DCA.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ train_UTSRMorph.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ __init__.py
‚îú‚îÄ‚îÄ üìÅ models/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ CTCF/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ configs.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ lambda_net.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ model.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ TransMorph_DCA/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ configs.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ model.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ UTSRMorph/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ configs.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ model.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ __init__.py
‚îú‚îÄ‚îÄ üìÅ utils/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ surface_distance/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ lookup_tables.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ core.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ctcf_losses.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ data.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dice.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ field.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ losses.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ misc.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ rand.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ spatial.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ train.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ trans.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ validation.py
‚îú‚îÄ‚îÄ üìÑ .gitignore
‚îú‚îÄ‚îÄ üìÑ environment.yml
‚îú‚îÄ‚îÄ üìÑ environment_clear.yml
‚îú‚îÄ‚îÄ üìÑ guide.md
‚îú‚îÄ‚îÄ üìÑ Pasenko_CN_2_–°—Ç–∞—Ç—å—è.doc
‚îú‚îÄ‚îÄ üìÑ Pasenko_CN_2_–°—Ç–∞—Ç—å—è.pdf
‚îî‚îÄ‚îÄ üìÑ README.md

====================================================================================================

–°–û–î–ï–†–ñ–ò–ú–û–ï –§–ê–ô–õ–û–í:
====================================================================================================


üìÑ experiments/IXI/__init__.py
--------------------------------------------------

==================================================

üìÑ experiments/IXI/datasets.py
--------------------------------------------------
import os, glob
import torch, sys
from torch.utils.data import Dataset
from .data_utils import pkload
import matplotlib.pyplot as plt

import numpy as np


class IXIBrainDataset(Dataset):
    def __init__(self, data_path, atlas_path, transforms):
        self.paths = data_path
        self.atlas_path = atlas_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        x, x_seg = pkload(self.atlas_path)
        y, y_seg = pkload(path)
        #print(x.shape)
        #print(x.shape)
        #print(np.unique(y))
        # print(x.shape, y.shape)#(240, 240, 155) (240, 240, 155)
        # transforms work with nhwtc
        x, y = x[None, ...], y[None, ...]
        # print(x.shape, y.shape)#(1, 240, 240, 155) (1, 240, 240, 155)
        x,y = self.transforms([x, y])
        #y = self.one_hot(y, 2)
        #print(y.shape)
        #sys.exit(0)
        x = np.ascontiguousarray(x)# [Bsize,channelsHeight,,Width,Depth]
        y = np.ascontiguousarray(y)
        #plt.figure()
        #plt.subplot(1, 2, 1)
        #plt.imshow(x[0, :, :, 8], cmap='gray')
        #plt.subplot(1, 2, 2)
        #plt.imshow(y[0, :, :, 8], cmap='gray')
        #plt.show()
        #sys.exit(0)
        #y = np.squeeze(y, axis=0)
        x, y = torch.from_numpy(x), torch.from_numpy(y)
        return x, y

    def __len__(self):
        return len(self.paths)


class IXIBrainInferDataset(Dataset):
    def __init__(self, data_path, atlas_path, transforms):
        self.atlas_path = atlas_path
        self.paths = data_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        x, x_seg = pkload(self.atlas_path)
        y, y_seg = pkload(path)
        x, y = x[None, ...], y[None, ...]
        x_seg, y_seg= x_seg[None, ...], y_seg[None, ...]
        x, x_seg = self.transforms([x, x_seg])
        y, y_seg = self.transforms([y, y_seg])
        x = np.ascontiguousarray(x)# [Bsize,channelsHeight,,Width,Depth]
        y = np.ascontiguousarray(y)
        x_seg = np.ascontiguousarray(x_seg)  # [Bsize,channelsHeight,,Width,Depth]
        y_seg = np.ascontiguousarray(y_seg)
        x, y, x_seg, y_seg = torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(x_seg), torch.from_numpy(y_seg)
        return x, y, x_seg, y_seg

    def __len__(self):
        return len(self.paths)
==================================================

üìÑ experiments/IXI/train_TransMorph_Dwin_bspl.py
--------------------------------------------------
from torch.utils.tensorboard import SummaryWriter
import os, utils, glob, losses
import sys
from torch.utils.data import DataLoader
from data import datasets, trans
import numpy as np
import torch
from torchvision import transforms
from torch import optim
import torch.nn as nn
import matplotlib.pyplot as plt
from natsort import natsorted
import models.transformation as transformation
from models.TransMorph_DWin_bspl import CONFIGS as CONFIGS_TM
import models.TransMorph_DWin_bspl as TransMorph_bspl

class Logger(object):
    def __init__(self, save_dir):
        self.terminal = sys.stdout
        self.log = open(save_dir+"logfile.log", "a")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        pass

def main():
    batch_size = 1
    atlas_dir = 'C:/Junyu_Files/IXI/atlas.pkl'
    train_dir = 'C:/Junyu_Files/IXI/Train/'
    val_dir = 'C:/Junyu_Files/IXI/Val/'
    weights = [1, 1]
    Dwin = [7, 5, 3]
    save_dir = 'TransMorphBSplineDWinFull_ncc_{}_diffusion_{}/'.format(weights[0], weights[1])
    if not os.path.exists('experiments/'+save_dir):
        os.makedirs('experiments/'+save_dir)
    if not os.path.exists('logs/' + save_dir):
        os.makedirs('logs/' + save_dir)
    sys.stdout = Logger('logs/' + save_dir)
    lr = 0.0001
    epoch_start = 0
    max_epoch = 500
    cont_training = False
    '''
    Initialize model
    '''
    H, W, D = 160, 192, 224
    config = CONFIGS_TM['TransMorphBSpline']
    config.img_size = (H, W, D)
    config.out_size = (H, W, D)
    config.dwin_kernel_size = (Dwin[0], Dwin[1], Dwin[2])
    config.window_size = (H // 32, W // 32, D // 32)
    model = TransMorph_bspl.TranMorphBSplineNet(config)
    model.cuda()

    '''
    If continue from previous training
    '''
    if cont_training:
        epoch_start = 335
        model_dir = 'experiments/'+save_dir
        updated_lr = round(lr * np.power(1 - (epoch_start) / max_epoch,0.9),8)
        best_model = torch.load(model_dir + natsorted(os.listdir(model_dir))[0])['state_dict']
        model.load_state_dict(best_model)
    else:
        updated_lr = lr

    '''
    Initialize training
    '''
    train_composed = transforms.Compose([trans.RandomFlip(0),
                                         trans.NumpyType((np.float32, np.float32)),
                                         ])

    val_composed = transforms.Compose([trans.Seg_norm(), #rearrange segmentation label to 1 to 46
                                       trans.NumpyType((np.float32, np.int16)),
                                        ])

    train_set = datasets.IXIBrainDataset(glob.glob(train_dir + '*.pkl'), atlas_dir, transforms=train_composed)
    val_set = datasets.IXIBrainInferDataset(glob.glob(val_dir + '*.pkl'), atlas_dir, transforms=val_composed)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)

    optimizer = optim.AdamW(model.parameters(), lr=updated_lr, weight_decay=0, amsgrad=True)
    criterion = losses.NCC(9)
    scaler = torch.cuda.amp.GradScaler()
    criterions = [criterion]
    criterions += [losses.Grad3d(penalty='l2')]
    best_dsc = 0
    writer = SummaryWriter(log_dir='logs/'+save_dir)
    for epoch in range(epoch_start, max_epoch):
        print('Training Starts')
        '''
        Training
        '''
        loss_all = utils.AverageMeter()
        idx = 0
        for data in train_loader:
            idx += 1
            model.train()
            adjust_learning_rate(optimizer, epoch, max_epoch, lr)
            with torch.no_grad():
                data = [t.cuda() for t in data]
                x = data[0].half()
                y = data[1].half()
            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                output = model((x,y))
                loss = 0
                loss_vals = []
                _ = 0
                for n, loss_function in enumerate(criterions):
                    if _>1: break
                    curr_loss = loss_function(output[n], y) * weights[n]
                    loss_vals.append(curr_loss)
                    loss += curr_loss
                    _ += 1
                loss_all.update(loss.item(), y.numel())
            # compute gradient and do SGD step
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            print('Iter {} of {} loss {:.4f}, Img Sim: {:.6f}, Reg: {:.6f}'.format(idx, len(train_loader), loss.item(), loss_vals[0].item(), loss_vals[1].item()))

        writer.add_scalar('Loss/train', loss_all.avg, epoch)
        print('Epoch {} loss {:.4f}'.format(epoch, loss_all.avg))
        '''
        Validation
        '''
        eval_dsc = utils.AverageMeter()
        with torch.no_grad():
            for data in val_loader:
                model.eval()
                data = [t.cuda() for t in data]
                x = data[0]
                y = data[1]
                x_seg = data[2]
                y_seg = data[3]
                grid_img = mk_grid_img(8, 1, (H, W, D))
                output = model((x, y))
                with torch.cuda.device(GPU_iden):
                    def_out = transformation.warp(x_seg.cuda().float(), output[2].cuda(), interp_mode='nearest')
                    def_grid = transformation.warp(grid_img.float(), output[2].cuda(), interp_mode='bilinear')
                dsc = utils.dice_val_VOI(def_out.long(), y_seg.long())
                eval_dsc.update(dsc.item(), x.size(0))
                print(eval_dsc.avg)
        best_dsc = max(eval_dsc.avg, best_dsc)
        save_checkpoint({
            'epoch': epoch + 1,
            'state_dict': model.state_dict(),
            'best_dsc': best_dsc,
            'optimizer': optimizer.state_dict(),
        }, save_dir='experiments/'+save_dir, filename='dsc{:.3f}.pth.tar'.format(eval_dsc.avg))
        writer.add_scalar('DSC/validate', eval_dsc.avg, epoch)
        plt.switch_backend('agg')
        pred_fig = comput_fig(def_out)
        grid_fig = comput_fig(def_grid)
        x_fig = comput_fig(x_seg)
        tar_fig = comput_fig(y_seg)
        writer.add_figure('Grid', grid_fig, epoch)
        plt.close(grid_fig)
        writer.add_figure('input', x_fig, epoch)
        plt.close(x_fig)
        writer.add_figure('ground truth', tar_fig, epoch)
        plt.close(tar_fig)
        writer.add_figure('prediction', pred_fig, epoch)
        plt.close(pred_fig)
        loss_all.reset()
    writer.close()

def comput_fig(img):
    img = img.detach().cpu().numpy()[0, 0, 48:64, :, :]
    fig = plt.figure(figsize=(12,12), dpi=180)
    for i in range(img.shape[0]):
        plt.subplot(4, 4, i + 1)
        plt.axis('off')
        plt.imshow(img[i, :, :], cmap='gray')
    fig.subplots_adjust(wspace=0, hspace=0)
    return fig

def adjust_learning_rate(optimizer, epoch, MAX_EPOCHES, INIT_LR, power=0.9):
    for param_group in optimizer.param_groups:
        param_group['lr'] = round(INIT_LR * np.power( 1 - (epoch) / MAX_EPOCHES ,power),8)

def mk_grid_img(grid_step, line_thickness=1, grid_sz=(160, 192, 224)):
    grid_img = np.zeros(grid_sz)
    for j in range(0, grid_img.shape[1], grid_step):
        grid_img[:, j+line_thickness-1, :] = 1
    for i in range(0, grid_img.shape[2], grid_step):
        grid_img[:, :, i+line_thickness-1] = 1
    grid_img = grid_img[None, None, ...]
    grid_img = torch.from_numpy(grid_img).cuda()
    return grid_img

def save_checkpoint(state, save_dir='models', filename='checkpoint.pth.tar', max_model_num=4):
    torch.save(state, save_dir+filename)
    model_lists = natsorted(glob.glob(save_dir + '*'))
    while len(model_lists) > max_model_num:
        os.remove(model_lists[0])
        model_lists = natsorted(glob.glob(save_dir + '*'))

if __name__ == '__main__':
    '''
    GPU configuration
    '''
    GPU_iden = 0
    GPU_num = torch.cuda.device_count()
    print('Number of GPU: ' + str(GPU_num))
    for GPU_idx in range(GPU_num):
        GPU_name = torch.cuda.get_device_name(GPU_idx)
        print('     GPU #' + str(GPU_idx) + ': ' + GPU_name)
    torch.cuda.set_device(GPU_iden)
    GPU_avai = torch.cuda.is_available()
    print('Currently using: ' + torch.cuda.get_device_name(GPU_iden))
    print('If the GPU is available? ' + str(GPU_avai))
    main()
==================================================

üìÑ experiments/IXI/train_UTSRMorph_ixi.py
--------------------------------------------------
import datetime
import time

from torch.utils.tensorboard import SummaryWriter
import os, utils_ixi, glob, losses
import sys
from torch.utils.data import DataLoader
from data import datasets, trans
import numpy as np
import torch
from torchvision import transforms
from torch import optim
import torch.nn as nn
import matplotlib.pyplot as plt
from natsort import natsorted
from models_ixi.UTSRMorph import CONFIGS as CONFIGS_UM
import models_ixi.UTSRMorph as UTSRMorph

class Logger(object):
    def __init__(self, save_dir):
        self.terminal = sys.stdout
        self.log = open(save_dir+"logfile.log", "a")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        pass

def main():
    batch_size = 1
    atlas_dir = '/home/buaaa302/pythoncodes/TransFrame/IXI_data/atlas.pkl'
    train_dir = '/home/buaaa302/pythoncodes/TransFrame/IXI_data/Train/'
    val_dir = '/home/buaaa302/pythoncodes/TransFrame/IXI_data/Val/'
    weights = [1, 4] # loss weights
    save_dir = 'UTSRMorph_ncc_{}_diffusion_{}/'.format(weights[0], weights[1])
    if not os.path.exists('experiments/'+save_dir):
        os.makedirs('experiments/'+save_dir)
    if not os.path.exists('logs/'+save_dir):
        os.makedirs('logs/'+save_dir)
    sys.stdout = Logger('logs/'+save_dir)
    lr = 0.0004 # learning rate
    epoch_start = 0
    max_epoch = 500 #max traning epoch
    cont_training = False #if continue training

    '''
    Initialize model
    '''
    config = CONFIGS_UM['UTSRMorph-Large']
    model = UTSRMorph.UTSRMorph(config)
    model.cuda()

    '''
    Initialize spatial transformation function
    '''
    reg_model = utils_ixi.register_model(config.img_size, 'nearest')
    reg_model.cuda()
    reg_model_bilin = utils_ixi.register_model(config.img_size, 'bilinear')
    reg_model_bilin.cuda()

    '''
    If continue from previous training
    '''
    if cont_training:
        epoch_start = 201
        model_dir = 'experiments/'+save_dir
        updated_lr = round(lr * np.power(1 - (epoch_start) / max_epoch,0.9),8)
        best_model = torch.load(model_dir + natsorted(os.listdir(model_dir))[-1])['state_dict']
        print('Model: {} loaded!'.format(natsorted(os.listdir(model_dir))[-1]))
        model.load_state_dict(best_model)
    else:
        updated_lr = lr

    '''
    Initialize training
    '''
    train_composed = transforms.Compose([trans.RandomFlip(0),
                                         trans.NumpyType((np.float32, np.float32)),
                                         ])

    val_composed = transforms.Compose([trans.Seg_norm(), #rearrange segmentation label to 1 to 46
                                       trans.NumpyType((np.float32, np.int16))])
    train_set = datasets.IXIBrainDataset(glob.glob(train_dir + '*.pkl'), atlas_dir, transforms=train_composed)
    val_set = datasets.IXIBrainInferDataset(glob.glob(val_dir + '*.pkl'), atlas_dir, transforms=val_composed)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)

    optimizer = optim.Adam(model.parameters(), lr=updated_lr, weight_decay=0, amsgrad=True)
    criterion = losses.NCC_vxm()
    criterions = [criterion]
    criterions += [losses.Grad3d(penalty='l2')]
    best_dsc = 0
    writer = SummaryWriter(log_dir='logs/'+save_dir)
    for epoch in range(epoch_start, max_epoch):
        print('Training Starts')
        '''
        Training
        '''
        loss_all = utils_ixi.AverageMeter()
        idx = 0
        time_start = time.time()
        for data in train_loader:
            idx += 1
            model.train()
            adjust_learning_rate(optimizer, epoch, max_epoch, lr)
            data = [t.cuda() for t in data]
            x = data[0]
            y = data[1]
            x_in = torch.cat((x,y), dim=1)
            output = model(x_in)
            loss = 0
            loss_vals = []
            for n, loss_function in enumerate(criterions):
                curr_loss = loss_function(output[n], y) * weights[n]
                loss_vals.append(curr_loss)
                loss += curr_loss
            loss_all.update(loss.item(), y.numel())
            # compute gradient and do SGD step
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            print('Iter {} of {} loss {:.4f}, Img Sim: {:.6f}, Reg: {:.6f}'.format(idx, len(train_loader), loss.item(), loss_vals[0].item(), loss_vals[1].item()))
        writer.add_scalar('Loss/train', loss_all.avg, epoch)
        print('Epoch {} loss {:.4f}'.format(epoch, loss_all.avg))
        '''
        Validation
        '''
        eval_dsc = utils_ixi.AverageMeter()
        with torch.no_grad():
            for data in val_loader:
                model.eval()
                data = [t.cuda() for t in data]
                x = data[0]
                y = data[1]
                x_seg = data[2]
                y_seg = data[3]
                x_in = torch.cat((x, y), dim=1)
                grid_img = mk_grid_img(8, 1, config.img_size)
                output = model(x_in)
                def_out = reg_model([x_seg.cuda().float(), output[1].cuda()])
                def_grid = reg_model_bilin([grid_img.float(), output[1].cuda()])
                dsc = utils_ixi.dice_val_VOI(def_out.long(), y_seg.long())
                eval_dsc.update(dsc.item(), x.size(0))
                print(eval_dsc.avg)
        time_end = time.time()
        alltime = (time_end-time_start)*(499-epoch)
        timeresult=str(datetime.timedelta(seconds=alltime))
        print("time:"+timeresult)
        best_dsc = max(eval_dsc.avg, best_dsc)
        save_checkpoint({
            'epoch': epoch + 1,
            'state_dict': model.state_dict(),
            'best_dsc': best_dsc,
            'optimizer': optimizer.state_dict(),
        }, save_dir='experiments/'+save_dir, filename='dsc{:.3f}.pth.tar'.format(eval_dsc.avg))
        writer.add_scalar('DSC/validate', eval_dsc.avg, epoch)
        plt.switch_backend('agg')
        pred_fig = comput_fig(def_out)
        grid_fig = comput_fig(def_grid)
        x_fig = comput_fig(x_seg)
        tar_fig = comput_fig(y_seg)
        writer.add_figure('Grid', grid_fig, epoch)
        plt.close(grid_fig)
        writer.add_figure('input', x_fig, epoch)
        plt.close(x_fig)
        writer.add_figure('ground truth', tar_fig, epoch)
        plt.close(tar_fig)
        writer.add_figure('prediction', pred_fig, epoch)
        plt.close(pred_fig)
        loss_all.reset()
    writer.close()

def comput_fig(img):
    img = img.detach().cpu().numpy()[0, 0, 48:64, :, :]
    fig = plt.figure(figsize=(12,12), dpi=180)
    for i in range(img.shape[0]):
        plt.subplot(4, 4, i + 1)
        plt.axis('off')
        plt.imshow(img[i, :, :], cmap='gray')
    fig.subplots_adjust(wspace=0, hspace=0)
    return fig

def adjust_learning_rate(optimizer, epoch, MAX_EPOCHES, INIT_LR, power=0.9):
    for param_group in optimizer.param_groups:
        param_group['lr'] = round(INIT_LR * np.power( 1 - (epoch) / MAX_EPOCHES ,power),8)

def mk_grid_img(grid_step, line_thickness=1, grid_sz=(160, 192, 224)):
    grid_img = np.zeros(grid_sz)
    for j in range(0, grid_img.shape[1], grid_step):
        grid_img[:, j+line_thickness-1, :] = 1
    for i in range(0, grid_img.shape[2], grid_step):
        grid_img[:, :, i+line_thickness-1] = 1
    grid_img = grid_img[None, None, ...]
    grid_img = torch.from_numpy(grid_img).cuda()
    return grid_img

def save_checkpoint(state, save_dir='models_1', filename='checkpoint.pth.tar', max_model_num=8):
    torch.save(state, save_dir+filename)
    model_lists = natsorted(glob.glob(save_dir + '*'))
    while len(model_lists) > max_model_num:
        os.remove(model_lists[0])
        model_lists = natsorted(glob.glob(save_dir + '*'))

if __name__ == '__main__':
    '''
    GPU configuration
    '''
    GPU_iden = 0
    GPU_num = torch.cuda.device_count()
    print('Number of GPU: ' + str(GPU_num))
    for GPU_idx in range(GPU_num):
        GPU_name = torch.cuda.get_device_name(GPU_idx)
        print('     GPU #' + str(GPU_idx) + ': ' + GPU_name)
    torch.cuda.set_device(GPU_iden)
    GPU_avai = torch.cuda.is_available()
    print('Currently using: ' + torch.cuda.get_device_name(GPU_iden))
    print('If the GPU is available? ' + str(GPU_avai))
    main()

==================================================

üìÑ experiments/OASIS/__init__.py
--------------------------------------------------

==================================================

üìÑ experiments/OASIS/datasets.py
--------------------------------------------------
import torch
from torch.utils.data import Dataset
import random
import numpy as np

from utils import pkload


class OASISBrainDataset(Dataset):
    def __init__(self, data_path, transforms):
        self.paths = data_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        tar_list = self.paths.copy()
        tar_list.remove(path)
        random.shuffle(tar_list)
        tar_file = tar_list[0]
        x, x_seg = pkload(path)
        y, y_seg = pkload(tar_file)
        x, y = x[None, ...], y[None, ...]
        x_seg, y_seg = x_seg[None, ...], y_seg[None, ...]
        x, x_seg = self.transforms([x, x_seg])
        y, y_seg = self.transforms([y, y_seg])
        x = np.ascontiguousarray(x)
        y = np.ascontiguousarray(y)
        x_seg = np.ascontiguousarray(x_seg)
        y_seg = np.ascontiguousarray(y_seg)
        x, y, x_seg, y_seg = torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(x_seg), torch.from_numpy(y_seg)

        # images must be float
        x = x.float()
        y = y.float()

        # labels must be Long for one_hot and for any index-based ops
        x_seg = x_seg.long()
        y_seg = y_seg.long()

        return x, y, x_seg, y_seg

    def __len__(self):
        return len(self.paths)


class OASISBrainInferDataset(Dataset):
    def __init__(self, data_path, transforms):
        self.paths = data_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        x, y, x_seg, y_seg = pkload(path)
        x, y = x[None, ...], y[None, ...]
        x_seg, y_seg= x_seg[None, ...], y_seg[None, ...]
        x, x_seg = self.transforms([x, x_seg])
        y, y_seg = self.transforms([y, y_seg])
        x = np.ascontiguousarray(x)
        y = np.ascontiguousarray(y)
        x_seg = np.ascontiguousarray(x_seg)
        y_seg = np.ascontiguousarray(y_seg)
        x, y, x_seg, y_seg = torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(x_seg), torch.from_numpy(y_seg)

        # images must be float
        x = x.float()
        y = y.float()   

        # labels must be Long for one_hot and for any index-based ops
        x_seg = x_seg.long()
        y_seg = y_seg.long()

        return x, y, x_seg, y_seg

    def __len__(self):
        return len(self.paths)
==================================================

üìÑ experiments/OASIS/inference.py
--------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Unified inference for OASIS Task-3 for 3 models in this repo:
  - TM-DCA    (models.TransMorph_DCA.model)
  - UTSRMorph (models.UTSRMorph.model)
  - CTCF      (models.CTCF.model)

Outputs (per run):
  out_dir/
    per_case.csv
    summary.json
    summary.csv
    png/   (optional visuals, NOW includes deformed grid)
    flows/ (optional displacement fields)

IMPORTANT FIX:
  - Previously def_grid was computed but never drawn into the PNG figure.
    Now it is rendered in an extra row ("Deformed grid").
"""

from __future__ import annotations

import os
import glob
import json
import time
import csv
import math
import argparse
from dataclasses import dataclass
from typing import Dict, Tuple, Optional

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms

from experiments.OASIS import datasets

from utils import (
    NumpyType,
    register_model,
    dice_val_VOI,
    jacobian_det,
    setup_device,
)

def make_grid_img(vol_shape, step=8, thickness=1, device="cuda"):
    D, H, W = vol_shape
    g = torch.zeros((1, 1, D, H, W), device=device, dtype=torch.float32)
    g[:, :, ::step, :, :] = 1
    g[:, :, :, ::step, :] = 1
    g[:, :, :, :, ::step] = 1
    if thickness > 1:
        g = F.max_pool3d(g, kernel_size=thickness, stride=1, padding=thickness//2)
    return g

# ------------------------------ Helpers ------------------------------ #

def ensure_dir(path: str) -> str:
    os.makedirs(path, exist_ok=True)
    return path


def resolve_checkpoint(path_or_dir: str, prefer_best: bool = True) -> str:
    if os.path.isfile(path_or_dir):
        return path_or_dir
    if not os.path.isdir(path_or_dir):
        raise FileNotFoundError(f"Checkpoint path not found: {path_or_dir}")

    exts = (".pth", ".pt", ".pth.tar")
    files = [os.path.join(path_or_dir, f) for f in os.listdir(path_or_dir) if f.endswith(exts)]
    if not files:
        raise RuntimeError(f"No checkpoint files found in directory: {path_or_dir}")

    if prefer_best:
        best = [f for f in files if "best" in os.path.basename(f).lower()]
        if best:
            best.sort()
            return best[-1]

    files.sort(key=lambda p: os.path.getmtime(p))
    return files[-1]


def load_state_dict(model: torch.nn.Module, ckpt_path: str) -> Dict:
    ckpt = torch.load(ckpt_path, map_location="cpu")
    if isinstance(ckpt, dict):
        if "state_dict" in ckpt:
            sd = ckpt["state_dict"]
        elif "model" in ckpt:
            sd = ckpt["model"]
        else:
            sd = ckpt
    else:
        sd = ckpt

    missing, unexpected = model.load_state_dict(sd, strict=False)
    if missing:
        print(f"[WARN] Missing keys: {len(missing)} (up to 10): {missing[:10]}")
    if unexpected:
        print(f"[WARN] Unexpected keys: {len(unexpected)} (up to 10): {unexpected[:10]}")
    return ckpt if isinstance(ckpt, dict) else {"state_dict": sd}


def case_id_from_path(pkl_path: str) -> str:
    base = os.path.basename(pkl_path)
    stem = os.path.splitext(base)[0]
    return stem[2:] if stem.startswith("p_") else stem


def dice_per_label_1to35(def_seg: torch.Tensor, y_seg: torch.Tensor) -> np.ndarray:
    pred = def_seg.detach().cpu().numpy()[0, 0]
    true = y_seg.detach().cpu().numpy()[0, 0]
    out = np.zeros((35,), dtype=np.float64)
    for i, lbl in enumerate(range(1, 36)):
        p = (pred == lbl)
        t = (true == lbl)
        inter = np.sum(p & t)
        union = np.sum(p) + np.sum(t)
        out[i] = (2.0 * inter) / (union + 1e-5)
    return out


def fold_percent_from_flow(flow: torch.Tensor) -> float:
    detJ = jacobian_det(flow.float())  # [B,1,D,H,W]
    return float((detJ <= 0.0).float().mean().item() * 100.0)


def logdet_std_from_flow(flow: torch.Tensor) -> float:
    detJ = jacobian_det(flow.float())
    detJ = torch.clamp(detJ, min=1e-9, max=1e9)
    logdet = torch.log(detJ)
    return float(torch.std(logdet).item())


def compute_ci95(mean: float, std: float, n: int) -> float:
    if n <= 1:
        return 0.0
    sem = std / math.sqrt(n)
    return 1.96 * sem


def require_surface_distance():
    try:
        from surface_distance import compute_robust_hausdorff, compute_surface_distances  # noqa
    except Exception as e:
        raise RuntimeError(
            "HD95 requested but 'surface-distance' is not installed.\n"
            "Install in your env:\n"
            "  pip install surface-distance\n"
        ) from e


def hd95_mean_1to35(def_seg: torch.Tensor, y_seg: torch.Tensor, spacing=(1.0, 1.0, 1.0)) -> float:
    require_surface_distance()
    from surface_distance import compute_robust_hausdorff, compute_surface_distances

    pred = def_seg.detach().cpu().numpy()[0, 0]
    true = y_seg.detach().cpu().numpy()[0, 0]

    hds = []
    for lbl in range(1, 36):
        p = (pred == lbl)
        t = (true == lbl)
        if p.sum() == 0 or t.sum() == 0:
            hds.append(0.0)
        else:
            sd = compute_surface_distances(t, p, spacing)
            hd = compute_robust_hausdorff(sd, 95.0)
            hds.append(float(hd))
    return float(np.mean(hds))


def save_flow_npz(flow: torch.Tensor, path: str):
    arr = flow.detach().cpu().numpy().astype(np.float16)
    np.savez_compressed(path, flow=arr)


def save_png_triplet(
    out_png: str,
    x: torch.Tensor,
    y: torch.Tensor,
    x_seg: torch.Tensor,
    y_seg: torch.Tensor,
    def_seg: torch.Tensor,
    def_grid: Optional[torch.Tensor] = None,
):
    """
    Paper-useful visualization:
      - fixed (3 orthogonal)
      - moving (3 orthogonal)
      - fixed seg (3 orthogonal)
      - warped seg (3 orthogonal)
      - deformed grid (3 orthogonal)  <-- FIXED: now actually rendered
    """
    import matplotlib.pyplot as plt

    x = x.detach().cpu().numpy()[0, 0]
    y = y.detach().cpu().numpy()[0, 0]
    ys = y_seg.detach().cpu().numpy()[0, 0]
    ds = def_seg.detach().cpu().numpy()[0, 0]

    dg = None
    if def_grid is not None:
        # usually [B,1,D,H,W]
        dg = def_grid.detach().cpu().numpy()[0, 0]

    D, H, W = y.shape
    cz, cy, cx = D // 2, H // 2, W // 2

    def slices(vol):
        return (vol[cz, :, :], vol[:, cy, :], vol[:, :, cx])

    y_ax, y_cor, y_sag = slices(y)
    x_ax, x_cor, x_sag = slices(x)
    ys_ax, ys_cor, ys_sag = slices(ys)
    ds_ax, ds_cor, ds_sag = slices(ds)

    n_rows = 5 if dg is not None else 4
    fig = plt.figure(figsize=(12, 12 if dg is not None else 10))
    axs = []

    # Row 1: fixed
    axs.append(fig.add_subplot(n_rows, 3, 1)); axs[-1].imshow(y_ax, cmap="gray"); axs[-1].set_title("Fixed (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 2)); axs[-1].imshow(y_cor, cmap="gray"); axs[-1].set_title("Fixed (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 3)); axs[-1].imshow(y_sag, cmap="gray"); axs[-1].set_title("Fixed (sag)")

    # Row 2: moving
    axs.append(fig.add_subplot(n_rows, 3, 4)); axs[-1].imshow(x_ax, cmap="gray"); axs[-1].set_title("Moving (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 5)); axs[-1].imshow(x_cor, cmap="gray"); axs[-1].set_title("Moving (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 6)); axs[-1].imshow(x_sag, cmap="gray"); axs[-1].set_title("Moving (sag)")

    # Row 3: fixed seg
    axs.append(fig.add_subplot(n_rows, 3, 7)); axs[-1].imshow(ys_ax); axs[-1].set_title("Fixed seg (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 8)); axs[-1].imshow(ys_cor); axs[-1].set_title("Fixed seg (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 9)); axs[-1].imshow(ys_sag); axs[-1].set_title("Fixed seg (sag)")

    # Row 4: warped seg
    axs.append(fig.add_subplot(n_rows, 3, 10)); axs[-1].imshow(ds_ax); axs[-1].set_title("Warped seg (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 11)); axs[-1].imshow(ds_cor); axs[-1].set_title("Warped seg (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 12)); axs[-1].imshow(ds_sag); axs[-1].set_title("Warped seg (sag)")

    # Row 5: deformed grid
    if dg is not None:
        dg_ax, dg_cor, dg_sag = slices(dg)
        axs.append(fig.add_subplot(n_rows, 3, 13)); axs[-1].imshow(dg_ax, cmap="gray"); axs[-1].set_title("Deformed grid (ax)")
        axs.append(fig.add_subplot(n_rows, 3, 14)); axs[-1].imshow(dg_cor, cmap="gray"); axs[-1].set_title("Deformed grid (cor)")
        axs.append(fig.add_subplot(n_rows, 3, 15)); axs[-1].imshow(dg_sag, cmap="gray"); axs[-1].set_title("Deformed grid (sag)")

    for a in axs:
        a.axis("off")

    fig.tight_layout()
    ensure_dir(os.path.dirname(out_png))
    fig.savefig(out_png, dpi=200)
    plt.close(fig)


# ------------------------------ Model builders + forward adapters ------------------------------ #

@dataclass
class ModelBundle:
    name: str
    model: torch.nn.Module
    forward_flow_fn: callable


def build_tm_dca(time_steps: int, vol_size=(160, 192, 224), dwin=(7, 5, 3)) -> ModelBundle:
    from models.TransMorph_DCA.model import CONFIGS as CONFIGS_TM
    import models.TransMorph_DCA.model as TransMorph

    D, H, W = vol_size
    half_size = (D // 2, H // 2, W // 2)

    config = CONFIGS_TM["TransMorph-3-LVL"]
    config.img_size = half_size
    config.dwin_kernel_size = tuple(dwin)
    config.window_size = (D // 32, H // 32, W // 32)

    model = TransMorph.TransMorphCascadeAd(config, time_steps)

    @torch.no_grad()
    def forward_flow(model_, x, y):
        x_half = F.avg_pool3d(x, 2)
        y_half = F.avg_pool3d(y, 2)
        use_amp = torch.cuda.is_available()
        with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
            flow_half = model_((x_half, y_half))
        flow_full = F.interpolate(flow_half.float(), scale_factor=2, mode="trilinear", align_corners=False) * 2.0
        return flow_full

    return ModelBundle("tm-dca", model, forward_flow)


def build_ctcf(time_steps: int, vol_size=(160, 192, 224), dwin=(7, 5, 3), config_key="CTCF-DCA-SR") -> ModelBundle:
    from models.CTCF.model import CONFIGS as CONFIGS_CTCF
    import models.CTCF.model as CTCF

    D, H, W = vol_size
    half_size = (D // 2, H // 2, W // 2)

    if config_key not in CONFIGS_CTCF:
        raise KeyError(f"Unknown CTCF config '{config_key}'. Available: {list(CONFIGS_CTCF.keys())}")

    config = CONFIGS_CTCF[config_key]
    config.img_size = half_size
    if hasattr(config, "dwin_kernel_size"):
        config.dwin_kernel_size = tuple(dwin)
    if hasattr(config, "window_size"):
        config.window_size = (D // 32, H // 32, W // 32)

    if hasattr(CTCF, "CTCF_DCA_SR"):
        model = CTCF.CTCF_DCA_SR(config, time_steps)
    else:
        raise AttributeError("models.CTCF.model does not expose CTCF_DCA_SR; check your model module.")

    @torch.no_grad()
    def forward_flow(model_, x, y):
        x_half = F.avg_pool3d(x, 2)
        y_half = F.avg_pool3d(y, 2)
        use_amp = torch.cuda.is_available()
        with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
            out_h, flow_h = model_((x_half, y_half))
        flow = F.interpolate(flow_h.float(), scale_factor=2, mode="trilinear", align_corners=False) * 2.0
        return flow

    return ModelBundle("ctcf", model, forward_flow)


def build_utsrmorph(config_key: str = "UTSRMorph-Large") -> ModelBundle:
    from models.UTSRMorph.model import CONFIGS as CONFIGS_UM, UTSRMorph

    if config_key not in CONFIGS_UM:
        raise KeyError(f"Unknown UTSRMorph config '{config_key}'. Available: {list(CONFIGS_UM.keys())}")

    config = CONFIGS_UM[config_key]
    model = UTSRMorph(config)

    @torch.no_grad()
    def forward_flow(model_, x, y):
        inp = torch.cat((x, y), dim=1)
        use_amp = torch.cuda.is_available()
        with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
            _, flow = model_(inp)
        return flow

    return ModelBundle("utsrmorph", model, forward_flow)


def build_model(model_name: str, args) -> ModelBundle:
    model_name = model_name.lower().strip()
    if model_name in ("tm-dca", "tm_dca", "tmdca"):
        return build_tm_dca(time_steps=args.time_steps, vol_size=args.vol_size, dwin=args.dwin)
    if model_name in ("ctcf",):
        return build_ctcf(time_steps=args.time_steps, vol_size=args.vol_size, dwin=args.dwin, config_key=args.ctcf_config)
    if model_name in ("utsrmorph", "utsr"):
        return build_utsrmorph(config_key=args.utsr_config)
    raise ValueError("Unknown --model. Use one of: tm-dca, utsrmorph, ctcf")


# ------------------------------ Inference ------------------------------ #

def run_inference(args):
    dev = setup_device(args.gpu, seed=args.seed, deterministic=args.deterministic)
    device = dev.device

    test_dir = args.test_dir
    if not test_dir.endswith(os.sep):
        test_dir += os.sep

    out_dir = ensure_dir(args.out_dir)
    png_dir = ensure_dir(os.path.join(out_dir, "png")) if args.save_pngs else None
    flow_dir = ensure_dir(os.path.join(out_dir, "flows")) if args.save_flow else None

    test_files = sorted(glob.glob(test_dir + "*.pkl"))
    if not test_files:
        raise RuntimeError(f"No .pkl files found in test_dir: {test_dir}")

    test_tf = transforms.Compose([NumpyType((np.float32, np.int16))])
    test_set = datasets.OASISBrainInferDataset(test_files, transforms=test_tf)
    test_loader = DataLoader(
        test_set,
        batch_size=1,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True,
    )

    bundle = build_model(args.model, args)
    model = bundle.model.to(device)
    forward_flow_fn = bundle.forward_flow_fn

    ckpt_path = resolve_checkpoint(args.ckpt, prefer_best=not args.prefer_last)
    print(f"[INFO] Model: {bundle.name}")
    print(f"[INFO] Checkpoint: {ckpt_path}")
    _ = load_state_dict(model, ckpt_path)
    model.eval()

    reg_nearest = None
    reg_bilin = None

    per_case_path = os.path.join(out_dir, "per_case.csv")
    header = (
        ["case_id", "dice_mean", "fold_percent", "logdet_std", "time_sec"]
        + (["hd95_mean"] if args.hd95 else [])
        + [f"dice_lbl_{i}" for i in range(1, 36)]
    )

    rows = []

    with torch.no_grad():
        for idx, batch in enumerate(test_loader):
            x, y, x_seg, y_seg = [t.to(device, non_blocking=True) for t in batch]
            vol_shape = tuple(x.shape[2:])

            if reg_nearest is None:
                reg_nearest = register_model(vol_shape, mode="nearest").to(device)
                reg_bilin = register_model(vol_shape, mode="bilinear").to(device)

            cid = case_id_from_path(test_files[idx])

            t0 = time.perf_counter()
            flow = forward_flow_fn(model, x, y)
            def_seg = reg_nearest([x_seg.float(), flow.float()])
            dt = time.perf_counter() - t0

            dice_mean = float(dice_val_VOI(def_seg.long(), y_seg.long()).item())
            dice_lbl = dice_per_label_1to35(def_seg.long(), y_seg.long())
            foldp = fold_percent_from_flow(flow)
            logdet_std = logdet_std_from_flow(flow)

            row = {
                "case_id": cid,
                "dice_mean": dice_mean,
                "fold_percent": foldp,
                "logdet_std": logdet_std,
                "time_sec": dt,
            }

            if args.hd95:
                row["hd95_mean"] = hd95_mean_1to35(def_seg.long(), y_seg.long(), spacing=(1.0, 1.0, 1.0))

            for i in range(35):
                row[f"dice_lbl_{i+1}"] = float(dice_lbl[i])

            rows.append(row)

            if args.save_flow:
                save_flow_npz(flow, os.path.join(flow_dir, f"flow_{cid}.npz"))

            if args.save_pngs and (args.png_limit < 0 or idx < args.png_limit):
                grid_img = make_grid_img(vol_shape, step=args.grid_step, thickness=args.line_thickness, device=device)
                def_grid = reg_bilin([grid_img.float(), flow.float()])

                save_png_triplet(
                    out_png=os.path.join(png_dir, f"{cid}.png"),
                    x=x, y=y, x_seg=x_seg, y_seg=y_seg, def_seg=def_seg, def_grid=def_grid,
                )

            if (idx + 1) % max(1, args.print_every) == 0:
                msg = f"[{idx+1:03d}/{len(test_loader):03d}] {cid} dice={dice_mean:.4f} fold%={foldp:.4f} time={dt:.3f}s"
                if args.hd95:
                    msg += f" hd95={row['hd95_mean']:.4f}"
                print(msg)

    with open(per_case_path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=header)
        w.writeheader()
        for r in rows:
            w.writerow(r)

    def agg(key: str) -> Tuple[float, float]:
        arr = np.array([r[key] for r in rows], dtype=np.float64)
        return float(arr.mean()), float(arr.std(ddof=1)) if len(arr) > 1 else 0.0

    n = len(rows)
    summary = {
        "model": bundle.name,
        "ckpt_path": ckpt_path,
        "test_dir": args.test_dir,
        "n_cases": n,
        "metrics": {},
    }

    keys = ["dice_mean", "fold_percent", "logdet_std", "time_sec"] + (["hd95_mean"] if args.hd95 else [])
    for key in keys:
        m, s = agg(key)
        sem = s / math.sqrt(n) if n > 1 else 0.0
        ci95 = compute_ci95(m, s, n)
        summary["metrics"][key] = {"mean": m, "std": s, "sem": sem, "ci95": ci95}

    summary_path = os.path.join(out_dir, "summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    summary_csv_path = os.path.join(out_dir, "summary.csv")
    with open(summary_csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["metric", "mean", "std", "sem", "ci95"])
        for k, v in summary["metrics"].items():
            w.writerow([k, f"{v['mean']:.6f}", f"{v['std']:.6f}", f"{v['sem']:.6f}", f"{v['ci95']:.6f}"])

    print(f"\n[SAVED] per-case:  {per_case_path}")
    print(f"[SAVED] summary:   {summary_path}")
    print(f"[SAVED] summary:   {summary_csv_path}")
    if args.save_pngs:
        print(f"[SAVED] png dir:   {png_dir}")
    if args.save_flow:
        print(f"[SAVED] flow dir:  {flow_dir}")


# ------------------------------ CLI ------------------------------ #

def build_parser():
    p = argparse.ArgumentParser(description="Unified OASIS inference for TM-DCA / UTSRMorph / CTCF")
    sub = p.add_subparsers(dest="cmd", required=True)

    pinf = sub.add_parser("infer", help="Run inference for one model and export metrics/visuals")
    pinf.add_argument("--model", required=True, choices=["tm-dca", "utsrmorph", "ctcf"])
    pinf.add_argument("--ckpt", required=True, help="Checkpoint FILE or directory containing checkpoints")
    pinf.add_argument("--test_dir", required=True, help="Path to OASIS Test/*.pkl")
    pinf.add_argument("--out_dir", required=True, help="Output directory (will be created)")
    pinf.add_argument("--gpu", type=int, default=0)
    pinf.add_argument("--seed", type=int, default=0)
    pinf.add_argument("--deterministic", action="store_true")
    pinf.add_argument("--num_workers", type=int, default=4)
    pinf.add_argument("--print_every", type=int, default=1)
    pinf.add_argument("--prefer_last", action="store_true", help="Prefer last/newest instead of best")
    pinf.add_argument("--save_flow", action="store_true", help="Save flow fields as .npz")
    pinf.add_argument("--save_pngs", action="store_true", help="Save PNG visuals (requires matplotlib)")
    pinf.add_argument("--png_limit", type=int, default=5, help="How many cases to save PNG for (-1 = all)")
    pinf.add_argument("--grid_step", type=int, default=8)
    pinf.add_argument("--line_thickness", type=int, default=1)
    pinf.add_argument("--hd95", action="store_true", help="Compute HD95 mean over labels 1..35 (requires surface-distance)")

    pinf.add_argument("--time_steps", type=int, default=12, help="TM-DCA/CTCF cascade steps")
    pinf.add_argument("--vol_size", type=int, nargs=3, default=[160, 192, 224], help="Volume size D H W")
    pinf.add_argument("--dwin", type=int, nargs=3, default=[7, 5, 3], help="TM-DCA/CTCF dwin kernel size")
    pinf.add_argument("--utsr_config", type=str, default="UTSRMorph-Large", help="UTSRMorph config key")
    pinf.add_argument("--ctcf_config", type=str, default="CTCF-DCA-SR", help="CTCF config key")

    return p


def main():
    parser = build_parser()
    args = parser.parse_args()
    if args.cmd == "infer":
        run_inference(args)
    else:
        raise RuntimeError("Unknown command")


if __name__ == "__main__":
    main()

==================================================

üìÑ experiments/OASIS/plot_bars.py
--------------------------------------------------
import json
import os
import matplotlib.pyplot as plt

BASE = r"C:\Users\user\Documents\Education\MasterWork\results\infer"

runs = {
    "CTCF":     os.path.join(BASE, "CTCF", "summary.json"),
    "UTSRMorph":os.path.join(BASE, "UTSRMorph", "summary.json"),
    "TM-DCA":   os.path.join(BASE, "TM-DCA", "summary.json"),
}

def load_metric(path, key):
    with open(path, "r", encoding="utf-8") as f:
        s = json.load(f)
    if key not in s["metrics"]:
        raise KeyError(f"Metric '{key}' not found in {path}. Did you run inference with the needed flags?")
    return s["metrics"][key]["mean"], s["metrics"][key]["std"]

methods = list(runs.keys())

def bar(metric_key, ylabel, title, out_png, use_errorbars=True):
    means = []
    stds = []
    for m in methods:
        mean, std = load_metric(runs[m], metric_key)
        means.append(mean)
        stds.append(std)

    plt.figure()
    if use_errorbars:
        plt.bar(methods, means, yerr=stds)
    else:
        plt.bar(methods, means)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.tight_layout()
    plt.savefig(os.path.join(BASE, out_png), dpi=250)
    plt.close()

if __name__ == "__main__":
    # Dice
    bar("dice_mean", "Dice", "Dice on OASIS Test (mean ¬± std)", "bar_dice.png", use_errorbars=True)

    # Fold%
    bar("fold_percent", "Fold (%)", "Folding Percentage on OASIS Test (mean ¬± std)", "bar_fold.png", use_errorbars=True)

    # HD95 (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç—ã –∑–∞–ø—É—Å–∫–∞–ª –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å --hd95)
    try:
        bar("hd95_mean", "HD95", "HD95 on OASIS Test (mean ¬± std)", "bar_hd95.png", use_errorbars=True)
    except KeyError:
        print("[WARN] hd95_mean not found in summaries. Run inference with --hd95 if you need HD95 bar chart.")

    print("[OK] Saved charts into:", BASE)

==================================================

üìÑ experiments/OASIS/train_CTCF.py
--------------------------------------------------
from torch.utils.tensorboard import SummaryWriter
import os, glob, argparse, time
import numpy as np
import torch
import contextlib
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim
import torch.nn.functional as F
import matplotlib.pyplot as plt

from experiments.OASIS import datasets
from models.CTCF.model import CONFIGS as CONFIGS_CTCF
import models.CTCF.model as CTCF
from models.CTCF.lambda_net import LambdaNet3D

from utils import (
    AverageMeter,
    setup_device,
    make_exp_dirs,
    attach_stdout_logger,
    save_checkpoint,
    load_checkpoint_if_exists,
    perf_epoch_start,
    perf_epoch_end,
    adjust_learning_rate_poly,
    NCC_vxm,
    DiceLoss,
    Grad3d,
    WeightedGrad3d,
    Grad1ch3d,
    NumpyType,
    dice_val_VOI,
    register_model,
    mk_grid_img,
    comput_fig,
    validate_oasis,
    icon_loss,
    cycle_image_loss,
    neg_jacobian_penalty,
)


# -------------------- Validation adapter --------------------

@torch.no_grad()
def forward_flow_ctcf(model, x, y):
    x_half = F.avg_pool3d(x, 2)
    y_half = F.avg_pool3d(y, 2)
    use_amp = torch.cuda.is_available()
    with torch.amp.autocast("cuda", enabled=use_amp):
        out_h, flow_h = model((x_half, y_half))

    flow = F.interpolate(
        flow_h.float(),
        scale_factor=2,
        mode='trilinear',
        align_corners=False
    ) * 2.0
    
    return flow

# ---------------------- CLI ----------------------

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument('--train_dir', required=True, help='Path to OASIS training .pkl files (e.g. .../All)')
    p.add_argument('--val_dir', required=True, help='Path to OASIS validation/test .pkl files (e.g. .../Test)')
    p.add_argument('--exp', default='CTCF_v1', help='Experiment name (results/<exp>/, logs/<exp>/)')
    p.add_argument('--lr', type=float, default=1e-4, help='Initial learning rate')
    p.add_argument('--max_epoch', type=int, default=500, help='Number of training epochs')
    p.add_argument('--batch_size', type=int, default=1, help='Batch size (default: 1)')
    p.add_argument('--cont', action='store_true', help='Resume from results/<exp>/last.pth.tar')
    p.add_argument('--gpu', type=int, default=0, help='GPU id to use')

    # base losses
    p.add_argument('--w_ncc', type=float, default=1.0)
    p.add_argument('--w_dsc', type=float, default=1.0)
    p.add_argument('--w_reg', type=float, default=1.0)

    # CTCF regularizers
    p.add_argument('--w_icon', type=float, default=0.1)
    p.add_argument('--w_cyc', type=float, default=0.05)
    p.add_argument('--w_jac', type=float, default=0.01)

    # LambdaNet settings
    p.add_argument('--lambda_epochs', type=float, default=30)
    p.add_argument('--lambda_prior', type=float, default=0.01)
    p.add_argument('--lambda_smooth', type=float, default=0.001)
    p.add_argument('--lambda_target', type=float, default=0.7)

    # model
    p.add_argument('--time_steps', type=int, default=12, help='Cascade steps (keep same as TM-DCA baseline unless testing)')
    p.add_argument('--unsup', action='store_true', help='Train without DSC loss (pure unsupervised).')

    # perf/memory
    p.add_argument('--num_workers', type=int, default=4)
    p.add_argument('--amp', action='store_true', help='Enable AMP (recommended on GPU).')
    return p.parse_args()


# ---------------------- main ----------------------

def main():
    args = parse_args()

    # ---------- device ----------

    dev = setup_device(args.gpu, seed=0, deterministic=False)
    device = dev.device

    # ---------- experiment dirs + logger ----------

    paths = make_exp_dirs(args.exp)
    attach_stdout_logger(paths.log_dir)
    writer = SummaryWriter(log_dir=paths.log_dir)

    train_dir = args.train_dir if args.train_dir.endswith(os.sep) else args.train_dir + os.sep
    val_dir   = args.val_dir   if args.val_dir.endswith(os.sep)   else args.val_dir   + os.sep

    lr = args.lr
    max_epoch = args.max_epoch
    batch_size = args.batch_size

    W_ncc = args.w_ncc
    W_dsc = args.w_dsc
    W_reg = args.w_reg
    W_icon = args.w_icon
    W_cyc = args.w_cyc
    W_jac = args.w_jac

    LAMBDA_WARMUP_EPOCHS = args.lambda_epochs
    W_LAM_PRIOR = args.lambda_prior
    W_LAM_SMOOTH = args.lambda_smooth
    LAM_TARGET = args.lambda_target

    time_steps = args.time_steps
    unsup = args.unsup

    print(f'>>> Experiment: {args.exp}')
    print(f'    train_dir = {train_dir}')
    print(f'    val_dir   = {val_dir}')
    print(f'    lr={lr}, max_epoch={max_epoch}, batch_size={batch_size}, cont={args.cont}')
    print(f'    time_steps={time_steps}')
    print(f'    loss weights: NCC={W_ncc}, DSC={W_dsc}, REG={W_reg}, ICON={W_icon}, CYC={W_cyc}, JAC={W_jac}')
    print(f'    UNSUPERVISED={"YES" if unsup else "NO"} (training)')

    # ---------- model config ----------

    config = CONFIGS_CTCF['CTCF-DCA-SR']
    full_size = (160, 192, 224)  # (D,H,W)
    model = CTCF.CTCF_DCA_SR(config, time_steps).to(device)

    # ---------- spatial transformers for visuals/seg warp ----------
    # For seg warp in training/validation (nearest) and grid (bilinear)

    reg_nearest = register_model(full_size, 'nearest').to(device)
    reg_bilin   = register_model(full_size, 'bilinear').to(device)

    # ---------- optimizer + losses ----------

    # ---- lambda-net for spatially-varying regularization ----
    lambda_net = LambdaNet3D(base_ch=16, lambda_min=0.3, lambda_max=1.0).to(device)

    optimizer = optim.AdamW(
        list(model.parameters()) + list(lambda_net.parameters()),
        lr=lr, weight_decay=0.0, amsgrad=True
    )

    criterion_reg_w = WeightedGrad3d(penalty='l2')   # weighted smoothness for flow
    criterion_lam_s = Grad1ch3d(penalty='l2')        # smoothness for lambda map

    criterion_ncc = NCC_vxm()
    criterion_dsc = DiceLoss()
    criterion_reg = Grad3d(penalty='l2')

    use_amp = bool(args.amp) and torch.cuda.is_available()
    scaler = torch.amp.GradScaler('cuda') if use_amp else None

    # ---------- resume ----------

    epoch_start = 0
    best_dsc = 0.0
    if args.cont:
        ckpt_path = os.path.join(paths.exp_dir, 'last.pth.tar')
        ckpt = load_checkpoint_if_exists(ckpt_path, model, optimizer, map_location='cuda' if torch.cuda.is_available() else 'cpu')
        if ckpt:
            epoch_start = ckpt.get('epoch', 0)
            best_dsc = ckpt.get('best_dsc', 0.0)
            print(f"Loaded last checkpoint: epoch_start={epoch_start}, best_dsc={best_dsc:.4f}")
        else:
            print('No last.pth.tar found, starting from scratch.')

    # ---------- datasets ----------

    train_tf = transforms.Compose([NumpyType((np.float32, np.int16))])
    val_tf   = transforms.Compose([NumpyType((np.float32, np.int16))])

    train_set = datasets.OASISBrainDataset(glob.glob(train_dir + '*.pkl'), transforms=train_tf)
    val_set   = datasets.OASISBrainInferDataset(glob.glob(val_dir + '*.pkl'), transforms=val_tf)

    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)
    val_loader   = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)

    print(f'>>> #train={len(train_loader.dataset)}, #val={len(val_loader.dataset)}')

    # -------------------- training loop --------------------

    for epoch in range(epoch_start, max_epoch):
        print(f'Training Starts (epoch {epoch})')

        t0 = perf_epoch_start()
        cur_lr = adjust_learning_rate_poly(optimizer, epoch, max_epoch, lr)

        loss_all = AverageMeter()
        idx = 0
        iter_time_sum = 0.0

        for batch in train_loader:
            idx += 1
            model.train()
            iter_t0 = time.perf_counter()

            batch = [t.to(device, non_blocking=True) for t in batch]
            x, y, x_seg_idx, y_seg_idx = batch  # x: moving, y: fixed ; seg are label indices [B,1,D,H,W]

            with torch.no_grad():
                x_half = F.avg_pool3d(x, 2)
                y_half = F.avg_pool3d(y, 2)

                if not unsup:
                    x_seg_oh = F.one_hot(x_seg_idx.long(), 36).float().squeeze(1).permute(0, 4, 1, 2, 3)
                    y_seg_oh = F.one_hot(y_seg_idx.long(), 36).float().squeeze(1).permute(0, 4, 1, 2, 3)
                else:
                    x_seg_oh, y_seg_oh = None, None

            # ---------------- bidirectional step (x->y and y->x) ----------------

            optimizer.zero_grad(set_to_none=True)
            autocast_ctx = torch.amp.autocast('cuda') if use_amp else contextlib.nullcontext()

            with autocast_ctx:
                out_xy_h, flow_xy_h = model((x_half, y_half))
                out_yx_h, flow_yx_h = model((y_half, x_half))

                out_xy = F.interpolate(out_xy_h, scale_factor=2, mode='trilinear', align_corners=False)
                out_yx = F.interpolate(out_yx_h, scale_factor=2, mode='trilinear', align_corners=False)
                flow_xy = F.interpolate(flow_xy_h, scale_factor=2, mode='trilinear', align_corners=False) * 2.0
                flow_yx = F.interpolate(flow_yx_h, scale_factor=2, mode='trilinear', align_corners=False) * 2.0

                # NCC in float32 for stability
                with torch.amp.autocast('cuda', enabled=False):
                    L_ncc = (criterion_ncc(out_xy.float(), y.float()) + criterion_ncc(out_yx.float(), x.float())) * 0.5
                    L_ncc = L_ncc * W_ncc

                # DSC (optional)
                if not unsup:
                    if hasattr(model, 'spatial_trans_full'):
                        def_xseg = model.spatial_trans_full(x_seg_oh, flow_xy)
                        def_yseg = model.spatial_trans_full(y_seg_oh, flow_yx)
                    else:
                        def_xseg = reg_bilin([x_seg_oh, flow_xy])
                        def_yseg = reg_bilin([y_seg_oh, flow_yx])
                    L_dsc = (criterion_dsc(def_xseg, y_seg_oh) + criterion_dsc(def_yseg, x_seg_oh)) * 0.5
                    L_dsc = L_dsc * W_dsc
                else:
                    L_dsc = torch.tensor(0.0, device=device, dtype=torch.float32)

                # -------- spatially-varying regularization (lambda(x)) --------

                # lambda on half-res (same grid as x_half/y_half)
                lambda_xy_h = lambda_net(x_half, y_half)   # (B,1,80,96,112)
                lambda_yx_h = lambda_net(y_half, x_half)

                # upsample lambda to FULL-res to match flow_xy/flow_yx
                lambda_xy = F.interpolate(lambda_xy_h, scale_factor=2, mode='trilinear', align_corners=False)
                lambda_yx = F.interpolate(lambda_yx_h, scale_factor=2, mode='trilinear', align_corners=False)

                # warmup: don't let lambda influence too early (stability)
                if epoch < LAMBDA_WARMUP_EPOCHS:
                    lambda_xy = lambda_xy.detach()
                    lambda_yx = lambda_yx.detach()

                # weighted smoothness on FULL-res flow (as baseline, but spatially weighted)
                L_reg = 0.5 * (criterion_reg_w(flow_xy, lambda_xy) + criterion_reg_w(flow_yx, lambda_yx))
                L_reg = L_reg * W_reg

                # keep lambda from collapsing to lambda_min everywhere (mean prior, MSE)
                lambda_prior = W_LAM_PRIOR * (
                    (lambda_xy.mean() - LAM_TARGET) ** 2 +
                    (lambda_yx.mean() - LAM_TARGET) ** 2
                )

                # avoid "noisy lambda mask" (smoothness on lambda itself)
                lambda_smooth = W_LAM_SMOOTH * 0.5 * (
                    criterion_lam_s(lambda_xy) + criterion_lam_s(lambda_yx)
                )

                L_icon = icon_loss(flow_xy, flow_yx) * W_icon
                L_cyc = cycle_image_loss(model, x_half, y_half, out_xy_h, out_yx_h, flow_xy_h, flow_yx_h) * W_cyc
                L_jac = (neg_jacobian_penalty(flow_xy) + neg_jacobian_penalty(flow_yx)) * 0.5
                L_jac = L_jac * W_jac

                loss = L_ncc + L_dsc + L_reg + L_icon + L_cyc + L_jac + lambda_prior + lambda_smooth

                if idx % 50 == 0:
                    with torch.no_grad():
                        lm = lambda_xy.mean().item()
                        ls = lambda_xy.std().item()
                        lmin = lambda_xy.min().item()
                        lmax = lambda_xy.max().item()
                    print(f"[lambda] mean={lm:.3f} std={ls:.3f} min={lmin:.3f} max={lmax:.3f}")

            if not torch.isfinite(loss):
                raise RuntimeError(
                    f"[NON-FINITE LOSS] loss={loss.item()} "
                    f"ncc={float(L_ncc):.4f} dsc={float(L_dsc):.4f} reg={float(L_reg):.4f} "
                    f"icon={float(L_icon):.4f} cyc={float(L_cyc):.4f} jac={float(L_jac):.4f}"
                )

            if scaler is not None:
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()

            loss_all.update(loss.item(), x.numel())
            iter_time_sum += (time.perf_counter() - iter_t0)

            if idx % 10 == 0:
                print(
                    f"Iter {idx:4d}/{len(train_loader):4d} | "
                    f"loss(avg)={loss_all.avg:.4f} | "
                    f"NCC={L_ncc.detach().item():.4f} DSC={L_dsc.detach().item():.4f} REG={L_reg.detach().item():.4f} "
                    f"ICON={L_icon.detach().item():.4f} CYC={L_cyc.detach().item():.4f} JAC={L_jac.detach().item():.4f} | "
                    f"lr={cur_lr:.1e}"
                )

        writer.add_scalar('Loss/train_total', loss_all.avg, epoch)
        writer.add_scalar('Loss/train_ncc', L_ncc.detach().item(), epoch)
        writer.add_scalar('Loss/train_dsc', L_dsc.detach().item(), epoch)
        writer.add_scalar('Loss/train_reg', L_reg.detach().item(), epoch)
        writer.add_scalar('Loss/train_icon', L_icon.detach().item(), epoch)
        writer.add_scalar('Loss/train_cyc', L_cyc.detach().item(), epoch)
        writer.add_scalar('Loss/train_jac', L_jac.detach().item(), epoch)

        print(f'Epoch {epoch} loss {loss_all.avg:.4f}')

        # ---------------- Performance ----------------

        perf = perf_epoch_end(t0, iters=idx, iter_time_sum=iter_time_sum)
        if perf.peak_gpu_mem_gib is not None:
            print(f"[PERF] Epoch {epoch}: time={perf.epoch_time_sec:.1f}s, iter={perf.mean_iter_time_ms:.1f} ms/iter, peak={perf.peak_gpu_mem_gib:.2f} GiB")
            writer.add_scalar('perf/epoch_time_sec', perf.epoch_time_sec, epoch)
            writer.add_scalar('perf/iter_time_ms', perf.mean_iter_time_ms, epoch)
            writer.add_scalar('perf/peak_gpu_mem_GB', perf.peak_gpu_mem_gib, epoch)
        else:
            print(f"[PERF] Epoch {epoch}: time={perf.epoch_time_sec:.1f}s, iter={perf.mean_iter_time_ms:.1f} ms/iter")

        # -------------------- Validation --------------------

        # OOM on 32 GB config:
        torch.cuda.empty_cache()

        val = validate_oasis(
            model=model,
            val_loader=val_loader,
            device=device,
            forward_flow_fn=lambda a, b: forward_flow_ctcf(model, a, b),
            dice_fn=dice_val_VOI,
            register_model_cls=register_model,
            mk_grid_img_fn=mk_grid_img,
        )

        print(f"val DSC: {val.dsc:.4f} | fold%: {val.fold_percent:.2f}")
        writer.add_scalar('DSC/validate', val.dsc, epoch)
        writer.add_scalar('Metric/validate_fold_percent', val.fold_percent, epoch)

        # OOM on 32 GB config:
        del flow_xy, flow_yx
        torch.cuda.empty_cache()

        # -------------------- Checkpoints (BEST + LAST) --------------------

        if val.dsc >= best_dsc:
            best_dsc = val.dsc
            save_checkpoint(
                {
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                    'best_dsc': best_dsc,
                    'optimizer': optimizer.state_dict(),
                },
                save_dir=paths.exp_dir,
                filename='best.pth.tar'
            )
            print(f"Saved new BEST checkpoint (DSC={best_dsc:.4f})")

        save_checkpoint(
            {
                'epoch': epoch + 1,
                'state_dict': model.state_dict(),
                'best_dsc': best_dsc,
                'optimizer': optimizer.state_dict(),
            },
            save_dir=paths.exp_dir,
            filename='last.pth.tar'
        )

        # -------------------- Visuals --------------------
        
        plt.switch_backend('agg')
        def_out  = val.last_vis.get('def_seg', None)
        def_grid = val.last_vis.get('def_grid', None)
        x_vis    = val.last_vis.get('x_seg', None)
        y_vis    = val.last_vis.get('y_seg', None)

        if def_out is not None and def_grid is not None and x_vis is not None and y_vis is not None:
            pred_fig = comput_fig(def_out)
            grid_fig = comput_fig(def_grid)
            x_fig = comput_fig(x_vis)
            tar_fig = comput_fig(y_vis)

            writer.add_figure('Grid', grid_fig, epoch); plt.close(grid_fig)
            writer.add_figure('input', x_fig, epoch); plt.close(x_fig)
            writer.add_figure('ground truth', tar_fig, epoch); plt.close(tar_fig)
            writer.add_figure('prediction', pred_fig, epoch); plt.close(pred_fig)

    writer.close()


if __name__ == '__main__':
    main()
==================================================

üìÑ experiments/OASIS/train_TM-DCA.py
--------------------------------------------------
from torch.utils.tensorboard import SummaryWriter
import os, glob, argparse, time
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim
import torch.nn.functional as F
import matplotlib.pyplot as plt

from experiments.OASIS import datasets
from models.TransMorph_DCA.model import CONFIGS as CONFIGS_TM
import models.TransMorph_DCA.model as TransMorph

from utils import (
    AverageMeter,
    register_model,
    NCC_vxm,
    DiceLoss,
    Grad3d,
    NumpyType,
    setup_device,
    save_checkpoint,
    comput_fig,
    mk_grid_img,
    dice_val_VOI,
    perf_epoch_end,
    perf_epoch_start,
    make_exp_dirs,
    attach_stdout_logger,
    load_checkpoint_if_exists,
    adjust_learning_rate_poly,
    validate_oasis
)


# ---------- Adapter for validate_oasis() ---------- #

def forward_flow_tm_dca(model, x, y):
    x_half = F.avg_pool3d(x, 2)
    y_half = F.avg_pool3d(y, 2)

    use_amp = torch.cuda.is_available()
    with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        flow_half = model((x_half, y_half))

    flow_full = F.interpolate(
        flow_half.float(),
        scale_factor=2,
        mode="trilinear",
        align_corners=False
    ) * 2.0

    return flow_full


# ---------------------- CLI ---------------------- #

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument('--train_dir', required=True, help='Path to OASIS training .pkl files (e.g. .../All)')
    p.add_argument('--val_dir', required=True, help='Path to OASIS validation/test .pkl files (e.g. .../Test)')
    p.add_argument('--exp', default='TM_DCA_v3', help='Experiment name (results/<exp>/, logs/<exp>/)')
    p.add_argument('--lr', type=float, default=1e-4, help='Initial learning rate')
    p.add_argument('--max_epoch', type=int, default=500, help='Number of training epochs')
    p.add_argument('--batch_size', type=int, default=1, help='Batch size (default: 1)')
    p.add_argument('--cont', action='store_true', help='Resume from results/<exp>/last.pth.tar')
    p.add_argument('--gpu', type=int, default=0, help='GPU id to use')

    p.add_argument('--w_ncc', type=float, default=1.0)
    p.add_argument('--w_dsc', type=float, default=1.0)
    p.add_argument('--w_reg', type=float, default=1.0)

    p.add_argument('--time_steps', type=int, default=12, help='Cascade time steps (original default: 12)')
    p.add_argument('--unsup', action='store_true', help='If set, train without DSC loss (pure unsupervised).')
    return p.parse_args()


# ---------------------- main ---------------------- #

def main():
    args = parse_args()

    # ---------- device ----------

    dev = setup_device(args.gpu, seed=0, deterministic=False)
    device = dev.device

    # ---------- experiment dirs + logger ----------

    paths = make_exp_dirs(args.exp)
    attach_stdout_logger(paths.log_dir)
    writer = SummaryWriter(log_dir=paths.log_dir)

    train_dir = args.train_dir if args.train_dir.endswith(os.sep) else args.train_dir + os.sep
    val_dir   = args.val_dir   if args.val_dir.endswith(os.sep)   else args.val_dir   + os.sep

    lr = args.lr
    max_epoch = args.max_epoch
    batch_size = args.batch_size
    time_steps = args.time_steps

    W_ncc = args.w_ncc
    W_dsc = args.w_dsc
    W_reg = args.w_reg
    unsup = args.unsup

    print(f'>>> Experiment: {args.exp}')
    print(f'    train_dir = {train_dir}')
    print(f'    val_dir   = {val_dir}')
    print(f'    lr={lr}, max_epoch={max_epoch}, batch_size={batch_size}, cont={args.cont}')
    print(f'    time_steps={time_steps}')
    print(f'    loss weights: NCC={W_ncc}, DSC={W_dsc}, REG={W_reg}')
    print(f'    UNSUPERVISED={"YES" if unsup else "NO"} (training)')

    D, H, W = 160, 192, 224
    full_size = (D, H, W)
    half_size = (D // 2, H // 2, W // 2)

    # ---------- model ----------

    config = CONFIGS_TM['TransMorph-3-LVL']
    config.img_size = half_size
    config.dwin_kernel_size = (7, 5, 3)
    config.window_size = (D // 32, H // 32, W // 32)

    model = TransMorph.TransMorphCascadeAd(config, time_steps).to(device)
    spatial_trans = TransMorph.SpatialTransformer(full_size).to(device)

    # ---------- optimizer + losses ----------

    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0, amsgrad=True)

    criterion_ncc = NCC_vxm()
    criterion_dsc = DiceLoss()
    criterion_reg = Grad3d(penalty='l2')

    scaler = torch.amp.GradScaler("cuda") if torch.cuda.is_available() else None

    # ---------- resume ----------

    epoch_start = 0
    best_dsc = 0.0
    if args.cont:
        ckpt_path = os.path.join(paths.exp_dir, 'last.pth.tar')
        ckpt = load_checkpoint_if_exists(ckpt_path, model, optimizer, map_location="cuda")
        if ckpt:
            epoch_start = ckpt.get('epoch', 0)
            best_dsc = ckpt.get('best_dsc', 0.0)
            print(f"Loaded last checkpoint: epoch_start={epoch_start}, best_dsc={best_dsc:.4f}")
        else:
            print("No last.pth.tar found, starting from scratch.")

    # ---------- datasets ----------

    train_tf = transforms.Compose([NumpyType((np.float32, np.int16))])
    val_tf   = transforms.Compose([NumpyType((np.float32, np.int16))])

    train_set = datasets.OASISBrainDataset(glob.glob(train_dir + '*.pkl'), transforms=train_tf)
    val_set   = datasets.OASISBrainInferDataset(glob.glob(val_dir + '*.pkl'), transforms=val_tf)

    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader   = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)

    print(f'>>> #train={len(train_loader.dataset)}, #val={len(val_loader.dataset)}')

    # -------------------- training loop -------------------- 

    for epoch in range(epoch_start, max_epoch):
        print(f'Training Starts (epoch {epoch})')

        t0 = perf_epoch_start()
        cur_lr = adjust_learning_rate_poly(optimizer, epoch, max_epoch, lr)

        loss_all = AverageMeter()
        idx = 0
        iter_time_sum = 0.0

        for batch in train_loader:
            idx += 1
            model.train()
            iter_t0 = time.perf_counter()
            
            batch = [t.to(device, non_blocking=True) for t in batch]
            x, y, x_seg_idx, y_seg_idx = batch  # x: moving, y: fixed

            with torch.no_grad():
                x = x.half()
                y = y.half()

                x_half = F.avg_pool3d(x, 2).half()
                y_half = F.avg_pool3d(y, 2).half()

                if not unsup:
                    x_seg_oh = F.one_hot(x_seg_idx.long(), 36).float().squeeze(1).permute(0, 4, 1, 2, 3).half()
                    y_seg_oh = F.one_hot(y_seg_idx.long(), 36).float().squeeze(1).permute(0, 4, 1, 2, 3).half()
                else:
                    x_seg_oh, y_seg_oh = None, None

            # ---------------- x -> y (step 1) ----------------

            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast("cuda", enabled=torch.cuda.is_available()):
                flow_half = model((x_half, y_half))  # half-res flow
                flow = F.interpolate(flow_half, scale_factor=2, mode='trilinear', align_corners=False) * 2.0
                out = spatial_trans(x, flow.half())

                # NCC stability: compute in float32
                with torch.amp.autocast("cuda", enabled=False):
                    L_ncc = criterion_ncc(out.float(), y.float()) * W_ncc

                if not unsup:
                    def_seg = spatial_trans(x_seg_oh, flow.half())
                    L_dsc = criterion_dsc(def_seg, y_seg_oh) * W_dsc
                else:
                    L_dsc = torch.tensor(0.0, device=device, dtype=torch.float32)

                L_reg = criterion_reg(flow.half(), y) * W_reg
                loss = L_ncc + L_dsc + L_reg

            if not torch.isfinite(loss):
                raise RuntimeError(
                    f"[NON-FINITE LOSS x->y] loss={loss.item()} "
                    f"ncc={L_ncc.item()} dsc={L_dsc.item()} reg={L_reg.item()}"
                )

            if torch.cuda.is_available():
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()

            loss_all.update(loss.item(), y.numel())

            # ---------------- y -> x (step 2) ----------------

            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast("cuda", enabled=torch.cuda.is_available()):
                flow_half = model((y_half, x_half))
                flow = F.interpolate(flow_half, scale_factor=2, mode='trilinear', align_corners=False) * 2.0
                out = spatial_trans(y, flow.half())

                with torch.amp.autocast("cuda", enabled=False):
                    L_ncc = criterion_ncc(out.float(), x.float()) * W_ncc

                if not unsup:
                    def_seg = spatial_trans(y_seg_oh, flow.half())
                    L_dsc = criterion_dsc(def_seg, x_seg_oh) * W_dsc
                else:
                    L_dsc = torch.tensor(0.0, device=device, dtype=torch.float32)

                L_reg = criterion_reg(flow.half(), x) * W_reg
                loss = L_ncc + L_dsc + L_reg

            if not torch.isfinite(loss):
                raise RuntimeError(
                    f"[NON-FINITE LOSS y->x] loss={loss.item()} "
                    f"ncc={L_ncc.item()} dsc={L_dsc.item()} reg={L_reg.item()}"
                )

            if torch.cuda.is_available():
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()

            loss_all.update(loss.item(), x.numel())

            iter_time_sum += (time.perf_counter() - iter_t0)

            if idx % 10 == 0:
                print(
                    f"Iter {idx:4d} / {len(train_loader):4d} | "
                    f"loss(avg)={loss_all.avg:.4f} | "
                    f"last NCC={L_ncc.item():.4f} DSC={L_dsc.item():.4f} REG={L_reg.item():.4f} | "
                    f"lr={cur_lr:.1e}"
                )

        writer.add_scalar('Loss/train_total', loss_all.avg, epoch)
        print(f'Epoch {epoch} loss {loss_all.avg:.4f}')

        # ---------------- Performance ----------------

        perf = perf_epoch_end(t0, iters=idx, iter_time_sum=iter_time_sum)
        if perf.peak_gpu_mem_gib is not None:
            print(f"[PERF] Epoch {epoch}: time={perf.epoch_time_sec:.1f}s, iter={perf.mean_iter_time_ms:.1f} ms/iter, peak={perf.peak_gpu_mem_gib:.2f} GiB")
            writer.add_scalar('perf/epoch_time_sec', perf.epoch_time_sec, epoch)
            writer.add_scalar('perf/iter_time_ms', perf.mean_iter_time_ms, epoch)
            writer.add_scalar('perf/peak_gpu_mem_GB', perf.peak_gpu_mem_gib, epoch)
        else:
            print(f"[PERF] Epoch {epoch}: time={perf.epoch_time_sec:.1f}s, iter={perf.mean_iter_time_ms:.1f} ms/iter")

        # -------------------- Validation -------------------- #

        val = validate_oasis(
            model=model,
            val_loader=val_loader,
            device=device,
            forward_flow_fn=lambda x, y: forward_flow_tm_dca(model, x, y),
            dice_fn=dice_val_VOI,
            register_model_cls=register_model,
            mk_grid_img_fn=mk_grid_img,
        )
        print(f"val DSC: {val.dsc:.4f} | fold%: {val.fold_percent:.2f}")
        writer.add_scalar("DSC/validate", val.dsc, epoch)
        writer.add_scalar("Metric/validate_fold_percent", val.fold_percent, epoch)

        # take tensors for visualisation
        def_out  = val.last_vis.get("def_seg", None)
        def_grid = val.last_vis.get("def_grid", None)
        x_vis    = val.last_vis.get("x_seg", None)
        y_vis    = val.last_vis.get("y_seg", None)

        # -------------------- Checkpoints (BEST + LAST) -------------------- #

        if val.dsc >= best_dsc:
            best_dsc = val.dsc
            save_checkpoint(
                {
                    "epoch": epoch + 1,
                    "state_dict": model.state_dict(),
                    "best_dsc": best_dsc,
                    "optimizer": optimizer.state_dict(),
                },
                save_dir=paths.exp_dir,
                filename="best.pth.tar",
            )
            print(f"Saved new BEST checkpoint (DSC={best_dsc:.4f})")

        save_checkpoint(
            {
                "epoch": epoch + 1,
                "state_dict": model.state_dict(),
                "best_dsc": best_dsc,
                "optimizer": optimizer.state_dict(),
            },
            save_dir=paths.exp_dir,
            filename="last.pth.tar",
        )

        # -------------------- Visuals -------------------- #
        
        plt.switch_backend("agg")
        if def_out is not None and def_grid is not None and x_vis is not None and y_vis is not None:
            pred_fig = comput_fig(def_out)
            grid_fig = comput_fig(def_grid)
            x_fig = comput_fig(x_vis)
            tar_fig = comput_fig(y_vis)

            writer.add_figure("Grid", grid_fig, epoch); plt.close(grid_fig)
            writer.add_figure("input", x_fig, epoch); plt.close(x_fig)
            writer.add_figure("ground truth", tar_fig, epoch); plt.close(tar_fig)
            writer.add_figure("prediction", pred_fig, epoch); plt.close(pred_fig)

    writer.close()


if __name__ == '__main__':
    main()
==================================================

üìÑ experiments/OASIS/train_UTSRMorph.py
--------------------------------------------------
from torch.utils.tensorboard import SummaryWriter
import os, glob, argparse, time
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import random

from experiments.OASIS import datasets
from models.UTSRMorph.model import CONFIGS as CONFIGS_UM, UTSRMorph

from utils import (
    AverageMeter,
    register_model,
    NCC_vxm,
    Grad3d,
    NumpyType,
    setup_device,
    save_checkpoint,
    comput_fig,
    mk_grid_img,
    dice_val_VOI,
    perf_epoch_end,
    perf_epoch_start,
    make_exp_dirs,
    attach_stdout_logger,
    load_checkpoint_if_exists,
    adjust_learning_rate_poly,
    validate_oasis,
)


# ---------- Adapter for validate_oasis() ---------- #

def forward_flow_utsrmorph(model, x, y):
    inp = torch.cat((x, y), dim=1)  # [B,2,D,H,W]
    with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):
        _, flow = model(inp)
    return flow


# ---------------------- CLI ---------------------- #

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--train_dir", required=True, help="Path to OASIS training .pkl files (e.g. .../All)")
    p.add_argument("--val_dir", required=True, help="Path to OASIS validation/test .pkl files (e.g. .../Test)")
    p.add_argument("--exp", default="UTSRMorph_v3", help="Experiment name (results/<exp>/, logs/<exp>/)")
    p.add_argument("--lr", type=float, default=1e-4, help="Initial learning rate")
    p.add_argument("--max_epoch", type=int, default=500, help="Number of training epochs")
    p.add_argument("--batch_size", type=int, default=1, help="Batch size (default: 1)")
    p.add_argument("--cont", action="store_true", help="Resume from results/<exp>/last.pth.tar")
    p.add_argument("--gpu", type=int, default=0, help="GPU id to use")

    # loss weights (unsupervised)
    p.add_argument("--w_ncc", type=float, default=1.0)
    p.add_argument("--w_reg", type=float, default=1.0)

    # model config key
    p.add_argument(
        "--config",
        type=str,
        default="UTSRMorph-Large",
        help="Key in CONFIGS_UM (e.g., UTSRMorph-Large)",
    )

    # determinism
    p.add_argument("--seed", type=int, default=0)
    p.add_argument("--deterministic", action="store_true", help="Enable deterministic mode (slower)")

    return p.parse_args()


# ---------------------- main ---------------------- #

def main():
    args = parse_args()

    # ---------- device ----------

    dev = setup_device(args.gpu, seed=args.seed, deterministic=args.deterministic)
    device = dev.device

    # ---------- experiment dirs + logger ----------

    paths = make_exp_dirs(args.exp)
    attach_stdout_logger(paths.log_dir)
    writer = SummaryWriter(log_dir=paths.log_dir)

    train_dir = args.train_dir if args.train_dir.endswith(os.sep) else args.train_dir + os.sep
    val_dir = args.val_dir if args.val_dir.endswith(os.sep) else args.val_dir + os.sep

    lr = args.lr
    max_epoch = args.max_epoch
    batch_size = args.batch_size

    W_ncc = args.w_ncc
    W_reg = args.w_reg

    print(f">>> Experiment: {args.exp}")
    print(f"    train_dir = {train_dir}")
    print(f"    val_dir   = {val_dir}")
    print(f"    cfg       = {args.config}")
    print(f"    lr={lr}, max_epoch={max_epoch}, batch_size={batch_size}, cont={args.cont}")
    print(f"    loss weights: NCC={W_ncc}, REG={W_reg}")

    # ---------- model ----------

    if args.config not in CONFIGS_UM:
        raise KeyError(f"Unknown UTSRMorph config '{args.config}'. Available: {list(CONFIGS_UM.keys())}")

    config = CONFIGS_UM[args.config]
    model = UTSRMorph(config).to(device)

    # ---------- optimizer + losses ----------

    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5, amsgrad=True)
    criterion_ncc = NCC_vxm()
    criterion_reg = Grad3d(penalty="l2")

    use_amp = torch.cuda.is_available()
    scaler = torch.amp.GradScaler("cuda") if use_amp else None

    # ---------- resume ----------

    epoch_start = 0
    best_dsc = 0.0
    if args.cont:
        ckpt_path = os.path.join(paths.exp_dir, "last.pth.tar")
        ckpt = load_checkpoint_if_exists(ckpt_path, model, optimizer, map_location="cuda" if use_amp else "cpu")
        if ckpt:
            epoch_start = ckpt.get("epoch", 0)
            best_dsc = ckpt.get("best_dsc", 0.0)
            print(f"Loaded last checkpoint: epoch_start={epoch_start}, best_dsc={best_dsc:.4f}")
        else:
            print("No last.pth.tar found, starting from scratch.")

    # ---------- datasets ----------

    train_tf = transforms.Compose([NumpyType((np.float32, np.int16))])
    val_tf = transforms.Compose([NumpyType((np.float32, np.int16))])

    train_set = datasets.OASISBrainDataset(glob.glob(train_dir + "*.pkl"), transforms=train_tf)
    val_set = datasets.OASISBrainInferDataset(glob.glob(val_dir + "*.pkl"), transforms=val_tf)

    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)

    print(f">>> #train={len(train_loader.dataset)}, #val={len(val_loader.dataset)}")

    # -------------------- training loop -------------------- 

    for epoch in range(epoch_start, max_epoch):
        print(f"Training Starts (epoch {epoch})")

        t0 = perf_epoch_start()
        cur_lr = adjust_learning_rate_poly(optimizer, epoch, max_epoch, lr)

        loss_all = AverageMeter()
        idx = 0
        iter_time_sum = 0.0

        for batch in train_loader:
            idx += 1
            model.train()
            iter_t0 = time.perf_counter()

            batch = [t.to(device, non_blocking=True) for t in batch]
            x, y, x_seg, y_seg = batch  # [B,1,D,H,W] + segs

            if random.randint(0, 1) == 0:
                src = x
                tgt = y
            else:
                src = y
                tgt = x

            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast("cuda", enabled=use_amp):
                inp = torch.cat((src, tgt), dim=1)  # [B,2,D,H,W]

                # NCC stability: compute in float32 (same idea as TM-DCA)
                with torch.amp.autocast("cuda", enabled=False):
                    out, flow = model(inp)
                    L_ncc = criterion_ncc(out.float(), tgt.float()) * W_ncc

                # regularization
                L_reg = criterion_reg(flow, tgt) * W_reg

                loss = L_ncc + L_reg

            if not torch.isfinite(loss):
                print("===== NON-FINITE LOSS DETECTED (UTSRMorph) =====")
                print(f"loss={loss.item()} NCC={L_ncc.item()} REG={L_reg.item()}")
                print(f"flow stats: min={flow.min().item():.4e}, max={flow.max().item():.4e}")
                raise RuntimeError("Non-finite loss in UTSRMorph training.")

            if use_amp:
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()

            loss_all.update(loss.item(), tgt.numel())
            iter_time_sum += (time.perf_counter() - iter_t0)

            if idx % 10 == 0:
                print(
                    f"Iter {idx:4d} / {len(train_loader):4d} | "
                    f"loss(avg)={loss_all.avg:.4f} | "
                    f"last NCC={L_ncc.item():.4f} REG={L_reg.item():.4f} | "
                    f"lr={cur_lr:.1e}"
                )

        writer.add_scalar("Loss/train_total", loss_all.avg, epoch)
        print(f"Epoch {epoch} loss {loss_all.avg:.4f}")

        # ---------------- Performance ----------------

        perf = perf_epoch_end(t0, iters=idx, iter_time_sum=iter_time_sum)
        if perf.peak_gpu_mem_gib is not None:
            print(
                f"[PERF] Epoch {epoch}: time={perf.epoch_time_sec:.1f}s, "
                f"iter={perf.mean_iter_time_ms:.1f} ms/iter, "
                f"peak={perf.peak_gpu_mem_gib:.2f} GiB"
            )
            writer.add_scalar("perf/epoch_time_sec", perf.epoch_time_sec, epoch)
            writer.add_scalar("perf/iter_time_ms", perf.mean_iter_time_ms, epoch)
            writer.add_scalar("perf/peak_gpu_mem_GB", perf.peak_gpu_mem_gib, epoch)
        else:
            print(f"[PERF] Epoch {epoch}: time={perf.epoch_time_sec:.1f}s, iter={perf.mean_iter_time_ms:.1f} ms/iter")

        # -------------------- Validation (unified) -------------------- #

        # OOM on 32 GB config:
        torch.cuda.empty_cache()

        val = validate_oasis(
            model=model,
            val_loader=val_loader,
            device=device,
            forward_flow_fn=lambda x, y: forward_flow_utsrmorph(model, x, y),
            dice_fn=dice_val_VOI,
            register_model_cls=register_model,
            mk_grid_img_fn=mk_grid_img,
        )
        print(f"val DSC: {val.dsc:.4f} | fold%: {val.fold_percent:.2f}")
        writer.add_scalar("DSC/validate", val.dsc, epoch)
        writer.add_scalar("Metric/validate_fold_percent", val.fold_percent, epoch)

        # tensors for visualization
        def_out = val.last_vis.get("def_seg", None)
        def_grid = val.last_vis.get("def_grid", None)
        x_vis = val.last_vis.get("x_seg", None)
        y_vis = val.last_vis.get("y_seg", None)

        # OOM on 32 GB config:
        del flow, out
        torch.cuda.empty_cache()

        # -------------------- Checkpoints (BEST + LAST) -------------------- #

        if val.dsc >= best_dsc:
            best_dsc = val.dsc
            save_checkpoint(
                {
                    "epoch": epoch + 1,
                    "state_dict": model.state_dict(),
                    "best_dsc": best_dsc,
                    "optimizer": optimizer.state_dict(),
                },
                save_dir=paths.exp_dir,
                filename="best.pth.tar",
            )
            print(f"Saved new BEST checkpoint (DSC={best_dsc:.4f})")

        save_checkpoint(
            {
                "epoch": epoch + 1,
                "state_dict": model.state_dict(),
                "best_dsc": best_dsc,
                "optimizer": optimizer.state_dict(),
            },
            save_dir=paths.exp_dir,
            filename="last.pth.tar",
        )

        # -------------------- Visuals -------------------- #

        if epoch % 10 == 0:
            plt.switch_backend("agg")
            if def_out is not None and def_grid is not None and x_vis is not None and y_vis is not None:
                pred_fig = comput_fig(def_out)
                grid_fig = comput_fig(def_grid)
                x_fig = comput_fig(x_vis)
                tar_fig = comput_fig(y_vis)

                writer.add_figure("Grid", grid_fig, epoch); plt.close(grid_fig)
                writer.add_figure("input", x_fig, epoch); plt.close(x_fig)
                writer.add_figure("ground truth", tar_fig, epoch); plt.close(tar_fig)
                writer.add_figure("prediction", pred_fig, epoch); plt.close(pred_fig)

    writer.close()


if __name__ == "__main__":
    main()
==================================================

üìÑ experiments/__init__.py
--------------------------------------------------

==================================================

üìÑ models/CTCF/__init__.py
--------------------------------------------------

==================================================

üìÑ models/CTCF/configs.py
--------------------------------------------------
import ml_collections


def get_CTCF_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 1
    config.embed_dim = 96
    config.dwin_kernel_size = (7, 5, 3)
    config.depths = (4, 4, 5)
    config.num_heads = (8, 8, 8)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    # SE block
    config.use_se = True
    config.se_reduction = 8
    # =======
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0.0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2)
    config.reg_head_chan = 16
    # config.img_size = (160, 192, 224)
    config.img_size = (80, 96, 112)
    config.time_steps = 10
    return config


def get_CTCF_debug_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 1
    config.embed_dim = 12
    config.dwin_kernel_size = (7, 5, 3)
    config.depths = (2, 2, 2)
    config.num_heads = (4, 4, 4)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0.0
    config.drop_path_rate = 0.1
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2)
    config.reg_head_chan = 1
    config.img_size = (160, 192, 224)
    config.time_steps = 1
    return config
==================================================

üìÑ models/CTCF/lambda_net.py
--------------------------------------------------
import torch
import torch.nn as nn

class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv3d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.InstanceNorm3d(out_ch, affine=True),
            nn.LeakyReLU(0.1, inplace=True),
            nn.Conv3d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.InstanceNorm3d(out_ch, affine=True),
            nn.LeakyReLU(0.1, inplace=True),
        )

    def forward(self, x):
        return self.net(x)

class LambdaNet3D(nn.Module):
    """
    Predict spatially-varying lambda(x) on HALF-res grid.
    Input:  mov_half, fix_half  (B,1,D,H,W each) -> concat -> (B,2,D,H,W)
    Output: lambda_half (B,1,D,H,W) in [lambda_min, lambda_max]
    """
    def __init__(self, base_ch=16, lambda_min=0.3, lambda_max=1.0):
        super().__init__()
        self.lambda_min = float(lambda_min)
        self.lambda_max = float(lambda_max)

        self.enc1 = ConvBlock(2, base_ch)
        self.pool1 = nn.AvgPool3d(2)

        self.enc2 = ConvBlock(base_ch, base_ch * 2)
        self.pool2 = nn.AvgPool3d(2)

        self.bot = ConvBlock(base_ch * 2, base_ch * 4)

        self.up2 = nn.Upsample(scale_factor=2, mode="trilinear", align_corners=False)
        self.dec2 = ConvBlock(base_ch * 4 + base_ch * 2, base_ch * 2)

        self.up1 = nn.Upsample(scale_factor=2, mode="trilinear", align_corners=False)
        self.dec1 = ConvBlock(base_ch * 2 + base_ch, base_ch)

        self.out = nn.Conv3d(base_ch, 1, kernel_size=1, bias=True)

    def forward(self, mov, fix):
        x = torch.cat([mov, fix], dim=1)  # (B,2,D,H,W)

        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b  = self.bot(self.pool2(e2))

        d2 = self.up2(b)
        d2 = self.dec2(torch.cat([d2, e2], dim=1))

        d1 = self.up1(d2)
        d1 = self.dec1(torch.cat([d1, e1], dim=1))

        raw = self.out(d1)
        sig = torch.sigmoid(raw)
        lam = self.lambda_min + (self.lambda_max - self.lambda_min) * sig
        return lam
==================================================

üìÑ models/CTCF/model.py
--------------------------------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F

import models.CTCF.configs as configs
from models.TransMorph_DCA.model import (
    SwinTransformer, 
    Conv3dReLU, 
    RegistrationHead, 
    SpatialTransformer
)
from models.UTSRMorph.model import (
    PixelShuffle3d,
    ConvergeHead,
    SR
)


class CTCF_DCA_SR(nn.Module):
    """
    CTCF = TM-DCA Cascade (flow integration preserved) + UTSRMorph SR-style decoder blocks.
    Expected tensor layout everywhere: [B, C, D, H, W]
    config.img_size is (D, H, W) of the resolution at which the model operates.
    """

    def __init__(self, config, time_steps: int = 7):
        super().__init__()

        self.if_convskip = bool(getattr(config, "if_convskip", True))
        self.if_transskip = bool(getattr(config, "if_transskip", True))
        embed_dim = config.embed_dim
        self.time_steps = time_steps
        self.img_size = tuple(config.img_size)  # (D, H, W)
        self.transformer = SwinTransformer(
            patch_size=config.patch_size,
            in_chans=config.in_chans,
            embed_dim=config.embed_dim,
            depths=config.depths,
            num_heads=config.num_heads,
            window_size=config.window_size,
            mlp_ratio=config.mlp_ratio,
            use_se=config.use_se,
            se_reduction=config.se_reduction,
            qkv_bias=config.qkv_bias,
            drop_rate=config.drop_rate,
            drop_path_rate=config.drop_path_rate,
            ape=config.ape,
            spe=config.spe,
            rpe=config.rpe,
            patch_norm=config.patch_norm,
            use_checkpoint=config.use_checkpoint,
            out_indices=config.out_indices,
            pat_merg_rf=config.pat_merg_rf,
            img_size=config.img_size,
            dwin_size=config.dwin_kernel_size,
        )

        # SR-style decoder backbone (replaces TM DecoderBlock up0/up1/up2)
        # These SR blocks must internally upsample by x2.
        self.up0 = SR(
            in_channels=embed_dim * 4,
            out_channels=embed_dim * 2,
            skip_channels=(embed_dim * 2 if self.if_transskip else 0),
            use_batchnorm=False,
        )
        self.up1 = SR(
            in_channels=embed_dim * 2,
            out_channels=embed_dim,
            skip_channels=(embed_dim if self.if_transskip else 0),
            use_batchnorm=False,
        )
        self.up2 = SR(
            in_channels=embed_dim,
            out_channels=embed_dim // 2,
            skip_channels=(embed_dim // 2 if self.if_transskip else 0),
            use_batchnorm=False,
        )

        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)
        self.c1 = Conv3dReLU(2, embed_dim // 2, kernel_size=3, stride=1, use_batchnorm=False)
        self.cs = nn.ModuleList()
        self.up3s = nn.ModuleList()
        self.reg_heads = nn.ModuleList()

        for _ in range(self.time_steps):
            self.cs.append(
                Conv3dReLU(2, embed_dim // 2, kernel_size=3, stride=1, use_batchnorm=False)
            )

            # SR block does the final x2 upsample before reg_head (replaces TM up3s[t] DecoderBlock)
            self.up3s.append(
                SR(
                    in_channels=embed_dim // 2,
                    out_channels=config.reg_head_chan,
                    skip_channels=(embed_dim // 2),
                    use_batchnorm=False,
                )
            )

            self.reg_heads.append(
                RegistrationHead(
                    in_channels=config.reg_head_chan,
                    out_channels=3,
                    kernel_size=3,
                )
            )
        self.spatial_trans = SpatialTransformer(self.img_size)

    def forward(self, inputs, return_all_flows: bool = False):
        mov, fix = inputs
        assert mov.dim() == 5 and fix.dim() == 5, "Expected [B,C,D,H,W]"
        assert tuple(mov.shape[2:]) == self.img_size, f"mov shape {tuple(mov.shape[2:])} != img_size {self.img_size}"
        assert tuple(fix.shape[2:]) == self.img_size, f"fix shape {tuple(fix.shape[2:])} != img_size {self.img_size}"

        x_cat = torch.cat((mov, fix), dim=1)
        x_s1 = self.avg_pool(x_cat)
        f3 = self.c1(x_s1.float()).to(x_s1.dtype) if self.if_convskip else None
        out_feats = self.transformer((mov, fix))
        if self.if_transskip:
            mov_f1, fix_f1 = out_feats[-2]
            f1 = (mov_f1 + fix_f1)
            mov_f2, fix_f2 = out_feats[-3]
            f2 = (mov_f2 + fix_f2)
        else:
            f1 = None
            f2 = None

        mov_f0, fix_f0 = out_feats[-1]
        f0 = mov_f0 + fix_f0
        x = self.up0(f0, f1)
        x = self.up1(x, f2)
        xx = self.up2(x, f3)
        def_x = mov.clone()
        flow_prev = torch.zeros((mov.shape[0], 3, *self.img_size), 
                                device=mov.device, 
                                dtype=mov.dtype)
        flows = []

        for t in range(self.time_steps):
            f_out = self.cs[t](torch.cat((def_x, fix), dim=1))
            x_t = self.up3s[t](xx, f_out)
            flow_step = self.reg_heads[t](x_t)
            flows.append(flow_step)
            flow_new = flow_prev + self.spatial_trans(flow_step, flow_prev)
            def_x = self.spatial_trans(mov, flow_new)
            flow_prev = flow_new

        if return_all_flows:
            return def_x, flow_prev, flows
        return def_x, flow_prev


CONFIGS = {
    'CTCF-DCA-SR': configs.get_CTCF_config(),
    'CTCF-DCA-SR-Debug': configs.get_CTCF_debug_config(),
}
==================================================

üìÑ models/TransMorph_DCA/__init__.py
--------------------------------------------------

==================================================

üìÑ models/TransMorph_DCA/configs.py
--------------------------------------------------
import ml_collections

'''
********************************************************
             Deformable Swin Transformer
********************************************************
if_transskip (bool): Enable skip connections from Transformer Blocks
if_convskip (bool): Enable skip connections from Convolutional Blocks
patch_size (int | tuple(int)): Patch size. Default: 4
in_chans (int): Number of input image channels. Default: 2 (for moving and fixed images)
embed_dim (int): Patch embedding dimension. Default: 96
dwin_kernel_size (tuple(int)): Deformable window size of each Swin Transformer layer. Default: (7, 5, 3)
depths (tuple(int)): Depth of each Swin Transformer layer.
num_heads (tuple(int)): Number of attention heads in different layers.
window_size (tuple(int)): Image size should be divisible by window size, 
                     e.g., if image has a size of (160, 192, 224), then the window size can be (5, 6, 7)
mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
pat_merg_rf (int): Embed_dim reduction factor in patch merging, e.g., N*C->N/4*C if set to four. Default: 4. 
qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
drop_rate (float): Dropout rate. Default: 0
drop_path_rate (float): Stochastic depth rate. Default: 0.1
ape (bool): Enable learnable position embedding. Default: False
spe (bool): Enable sinusoidal position embedding. Default: False
patch_norm (bool): If True, add normalization after patch embedding. Default: True
use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False 
                       (Carried over from Swin Transformer, it is not needed)
out_indices (tuple(int)): Indices of Transformer blocks to output features. Default: (0, 1, 2, 3)
reg_head_chan (int): Number of channels in the registration head (i.e., the final convolutional layer) 
img_size (int | tuple(int)): Input image size, e.g., (160, 192, 224)
'''

def get_3DTransMorphDWin3Lvl_config():
    '''
    Trainable params: 15,201,579
    '''
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 96
    config.dwin_kernel_size = (7, 5, 3)
    config.depths = (4, 4, 5)
    config.num_heads = (8, 8, 8)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2)
    config.reg_head_chan = 16
    config.img_size = (160, 192, 224)
    return config

def get_3DTransMorphDWin3Lvl_debug_config():
    '''
    Embed-dim: 96 -> 24.
    Depths: 8,8,8 -> 4,4,4
    Reg_head_chan: 16 -> 4
    '''
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 24
    config.dwin_kernel_size = (7, 5, 3)
    config.depths = (4, 4, 5)
    config.num_heads = (4, 4, 4)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2)
    config.reg_head_chan = 4
    config.img_size = (160, 192, 224)
    return config
==================================================

üìÑ models/TransMorph_DCA/model.py
--------------------------------------------------
'''
TransMorph-DCA model
Chen, J., Liu, Y., He, Y., & Du, Y. (2023).
Deformable Cross-Attention Transformer for Medical Image Registration.
TransMorph: Transformer for unsupervised medical image registration.
In Machine Learning in Medical Imaging: 14th International Workshop, MLMI 2023,
Held in Conjunction with MICCAI 2023.

Created by:
Junyu Chen
jchen245@jhmi.edu
Johns Hopkins University
'''

import torch
import torch.nn as nn
import torch.utils.checkpoint as checkpoint
from timm.layers import DropPath, trunc_normal_, to_3tuple
from torch.distributions.normal import Normal
import torch.nn.functional as nnf
import numpy as np
import einops

import models.TransMorph_DCA.configs as configs


class LayerNormProxy(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        x = einops.rearrange(x, 'b c h w d -> b h w d c')
        x = self.norm(x)
        return einops.rearrange(x, 'b h w d c -> b c h w d')


class Offset_block0(nn.Module):
    def __init__(self, in_channels, num_heads, kernel_size=3):
        super().__init__()
        self.conv3d = nn.Conv3d(in_channels, num_heads, kernel_size=kernel_size, padding=kernel_size // 2, groups=num_heads, bias=False)
        self.LN = LayerNormProxy(num_heads)
        self.act = nn.GELU()
        self.offsetx = nn.Conv3d(num_heads, num_heads, kernel_size=1, bias=False)
        self.offsety = nn.Conv3d(num_heads, num_heads, kernel_size=1, bias=False)
        self.offsetz = nn.Conv3d(num_heads, num_heads, kernel_size=1, bias=False)
    def forward(self, x):
        x = self.conv3d(x)
        x = self.LN(x)
        x = self.act(x)
        dx = self.offsetx(x).unsqueeze(2)
        dy = self.offsety(x).unsqueeze(2)
        dz = self.offsetz(x).unsqueeze(2)
        x = torch.cat((dx, dy, dz), dim=2)
        return x


class Offset_block(nn.Module):
    def __init__(self, in_channels, num_heads, kernel_size=3):
        super().__init__()
        self.conv3d_1 = nn.Conv3d(in_channels, in_channels//2, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)
        self.conv3d_1.weight = nn.Parameter(Normal(0, 1e-5).sample(self.conv3d_1.weight.shape))

        self.conv3d_2 = nn.Conv3d(in_channels//2, in_channels//4, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)
        self.conv3d_2.weight = nn.Parameter(Normal(0, 1e-5).sample(self.conv3d_2.weight.shape))

        self.conv3d_3 = nn.Conv3d(in_channels // 4, 3*num_heads, kernel_size=1, bias=False)
        self.conv3d_3.weight = nn.Parameter(Normal(0, 1e-5).sample(self.conv3d_3.weight.shape))

        self.bn_1 = nn.BatchNorm3d(in_channels // 2)
        self.bn_2 = nn.BatchNorm3d(in_channels // 4)
        self.relu_1 = nn.ReLU(inplace=True)
        self.relu_2 = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv3d_1(x)
        x = self.bn_1(x)
        x = self.relu_1(x)
        x = self.conv3d_2(x)
        x = self.bn_2(x)
        x = self.relu_2(x)
        x = self.conv3d_3(x)
        return x


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class XMlp(nn.Module):
    def __init__(self, in_size, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, reduction=4, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

        kernel_size = 3
        self.avg_pool_1 = nn.AdaptiveAvgPool1d(1)
        self.avg_pool_2 = nn.AdaptiveAvgPool1d(1)
        self.se_conv_1 = nn.Sequential(
            nn.Conv3d(in_features*2, in_features//4, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_features//4, in_features//16, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_features//16, 1, kernel_size=1, padding=0, bias=False),
            nn.Sigmoid()
        )
        self.se_fc_2 = nn.Sequential(
            nn.Linear(in_features*2, in_features // reduction, bias=False),
            act_layer(),
            nn.Linear(in_features // reduction, in_features, bias=False),
            nn.Sigmoid()
        )
        self.H, self.W, self.T = in_size

    def forward(self, x, y):
        N, D, C = y.shape
        y_conv = torch.reshape(y.permute(0, 2, 1), (N, C, self.H, self.W, self.T))
        x_conv = torch.reshape(x.permute(0, 2, 1), (N, C, self.H, self.W, self.T))
        y_se = self.se_conv_1(torch.cat((x_conv, y_conv), dim=1)).view(N, 1, D).permute(0, 2, 1)
        x = x * y_se
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        x_se = self.avg_pool_2(torch.cat((x, y), dim=2).permute(0, 2, 1)).view(N, 2*C)
        x_se = self.se_fc_2(x_se).view(N, 1, C)
        x = x * x_se
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, L, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, window_size, C)
    """
    B, H, W, L, C = x.shape
    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], L // window_size[2], window_size[2], C)

    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0], window_size[1], window_size[2], C)
    return windows


def deform_window_partition(x, window_size):
    """
    Args:
        x: (Head_size, B, H, W, L, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, window_size, C)
    """
    B, Head, H, W, L, C = x.shape
    x = x.view(B, Head, H // window_size[0], window_size[0], W // window_size[1], window_size[1], L // window_size[2], window_size[2], C)

    windows = x.permute(0, 1, 2, 4, 6, 3, 5, 7, 8).contiguous().view(B, Head, -1, window_size[0], window_size[1], window_size[2], C)
    return windows


def window_reverse(windows, window_size, H, W, L):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
        L (int): Length of image
    Returns:
        x: (B, H, W, L, C)
    """
    B = int(windows.shape[0] / (H * W * L / window_size[0] / window_size[1] / window_size[2]))
    x = windows.view(B, H // window_size[0], W // window_size[1], L // window_size[2], window_size[0], window_size[1], window_size[2], -1)
    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, H, W, L, -1)#
    return x


class WindowAttentionReverse(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, rpe=True, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1 * 2*Wt-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords_t = torch.arange(self.window_size[2])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  # 3, Wh, Ww, Wt
        coords_flatten = torch.flatten(coords, 1)  # 3, Wh*Ww*Wt
        self.rpe = rpe
        if self.rpe:
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wh*Ww*Wt, Wh*Ww*Wt
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww*Wt, Wh*Ww*Wt, 3
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 2] += self.window_size[2] - 1
            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww*Wt, Wh*Ww*Wt
            self.register_buffer("relative_position_index", relative_position_index)

        self.f_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.f_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.m_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.m_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.m_proj = nn.Linear(dim, dim)
        self.f_proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """ Forward function.
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww, Wt*Ww) or None
        """
        mov, fix, dmov, dfix = x
        B_, N, C = mov.shape
        dmov_kv = self.m_kv(dmov).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dfix_kv = self.f_kv(dfix).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        mov_q = self.m_q(mov).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        fix_q = self.f_q(fix).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        mov_Q, dmov_K, dmov_V = mov_q[0], dmov_kv[0], dmov_kv[1]  # make torchscript happy (cannot use tensor as tuple)
        fix_Q, dfix_K, dfix_V = fix_q[0], dfix_kv[0], dfix_kv[1]  # make torchscript happy (cannot use tensor as tuple)

        mov_Q = mov_Q * self.scale
        fix_Q = fix_Q * self.scale
        mov_attn = (fix_Q @ dmov_K.transpose(-2, -1))
        fix_attn = (mov_Q @ dfix_K.transpose(-2, -1))

        if self.rpe:
            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1] * self.window_size[2],
                self.window_size[0] * self.window_size[1] * self.window_size[2], -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
            mov_attn = mov_attn + relative_position_bias.unsqueeze(0)
            fix_attn = fix_attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            mov_attn = mov_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            mov_attn = mov_attn.view(-1, self.num_heads, N, N)
            mov_attn = self.softmax(mov_attn)
            fix_attn = fix_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            fix_attn = fix_attn.view(-1, self.num_heads, N, N)
            fix_attn = self.softmax(fix_attn)
        else:
            mov_attn = self.softmax(mov_attn)
            fix_attn = self.softmax(fix_attn)

        mov_attn = self.attn_drop(mov_attn)
        fix_attn = self.attn_drop(fix_attn)

        mov = (mov_attn @ dmov_V).transpose(1, 2).reshape(B_, N, C)
        mov = self.m_proj(mov)
        mov = self.proj_drop(mov)

        fix = (fix_attn @ dfix_V).transpose(1, 2).reshape(B_, N, C)
        fix = self.f_proj(fix)
        fix = self.proj_drop(fix)

        return mov, fix


class WindowAttention(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, rpe=True, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1 * 2*Wt-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords_t = torch.arange(self.window_size[2])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  # 3, Wh, Ww, Wt
        coords_flatten = torch.flatten(coords, 1)  # 3, Wh*Ww*Wt
        self.rpe = rpe
        if self.rpe:
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wh*Ww*Wt, Wh*Ww*Wt
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww*Wt, Wh*Ww*Wt, 3
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 2] += self.window_size[2] - 1
            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww*Wt, Wh*Ww*Wt
            self.register_buffer("relative_position_index", relative_position_index)

        self.m_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.m_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.f_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.f_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.m_proj = nn.Linear(dim, dim)
        self.f_proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, mov, fix, dmov, dfix, mask=None):
        """ Forward function.
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww, Wt*Ww) or None
        """
        B_, N, C = mov.shape
        mov_kv = self.m_kv(mov).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        fix_kv = self.f_kv(fix).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dmov_q = self.m_q(dmov).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dfix_q = self.f_q(dfix).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dmov_Q, mov_K, mov_V = dmov_q[0], mov_kv[0], mov_kv[1]  # make torchscript happy (cannot use tensor as tuple)
        dfix_Q, fix_K, fix_V = dfix_q[0], fix_kv[0], fix_kv[1]  # make torchscript happy (cannot use tensor as tuple)

        dmov_Q = dmov_Q * self.scale
        dfix_Q = dfix_Q * self.scale
        mov_attn = (dfix_Q @ mov_K.transpose(-2, -1))
        fix_attn = (dmov_Q @ fix_K.transpose(-2, -1))

        if self.rpe:
            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1] * self.window_size[2],
                self.window_size[0] * self.window_size[1] * self.window_size[2], -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
            mov_attn = mov_attn + relative_position_bias.unsqueeze(0)
            fix_attn = fix_attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            mov_attn = mov_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            mov_attn = mov_attn.view(-1, self.num_heads, N, N)
            mov_attn = self.softmax(mov_attn)
            fix_attn = fix_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            fix_attn = fix_attn.view(-1, self.num_heads, N, N)
            fix_attn = self.softmax(fix_attn)
        else:
            mov_attn = self.softmax(mov_attn)
            fix_attn = self.softmax(fix_attn)

        mov_attn = self.attn_drop(mov_attn)
        fix_attn = self.attn_drop(fix_attn)

        mov = (mov_attn @ mov_V).transpose(1, 2).reshape(B_, N, C)
        mov = self.m_proj(mov)
        mov = self.proj_drop(mov)

        fix = (fix_attn @ fix_V).transpose(1, 2).reshape(B_, N, C)
        fix = self.f_proj(fix)
        fix = self.proj_drop(fix)

        return mov, fix
    
#========================ADDED TO CTCF========================
class SEToken(nn.Module):
    def __init__(self, dim, reduction=8):
        super().__init__()
        hidden = max(dim // reduction, 4)
        self.fc1 = nn.Linear(dim, hidden)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden, dim)
        self.gate = nn.Sigmoid()

    def forward(self, x):  # x: (B,L,C)
        s = x.mean(dim=1)  # (B,C)
        a = self.gate(self.fc2(self.act(self.fc1(s))))  # (B,C)
        return x * a.unsqueeze(1)
#=============================================================


class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, 
                 num_heads, 
                 window_size=(7, 7, 7), 
                 shift_size=(0, 0, 0),
                 mlp_ratio=4.,
                 # ADDED (CTCF)
                 use_se=False,
                 se_reduction=8,
                 # ===========
                 qkv_bias=True, 
                 qk_scale=None, 
                 rpe=True, 
                 drop=0., 
                 attn_drop=0., 
                 drop_path=0.,
                 act_layer=nn.GELU, 
                 norm_layer=nn.LayerNorm, 
                 img_size=(160, 160, 160), 
                 dwin_size=3):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        assert 0 <= min(self.shift_size) < min(self.window_size), "shift_size must in 0-window_size, shift_sz: {}, win_size: {}".format(self.shift_size, self.window_size)

        self.m_norm1 = norm_layer(dim)
        self.f_norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=self.window_size, num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, rpe=rpe, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.m_norm2 = norm_layer(dim)
        self.f_norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.m_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop) #XMlp(in_size=img_size,
        self.f_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)#XMlp(in_size=img_size,

        vectors = [torch.arange(0, s) for s in img_size]

        grids = torch.meshgrid(vectors, indexing='ij')
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)
        sample_grid1 = grid.cuda()
        sample_grid1.requires_grad = False
        sample_grid1 = sample_grid1.permute(0, 2, 3, 4, 1)
        sample_grid1 = sample_grid1[..., [2, 1, 0]]
        self.grid = window_partition(sample_grid1, self.window_size)
        self.grid.requires_grad = False
        nW, wW, wH, wD, gC = self.grid.shape
        self.nW = nW
        self.wW = wW
        self.wH = wH
        self.wD = wD
        self.grid = self.grid.view(1, nW, -1, 3)
        self.grid = torch.unsqueeze(self.grid, 0)

        self.offset_block_1 = Offset_block(self.dim*2, num_heads, dwin_size)
        self.offset_block_2 = Offset_block(self.dim*2, num_heads, dwin_size)

        self.H = None
        self.W = None
        self.T = None

        # ADDED (CTCF)
        self.use_se = use_se
        if self.use_se:
            self.se_m = SEToken(dim, se_reduction)
            self.se_f = SEToken(dim, se_reduction)
        # ============

    def forward(self, mov, fix, mask_matrix):
        H, W, T = self.H, self.W, self.T
        B, L, C = mov.shape
        assert L == H * W * T, "input feature has wrong size"

        mov_shortcut = mov
        fix_shortcut = fix

        mov = self.m_norm1(mov)
        mov = mov.view(B, H, W, T, C)

        fix = self.f_norm1(fix)
        fix = fix.view(B, H, W, T, C)

        # pad feature maps to multiples of window size
        pad_l = pad_t = pad_f = 0
        pad_r = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]
        pad_b = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]
        pad_h = (self.window_size[2] - T % self.window_size[2]) % self.window_size[2]
        mov = nnf.pad(mov, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = mov.shape
        fix = nnf.pad(fix, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = fix.shape

        # cyclic shift
        if min(self.shift_size) > 0:
            shifted_mov = torch.roll(mov, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))
            shifted_fix = torch.roll(fix, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))
            attn_mask = mask_matrix
        else:
            shifted_mov = mov
            shifted_fix = fix
            attn_mask = None

        offset_range = torch.tensor([Hp, Wp, Tp], device='cuda').reshape(1, 1, 3, 1, 1, 1) * 0.5
        off_mov = shifted_mov.permute(0, 4, 1, 2, 3)
        off_fix = shifted_fix.permute(0, 4, 1, 2, 3)
        offset_mov_org = self.offset_block_1(torch.cat((off_mov, off_fix), dim=1))
        offset_mov_org = offset_mov_org.tanh().reshape(B, self.num_heads, 3, Hp, Wp, Tp).mul(offset_range)
        offset_mov = deform_window_partition(offset_mov_org.permute(0, 1, 3, 4, 5, 2), self.window_size)
        offset_mov = torch.reshape(offset_mov, (B, self.num_heads, self.nW, self.wW * self.wH * self.wD, 3))
        offset_mov = offset_mov[..., [2, 1, 0]]
        offset_mov = torch.unsqueeze(offset_mov, 2)

        offset_fix_org = self.offset_block_2(torch.cat((off_fix, off_mov), dim=1))
        offset_fix_org = offset_fix_org.tanh().reshape(B, self.num_heads, 3, Hp, Wp, Tp).mul(offset_range)
        offset_fix = deform_window_partition(offset_fix_org.permute(0, 1, 3, 4, 5, 2), self.window_size)
        offset_fix = torch.reshape(offset_fix, (B, self.num_heads, self.nW, self.wW * self.wH * self.wD, 3))
        offset_fix = offset_fix[..., [2, 1, 0]]
        offset_fix = torch.unsqueeze(offset_fix, 2)

        dmov = torch.clone(shifted_mov)
        dfix = torch.clone(shifted_fix)

        # deformable window for moving image features
        offset_mov_ = offset_mov.repeat(1, 1, C // self.num_heads, 1, 1, 1).view(B, C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_mov_ = offset_mov_.view(B * C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_mov_ = torch.unsqueeze(offset_mov_, 1)
        offset_fix_ = offset_fix.repeat(1, 1, C // self.num_heads, 1, 1, 1).view(B, C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_fix_ = offset_fix_.view(B * C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_fix_ = torch.unsqueeze(offset_fix_, 1)
        mov_locs = self.grid.repeat(B*C, 1, 1, 1, 1) + offset_mov_
        fix_locs = self.grid.repeat(B*C, 1, 1, 1, 1) + offset_fix_
        # need to normalize grid values to [-1, 1] for resampler
        shape = [Tp, Hp, Wp]
        for i in range(len(shape)):
            mov_locs[..., i] = 2 * (mov_locs[..., i] / (shape[i] - 1) - 0.5)
            fix_locs[..., i] = 2 * (fix_locs[..., i] / (shape[i] - 1) - 0.5)

        dmov = dmov.permute(0, 4, 1, 2, 3)
        dmov = torch.reshape(dmov, (B * C, 1, Hp, Wp, Tp))
        dmov_windows = nnf.grid_sample(dmov, mov_locs, align_corners=False)
        dmov_windows = torch.reshape(dmov_windows, (B, C, self.nW, self.window_size[0], self.window_size[1], self.window_size[2])).permute(0, 2, 3, 4, 5, 1)
        dmov_windows = torch.reshape(dmov_windows, (-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C))

        # deformable window for moving image features
        dfix = dfix.permute(0, 4, 1, 2, 3)
        dfix = torch.reshape(dfix, (B * C, 1, Hp, Wp, Tp))
        dfix_windows = nnf.grid_sample(dfix, fix_locs, align_corners=False)
        dfix_windows = torch.reshape(dfix_windows, (B, C, self.nW, self.window_size[0], self.window_size[1], self.window_size[2])).permute(0, 2, 3, 4, 5, 1)
        dfix_windows = torch.reshape(dfix_windows, (-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C))

        # partition windows
        mov_windows = window_partition(shifted_mov, self.window_size)  # nW*B, window_size, window_size, C
        mov_windows = mov_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C)  # nW*B, window_size**3, C

        fix_windows = window_partition(shifted_fix, self.window_size)  # nW*B, window_size, window_size, C
        fix_windows = fix_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C)  # nW*B, window_size**3, C

        # Deformable Cross W-MSA/SW-MSA
        mov_attn_windows, fix_attn_windows = self.attn(mov_windows, fix_windows, dmov_windows, dfix_windows, mask=attn_mask)  # nW*B, window_size**3, C

        # merge windows
        mov_attn_windows = mov_attn_windows.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], C)
        shifted_mov = window_reverse(mov_attn_windows, self.window_size, Hp, Wp, Tp)  # B H' W' L' C

        fix_attn_windows = fix_attn_windows.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], C)
        shifted_fix = window_reverse(fix_attn_windows, self.window_size, Hp, Wp, Tp)  # B H' W' L' C

        # reverse cyclic shift
        if min(self.shift_size) > 0:
            mov = torch.roll(shifted_mov, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))
            fix = torch.roll(shifted_fix, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))
        else:
            mov, fix = shifted_mov, shifted_fix

        if pad_r > 0 or pad_b > 0:
            mov = mov[:, :H, :W, :T, :].contiguous()
            fix = fix[:, :H, :W, :T, :].contiguous()

        mov = mov.view(B, H * W * T, C)
        fix = fix.view(B, H * W * T, C)

        # FFN
        mov = mov_shortcut + self.drop_path(mov)
        fix = fix_shortcut + self.drop_path(fix)
        mov_norm = self.m_norm2(mov)
        fix_norm = self.f_norm2(fix)
        mov = mov + self.drop_path(self.m_mlp(mov_norm))#, fix_norm))
        fix = fix + self.drop_path(self.f_mlp(fix_norm))#, mov_norm))

        # SE block
        if self.use_se:
            mov = self.se_m(mov)
            fix = self.se_f(fix)
        return mov, fix


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.
    Args:
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, norm_layer=nn.LayerNorm, reduce_factor=2):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(8 * dim, (8//reduce_factor) * dim, bias=False)
        self.norm = norm_layer(8 * dim)


    def forward(self, x, H, W, T):
        """
        x: B, H*W*T, C
        """
        B, L, C = x.shape
        assert L == H * W * T, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0 and T % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, T, C)

        # padding
        pad_input = (H % 2 == 1) or (W % 2 == 1) or (T % 2 == 1)
        if pad_input:
            x = nnf.pad(x, (0, 0, 0, T % 2, 0, W % 2, 0, H % 2))

        x0 = x[:, 0::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x1 = x[:, 1::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x2 = x[:, 0::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x3 = x[:, 0::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x4 = x[:, 1::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x5 = x[:, 0::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x6 = x[:, 1::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x7 = x[:, 1::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)  # B H/2 W/2 T/2 8*C
        x = x.view(B, -1, 8 * C)  # B H/2*W/2*T/2 8*C

        x = self.norm(x)
        x = self.reduction(x)

        return x


class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.
    Args:
        dim (int): Number of feature channels
        depth (int): Depths of this stage.
        num_heads (int): Number of attention head.
        window_size (int): Local window size. Default: 7.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self,
                 dim,
                 depth,
                 num_heads,
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 # ADDED (CTCF)
                 use_se=False,
                 se_reduction=8,
                 # ===========
                 qkv_bias=True,
                 qk_scale=None,
                 rpe=True,
                 drop=0.,
                 attn_drop=0.,
                 drop_path=0.,
                 norm_layer=nn.LayerNorm,
                 downsample=None,
                 use_checkpoint=False,
                 pat_merg_rf=2,
                 img_size=(160, 160, 160),
                 dwin_size=3):
        super().__init__()
        self.window_size = window_size
        self.shift_size = (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2)
        self.depth = depth
        self.use_checkpoint = use_checkpoint
        self.pat_merg_rf = pat_merg_rf
        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=(0, 0, 0) if (i % 2 == 0) else (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2),
                mlp_ratio=mlp_ratio,
                # ADDED (CTCF)
                use_se=use_se,
                se_reduction=se_reduction,
                # ===========
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                rpe=rpe,
                drop=drop,
                attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                norm_layer=norm_layer,
                img_size=img_size,
                dwin_size=dwin_size)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(dim=dim, norm_layer=norm_layer, reduce_factor=self.pat_merg_rf)
        else:
            self.downsample = None

    def forward(self, x, H, W, T):
        """ Forward function.
        Args:
            x: Input feature, tensor size (B, H*W*T, C).
            H, W, T: Spatial resolution of the input feature.
        """
        mov, fix = x
        # calculate attention mask for SW-MSA
        Hp = int(np.ceil(H / self.window_size[0])) * self.window_size[0]
        Wp = int(np.ceil(W / self.window_size[1])) * self.window_size[1]
        Tp = int(np.ceil(T / self.window_size[2])) * self.window_size[2]
        img_mask = torch.zeros((1, Hp, Wp, Tp, 1), device=mov.device)  # 1 Hp Wp 1
        h_slices = (slice(0, -self.window_size[0]),
                    slice(-self.window_size[0], -self.shift_size[0]),
                    slice(-self.shift_size[0], None))
        w_slices = (slice(0, -self.window_size[1]),
                    slice(-self.window_size[1], -self.shift_size[1]),
                    slice(-self.shift_size[1], None))
        t_slices = (slice(0, -self.window_size[2]),
                    slice(-self.window_size[2], -self.shift_size[2]),
                    slice(-self.shift_size[2], None))
        cnt = 0
        for h in h_slices:
            for w in w_slices:
                for t in t_slices:
                    img_mask[:, h, w, t, :] = cnt
                    cnt += 1

        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
        mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2])
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

        x_m, x_f = mov, fix
        for blk in self.blocks:
            blk.H, blk.W, blk.T = H, W, T
            if self.use_checkpoint:
                x_m, x_f = checkpoint.checkpoint(blk, x_m, x_f, attn_mask)
            else:
                x_m, x_f = blk(x_m, x_f, attn_mask)
        mov, fix = x_m, x_f
        if self.downsample is not None:
            mov_down = self.downsample(mov, H, W, T)
            fix_down = self.downsample(fix, H, W, T)
            Wh, Ww, Wt = (H + 1) // 2, (W + 1) // 2, (T + 1) // 2
            return (mov, fix), H, W, T, (mov_down, fix_down), Wh, Ww, Wt
        else:
            return (mov, fix), H, W, T, (mov, fix), H, W, T


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    Args:
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        patch_size = to_3tuple(patch_size)
        self.patch_size = patch_size

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        """Forward function."""
        # padding
        _, _, H, W, T = x.size()
        if T % self.patch_size[2] != 0:
            x = nnf.pad(x, (0, self.patch_size[2] - T % self.patch_size[2]))
        if W % self.patch_size[1] != 0:
            x = nnf.pad(x, (0, 0, 0, self.patch_size[1] - W % self.patch_size[1]))
        if H % self.patch_size[0] != 0:
            x = nnf.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))

        x = self.proj(x)  # B C Wh Ww Wt
        if self.norm is not None:
            Wh, Ww, Wt = x.size(2), x.size(3), x.size(4)
            x = x.flatten(2).transpose(1, 2)
            x = self.norm(x)
            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww, Wt)

        return x


class SinusoidalPositionEmbedding(nn.Module):
    '''
    Rotary Position Embedding
    '''
    def __init__(self,):
        super(SinusoidalPositionEmbedding, self).__init__()

    def forward(self, x):
        batch_sz, n_patches, hidden = x.shape
        position_ids = torch.arange(0, n_patches).float().cuda()
        indices = torch.arange(0, hidden//2).float().cuda()
        indices = torch.pow(10000.0, -2 * indices / hidden)
        embeddings = torch.einsum('b,d->bd', position_ids, indices)
        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)
        embeddings = torch.reshape(embeddings, (1, n_patches, hidden))
        return embeddings


class SinPositionalEncoding3D(nn.Module):
    def __init__(self, channels):
        """
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        """
        super(SinPositionalEncoding3D, self).__init__()
        channels = int(np.ceil(channels/6)*2)
        if channels % 2:
            channels += 1
        self.channels = channels
        self.inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))
        #self.register_buffer('inv_freq', inv_freq)

    def forward(self, tensor):
        """
        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)
        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)
        """
        tensor = tensor.permute(0, 2, 3, 4, 1)
        if len(tensor.shape) != 5:
            raise RuntimeError("The input tensor has to be 5d!")
        batch_size, x, y, z, orig_ch = tensor.shape
        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())
        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())
        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())
        sin_inp_x = torch.einsum("i,j->ij", pos_x, self.inv_freq)
        sin_inp_y = torch.einsum("i,j->ij", pos_y, self.inv_freq)
        sin_inp_z = torch.einsum("i,j->ij", pos_z, self.inv_freq)
        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1).unsqueeze(1)
        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1).unsqueeze(1)
        emb_z = torch.cat((sin_inp_z.sin(), sin_inp_z.cos()), dim=-1)
        emb = torch.zeros((x,y,z,self.channels*3),device=tensor.device).type(tensor.type())
        emb[:,:,:,:self.channels] = emb_x
        emb[:,:,:,self.channels:2*self.channels] = emb_y
        emb[:,:,:,2*self.channels:] = emb_z
        emb = emb[None,:,:,:,:orig_ch].repeat(batch_size, 1, 1, 1, 1)
        return emb.permute(0, 4, 1, 2, 3)


class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030
    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (tuple): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(self, pretrain_img_size=224,
                 patch_size=4,
                 in_chans=3,
                 embed_dim=96,
                 depths=[2, 2, 6, 2],
                 num_heads=[3, 6, 12, 24],
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 # ADDED (CTCF)
                 use_se=False,
                 se_reduction=8,
                 # ===========
                 qkv_bias=True,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.2,
                 norm_layer=nn.LayerNorm,
                 ape=False,
                 spe=False,
                 rpe=True,
                 patch_norm=True,
                 out_indices=(0, 1, 2, 3),
                 frozen_stages=-1,
                 use_checkpoint=False,
                 pat_merg_rf=2,
                 img_size=(160, 160, 160),
                 dwin_size=(3, 3, 3)):
        super().__init__()
        self.pretrain_img_size = pretrain_img_size
        self.num_layers = len(depths)
        print('Depths: {}'.format(depths))
        print('DWin kernel size: {}'.format(dwin_size))
        self.embed_dim = embed_dim
        self.ape = ape
        self.spe = spe
        self.rpe = rpe
        self.patch_norm = patch_norm
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            patch_size=patch_size, in_chans=1, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)

        # absolute position embedding
        if self.ape:
            pretrain_img_size = to_3tuple(self.pretrain_img_size)
            patch_size = to_3tuple(patch_size)
            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1], pretrain_img_size[2] // patch_size[2]]

            self.absolute_pos_embed = nn.Parameter(
                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1], patches_resolution[2]))
            trunc_normal_(self.absolute_pos_embed, std=.02)
        elif self.spe:
            self.pos_embd = SinPositionalEncoding3D(embed_dim).cuda()
            #self.pos_embd = SinusoidalPositionEmbedding().cuda()
        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                                depth=depths[i_layer],
                                num_heads=num_heads[i_layer],
                                window_size=window_size,
                                mlp_ratio=mlp_ratio,
                                # ADDED (CTCF)
                                use_se=use_se,
                                se_reduction=se_reduction,
                                # ===========
                                qkv_bias=qkv_bias,
                                rpe=rpe,
                                qk_scale=qk_scale,
                                drop=drop_rate,
                                attn_drop=attn_drop_rate,
                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                                norm_layer=norm_layer,
                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                                use_checkpoint=use_checkpoint,
                               pat_merg_rf=pat_merg_rf,
                               img_size=(img_size[0]//4//2**i_layer, img_size[1]//4//2**i_layer, img_size[2]//4//2**i_layer),
                               dwin_size=dwin_size[i_layer])
            self.layers.append(layer)

        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
        self.num_features = num_features

        # add a norm layer for each output
        for i_layer in out_indices:
            layer = norm_layer(num_features[i_layer])
            layer_name = f'm_norm{i_layer}'
            self.add_module(layer_name, layer)
            layer = norm_layer(num_features[i_layer])
            layer_name = f'f_norm{i_layer}'
            self.add_module(layer_name, layer)

        self._freeze_stages()

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False

        if self.frozen_stages >= 1 and self.ape:
            self.absolute_pos_embed.requires_grad = False

        if self.frozen_stages >= 2:
            self.pos_drop.eval()
            for i in range(0, self.frozen_stages - 1):
                m = self.layers[i]
                m.eval()
                for param in m.parameters():
                    param.requires_grad = False

    def init_weights(self, pretrained=None):
        """Initialize the weights in backbone.
        Args:
            pretrained (str, optional): Path to pre-trained weights.
                Defaults to None.
        """

        def _init_weights(m):
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

        if isinstance(pretrained, str):
            self.apply(_init_weights)
        elif pretrained is None:
            self.apply(_init_weights)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, x):
        mov, fix = x
        """Forward function."""
        mov = self.patch_embed(mov)
        fix = self.patch_embed(fix)

        Wh, Ww, Wt = mov.size(2), mov.size(3), mov.size(4)

        if self.ape:
            # interpolate the position embedding to the corresponding size
            absolute_pos_embed = nnf.interpolate(self.absolute_pos_embed, size=(Wh, Ww, Wt), mode='trilinear')
            mov = (mov + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww*Wt C
            fix = (fix + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww*Wt C
        elif self.spe:
            mov = (mov + self.pos_embd(mov)).flatten(2).transpose(1, 2)
            fix = (fix + self.pos_embd(mov)
[... –æ–±—Ä–µ–∑–∞–Ω–æ ...]
==================================================

üìÑ models/UTSRMorph/__init__.py
--------------------------------------------------

==================================================

üìÑ models/UTSRMorph/configs.py
--------------------------------------------------
import ml_collections

'''
********************************************************
                   Swin Transformer
********************************************************
if_transskip (bool): Enable skip connections from Transformer Blocks
if_convskip (bool): Enable skip connections from Convolutional Blocks
patch_size (int | tuple(int)): Patch size. Default: 4
in_chans (int): Number of input image channels. Default: 2 (for moving and fixed images)
embed_dim (int): Patch embedding dimension. Default: 96
depths (tuple(int)): Depth of each Swin Transformer layer.
num_heads (tuple(int)): Number of attention heads in different layers.
window_size (tuple(int)): Image size should be divisible by window size, 
                     e.g., if image has a size of (160, 192, 224), then the window size can be (5, 6, 7)
mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
pat_merg_rf (int): Embed_dim reduction factor in patch merging, e.g., N*C->N/4*C if set to four. Default: 4. 
qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
drop_rate (float): Dropout rate. Default: 0
drop_path_rate (float): Stochastic depth rate. Default: 0.1
ape (bool): Enable learnable position embedding. Default: False
spe (bool): Enable sinusoidal position embedding. Default: False
patch_norm (bool): If True, add normalization after patch embedding. Default: True
use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False 
                       (Carried over from Swin Transformer, it is not needed)
out_indices (tuple(int)): Indices of Transformer blocks to output features. Default: (0, 1, 2, 3)
reg_head_chan (int): Number of channels in the registration head (i.e., the final convolutional layer) 
img_size (int | tuple(int)): Input image size, e.g., (160, 192, 224)
'''

def get_UTSRMorph_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 96
    config.depths = (2, 2, 4, 2)
    config.num_heads = (4, 4, 8, 8)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2, 3)
    config.reg_head_chan = 16
    config.img_size = (160, 192, 224)
    return config


def get_UTSRMorph_debug_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 40
    config.depths = (2, 2, 18, 2)
    config.num_heads = (4, 4, 8, 16)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2, 3)
    config.reg_head_chan = 4
    config.img_size = (160, 192, 224)
    return config


def get_UTSRMorphLarge_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 160
    config.depths = (2, 2, 18, 2)
    config.num_heads = (4, 4, 8, 16)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2, 3)
    config.reg_head_chan = 16
    config.img_size = (160, 192, 224)
    return config
==================================================

üìÑ models/UTSRMorph/model.py
--------------------------------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from timm.layers import DropPath, trunc_normal_, to_3tuple
from torch.distributions.normal import Normal
import torch.nn.functional as nnf
from typing import Tuple, Union
from einops import rearrange
import numpy as np
import math

import models.UTSRMorph.configs as configs


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
    

class CA(nn.Module):
    """Channel attention used in RCAN.
    Args:
        num_feat (int): Channel number of intermediate features.
        squeeze_factor (int): Channel squeeze factor. Default: 16.
    """

    def __init__(self, num_feat, squeeze_factor=16):
        super(CA, self).__init__()
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Conv3d(num_feat, num_feat // squeeze_factor, 1, padding=0),
            nn.ReLU(inplace=True),
            nn.Conv3d(num_feat // squeeze_factor, num_feat, 1, padding=0),
            nn.Sigmoid())

    def forward(self, x):
        y = self.attention(x)
        return x * y
    

class CAB(nn.Module):

    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):
        super(CAB, self).__init__()

        self.cab = nn.Sequential(
            nn.Conv3d(num_feat, num_feat // compress_ratio, 3, 1, 1),
            nn.GELU(),
            nn.Conv3d(num_feat // compress_ratio, num_feat, 3, 1, 1),
            CA(num_feat, squeeze_factor)
            )

    def forward(self, x):
        return self.cab(x)
    

def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, L, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, window_size, C)
    """
    B, H, W, L, C = x.shape
    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], L // window_size[2], window_size[2], C)

    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0], window_size[1], window_size[2], C)
    return windows


def window_reverse(windows, window_size, H, W, L):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
        L (int): Length of image
    Returns:
        x: (B, H, W, L, C)
    """
    B = int(windows.shape[0] / (H * W * L / window_size[0] / window_size[1] / window_size[2]))
    x = windows.view(B, H // window_size[0], W // window_size[1], L // window_size[2], window_size[0], window_size[1], window_size[2], -1)
    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, H, W, L, -1)
    return x


def filter_dilated_rows(
    tensor: torch.Tensor,
    dilation: Tuple[int, int, int],
    dilated_kernel_size: Tuple[int, int, int],
    kernel_size: Tuple[int, int, int],
):
    """
    A helper function that removes extra rows created during the process of
    implementing dilation.

    Args:
        tensor: A tensor containing the output slices resulting from unfolding
                the input tensor to `unfold3d()`.
                Shape is ``(B, C, D_out, H_out, W_out, dilated_kernel_size[0],
                dilated_kernel_size[1], dilated_kernel_size[2])``.
        dilation: The dilation given to `unfold3d()`.
        dilated_kernel_size: The size of the dilated kernel.
        kernel_size: The size of the kernel given to `unfold3d()`.

    Returns:
        A tensor of shape (B, C, D_out, H_out, W_out, kernel_size[0], kernel_size[1], kernel_size[2])
        For D_out, H_out, W_out definitions see :class:`torch.nn.Unfold`.

    Example:
        >>> tensor = torch.zeros([1, 1, 3, 3, 3, 5, 5, 5])
        >>> dilation = (2, 2, 2)
        >>> dilated_kernel_size = (5, 5, 5)
        >>> kernel_size = (3, 3, 3)
        >>> filter_dilated_rows(tensor, dilation, dilated_kernel_size, kernel_size).shape
        torch.Size([1, 1, 3, 3, 3, 3, 3, 3])
    """

    kernel_rank = len(kernel_size)
    indices_to_keep = [list(range(0, dilated_kernel_size[i], dilation[i])) for i in range(kernel_rank)]
    tensor_np = tensor.numpy()
    axis_offset = len(tensor.shape) - kernel_rank

    for dim in range(kernel_rank):
        tensor_np = np.take(tensor_np, indices_to_keep[dim], axis=axis_offset + dim)

    return torch.Tensor(tensor_np)


def unfold3d(
    tensor: torch.Tensor,
    *,
    kernel_size: Union[int, Tuple[int, int, int]],
    padding: Union[int, Tuple[int, int, int]] = 0,
    stride: Union[int, Tuple[int, int, int]] = 1,
    dilation: Union[int, Tuple[int, int, int]] = 1,
):
    r"""
    Extracts sliding local blocks from an batched input tensor.

    :class:`torch.nn.Unfold` only supports 4D inputs (batched image-like tensors).
    This method implements the same action for 5D inputs

    Args:
        tensor: An input tensor of shape ``(B, C, D, H, W)``.
        kernel_size: the size of the sliding blocks
        padding: implicit zero padding to be added on both sides of input
        stride: the stride of the sliding blocks in the input spatial dimensions
        dilation: the spacing between the kernel points.

    Returns:
        A tensor of shape ``(B, C * np.product(kernel_size), L)``, where L - output spatial dimensions.
        See :class:`torch.nn.Unfold` for more details

    Example:
        >>> B, C, D, H, W = 3, 4, 5, 6, 7
        >>> tensor = torch.arange(1, B*C*D*H*W + 1.).view(B, C, D, H, W)
        >>> unfold3d(tensor, kernel_size=2, padding=0, stride=1).shape
        torch.Size([3, 32, 120])
    """

    if len(tensor.shape) != 5:
        raise ValueError(
            f"Input tensor must be of the shape [B, C, D, H, W]. Got{tensor.shape}"
        )

    if isinstance(kernel_size, int):
        kernel_size = (kernel_size, kernel_size, kernel_size)

    if isinstance(padding, int):
        padding = (padding, padding, padding)

    if isinstance(stride, int):
        stride = (stride, stride, stride)

    if isinstance(dilation, int):
        dilation = (dilation, dilation, dilation)

    if padding == "same":
        total_pad_D = dilation[0] * (kernel_size[0] - 1)
        total_pad_H = dilation[1] * (kernel_size[1] - 1)
        total_pad_W = dilation[2] * (kernel_size[2] - 1)
        pad_D_left = math.floor(total_pad_D / 2)
        pad_D_right = total_pad_D - pad_D_left
        pad_H_left = math.floor(total_pad_H / 2)
        pad_H_right = total_pad_H - pad_H_left
        pad_W_left = math.floor(total_pad_W / 2)
        pad_W_right = total_pad_W - pad_W_left

    elif padding == "valid":
        pad_D_left, pad_D_right, pad_W_left, pad_W_right, pad_H_left, pad_H_right = (
            0,
            0,
            0,
            0,
            0,
            0,
        )
    else:
        pad_D_left, pad_D_right, pad_H_left, pad_H_right, pad_W_left, pad_W_right = (
            padding[0],
            padding[0],
            padding[1],
            padding[1],
            padding[2],
            padding[2],
        )

    batch_size, channels, _, _, _ = tensor.shape

    # Input shape: (B, C, D, H, W)
    tensor = F.pad(
        tensor,
        (pad_W_left, pad_W_right, pad_H_left, pad_H_right, pad_D_left, pad_D_right),
    )
    # Output shape: (B, C, D+pad_W_left+pad_W_right, H+pad_H_left+pad_H_right, W+pad_D_left+pad_D_right)

    dilated_kernel_size = (
        kernel_size[0] + (kernel_size[0] - 1) * (dilation[0] - 1),
        kernel_size[1] + (kernel_size[1] - 1) * (dilation[1] - 1),
        kernel_size[2] + (kernel_size[2] - 1) * (dilation[2] - 1),
    )

    tensor = tensor.unfold(dimension=2, size=dilated_kernel_size[0], step=stride[0])
    tensor = tensor.unfold(dimension=3, size=dilated_kernel_size[1], step=stride[1])
    tensor = tensor.unfold(dimension=4, size=dilated_kernel_size[2], step=stride[2])

    if dilation != (1, 1, 1):
        tensor = filter_dilated_rows(tensor, dilation, dilated_kernel_size, kernel_size)

    # Output shape: (B, C, D_out, H_out, W_out, kernel_size[0], kernel_size[1], kernel_size[2])
    # For D_out, H_out, W_out definitions see :class:`torch.nn.Unfold`

    tensor = tensor.permute(0, 2, 3, 4, 1, 5, 6, 7)
    # Output shape: (B, D_out, H_out, W_out, C, kernel_size[0], kernel_size[1], kernel_size[2])

    tensor = tensor.reshape(batch_size, -1, channels * np.prod(kernel_size)).transpose(1, 2)
    # Output shape: (B, D_out * H_out * W_out, C * kernel_size[0] * kernel_size[1] * kernel_size[2]

    return tensor


class OAB(nn.Module):
    # overlapping cross-attention block

    def __init__(self, dim,
                #input_resolution,
                window_size,
                overlap_ratio,
                num_heads,
                qkv_bias=True,
                qk_scale=None,
                mlp_ratio=2,
                norm_layer=nn.LayerNorm,
                rpe=True
                ):

        super().__init__()
        self.dim = dim
        #self.input_resolution = input_resolution
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5
        self.overlap_win_size = int(window_size * overlap_ratio) + window_size

        self.norm1 = norm_layer(dim)
        self.qkv = nn.Linear(dim, dim * 3,  bias=qkv_bias)
        #self.unfold = nn.Unfold(kernel_size=(self.overlap_win_size, self.overlap_win_size, self.overlap_win_size), stride=window_size, padding=(self.overlap_win_size-window_size)//2)

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((window_size + self.overlap_win_size - 1) * (window_size + self.overlap_win_size - 1) * (window_size + self.overlap_win_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH
        # get pair-wise relative position index for each token inside the window
        window_size_ori = self.window_size
        window_size_ext = self.overlap_win_size

        coords_h = torch.arange(window_size_ori)
        coords_w = torch.arange(window_size_ori)
        coords_t = torch.arange(window_size_ori)
        coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  #3, ws, ws
        coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws

        coords_h = torch.arange(window_size_ext)
        coords_w = torch.arange(window_size_ext)
        coords_t = torch.arange(window_size_ext)
        coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  # 3, wse, wse
        coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse

        relative_coords = coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]   # 3, ws*ws, wse*wse

        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # ws*ws, wse*wse, 2
        relative_coords[:, :, 0] += window_size_ori - window_size_ext + 1  # shift to start from 0
        relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1
        relative_coords[:, :, 2] += window_size_ori - window_size_ext + 1

        relative_coords[:, :, 0] *= (window_size_ori + window_size_ext - 1) * (window_size_ori + window_size_ext - 1)
        relative_coords[:, :, 1] *= window_size_ori + window_size_ext - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer("relative_position_index_OAB", relative_position_index)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

        self.proj = nn.Linear(dim,dim)

        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU)
        self.H = None
        self.W = None
        self.T = None

    def forward(self, x, mask=None):
        H, W, T = self.H, self.W, self.T
        B, L, C = x.shape

        assert L == H * W * T, "input feature has wrong size"
        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, T, C)

        pad_l = pad_t = pad_f = 0
        pad_r = (self.window_size - H % self.window_size) % self.window_size
        pad_b = (self.window_size- W % self.window_size) % self.window_size
        pad_h = (self.window_size - T % self.window_size) % self.window_size
        x = nnf.pad(x, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = x.shape
        qkv = self.qkv(x).reshape(B, Hp, Wp, Tp, 3, C).permute(4, 0, 5, 1, 2, 3) # 3, b, c, h, w, t
        q = qkv[0].permute(0, 2, 3, 4, 1) # b, h, w, T, c
        kv = torch.cat((qkv[1], qkv[2]), dim=1) # b, 2*c, h, w, T

        # partition windows
        q_windows = window_partition(q, [self.window_size,self.window_size,self.window_size])  # nw*b, window_size, window_size, c
        q_windows = q_windows.view(-1, self.window_size * self.window_size * self.window_size, C)  # nw*b, window_size*window_size, c

        kv_windows = unfold3d(kv, kernel_size=self.overlap_win_size, stride=self.window_size, padding=int((self.window_size)/2)) # b, c*w*w*W, nw

        kv_windows = rearrange(kv_windows, 'b (nc ch owh oww owt) nw -> nc (b nw) (owh oww owt) ch', nc=2, ch=C, owh=self.overlap_win_size, oww=self.overlap_win_size, owt=self.overlap_win_size).contiguous() # 2, nw*b, ow*ow, c
        k_windows, v_windows = kv_windows[0], kv_windows[1] # nw*b, ow*ow*ow, c

        b_, nq, _ = q_windows.shape
        _, n, _ = k_windows.shape
        d = self.dim // self.num_heads

        q = q_windows.reshape(b_, nq, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, nq, d
        k = k_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d
        v = v_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index_OAB.view(-1)].view(
                self.window_size * self.window_size * self.window_size, self.overlap_win_size * self.overlap_win_size * self.overlap_win_size, -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
        attn = attn + relative_position_bias.unsqueeze(0)


        attn = self.softmax(attn)
        attn_windows = (attn @ v).transpose(1, 2).reshape(b_, nq, self.dim)

        # merge windows
        #attn_windows = attn_windows.view(-1, self.window_size, self.window_size, self.window_size, self.dim)
        #x = window_reverse(attn_windows, self.window_size, H, W, T)  # b h w t c
        #x = x.view(B, H * W * T, self.dim)

        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, [self.window_size,self.window_size,self.window_size], Hp, Wp, Tp)  # B H' W' L' C
        x = shifted_x

        if pad_r > 0 or pad_b > 0 or pad_h > 0:
            x = x[:, :H, :W, :T, :].contiguous()

        x = x.view(B, H * W * T, C)
        x = self.proj(x) + shortcut

        x = x + self.mlp(self.norm2(x))
        return x
    

class WindowAttention(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, rpe=True, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1 * 2*Wt-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords_t = torch.arange(self.window_size[2])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  # 3, Wh, Ww, Wt
        coords_flatten = torch.flatten(coords, 1)  # 3, Wh*Ww*Wt
        self.rpe = rpe
        if self.rpe:
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wh*Ww*Wt, Wh*Ww*Wt
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww*Wt, Wh*Ww*Wt, 3
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 2] += self.window_size[2] - 1
            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww*Wt, Wh*Ww*Wt
            self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """ Forward function.
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww, Wt*Ww) or None
        """
        B_, N, C = x.shape #(num_windows*B, Wh*Ww*Wt, C)
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        if self.rpe:
            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1] * self.window_size[2],
                self.window_size[0] * self.window_size[1] * self.window_size[2], -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
            attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, num_heads, window_size=(7, 7, 7), shift_size=(0, 0, 0),
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, rpe=True, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        assert 0 <= min(self.shift_size) < min(self.window_size), "shift_size must in 0-window_size, shift_sz: {}, win_size: {}".format(self.shift_size, self.window_size)

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=self.window_size, num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, rpe=rpe, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
        self.conv_block = CAB(num_feat=self.dim, compress_ratio=3, squeeze_factor=30)
        self.H = None
        self.W = None
        self.T = None

    def forward(self, x, mask_matrix):
        H, W, T = self.H, self.W, self.T
        B, L, C = x.shape
        assert L == H * W * T, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, T, C)
        # Conv_X
        conv_x = self.conv_block(x.permute(0, 4, 1, 2, 3))
        conv_x = conv_x.permute(0, 2, 3, 4, 1).contiguous().view(B, H * W * T, C)
        # pad feature maps to multiples of window size
        pad_l = pad_t = pad_f = 0
        pad_r = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]
        pad_b = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]
        pad_h = (self.window_size[2] - T % self.window_size[2]) % self.window_size[2]
        x = nnf.pad(x, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = x.shape

        # cyclic shift
        if min(self.shift_size) > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))
            attn_mask = mask_matrix
        else:
            shifted_x = x
            attn_mask = None

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C)  # nW*B, window_size*window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], C)
        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp, Tp)  # B H' W' L' C

        # reverse cyclic shift
        if min(self.shift_size) > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))
        else:
            x = shifted_x

        if pad_r > 0 or pad_b > 0 or pad_h > 0:
            x = x[:, :H, :W, :T, :].contiguous()

        x = x.view(B, H * W * T, C)

        # FFN
        # FFN
        x = shortcut + self.drop_path(x) + conv_x * 0.01
        #x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.
    Args:
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, norm_layer=nn.LayerNorm, reduce_factor=2):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(8 * dim, (8//reduce_factor) * dim, bias=False)
        self.norm = norm_layer(8 * dim)


    def forward(self, x, H, W, T):
        """
        x: B, H*W*T, C
        """
        B, L, C = x.shape
        assert L == H * W * T, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0 and T % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, T, C)

        # padding
        pad_input = (H % 2 == 1) or (W % 2 == 1) or (T % 2 == 1)
        if pad_input:
            x = nnf.pad(x, (0, 0, 0, T % 2, 0, W % 2, 0, H % 2))

        x0 = x[:, 0::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x1 = x[:, 1::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x2 = x[:, 0::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x3 = x[:, 0::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x4 = x[:, 1::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x5 = x[:, 0::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x6 = x[:, 1::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x7 = x[:, 1::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)  # B H/2 W/2 T/2 8*C
        x = x.view(B, -1, 8 * C)  # B H/2*W/2*T/2 8*C

        x = self.norm(x)
        x = self.reduction(x)

        return x


class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.
    Args:
        dim (int): Number of feature channels
        depth (int): Depths of this stage.
        num_heads (int): Number of attention head.
        window_size (int): Local window size. Default: 7.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """
    def __init__(self,
                 dim,
                 depth,
                 num_heads,
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 rpe=True,
                 drop=0.,
                 attn_drop=0.,
                 drop_path=0.,
                 norm_layer=nn.LayerNorm,
                 downsample=None,
                 use_checkpoint=False,
                 pat_merg_rf=2,):
        super().__init__()
        self.window_size = window_size
        self.shift_size = (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2)
        self.depth = depth
        self.use_checkpoint = use_checkpoint
        self.pat_merg_rf = pat_merg_rf

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=(0, 0, 0) if (i % 2 == 0) else (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2),
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                rpe=rpe,
                drop=drop,
                attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                norm_layer=norm_layer,)
            for i in range(depth)])

        self.overlap_attn = OAB(
            dim=dim,
            # input_resolution=input_resolution,
            window_size=4,
            overlap_ratio=0.5,
            num_heads=4,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            mlp_ratio=mlp_ratio,
            norm_layer=norm_layer
        )


        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(dim=dim, norm_layer=norm_layer, reduce_factor=self.pat_merg_rf)
        else:
            self.downsample = None

    def forward(self, x, H, W, T):
        """ Forward function.
        Args:
            x: Input feature, tensor size (B, H*W*T, C).
            H, W: Spatial resolution of the input feature.
        """
        # calculate attention mask for SW-MSA
        Hp = int(np.ceil(H / self.window_size[0])) * self.window_size[0]
        Wp = int(np.ceil(W / self.window_size[1])) * self.window_size[1]
        Tp = int(np.ceil(T / self.window_size[2])) * self.window_size[2]
        img_mask = torch.zeros((1, Hp, Wp, Tp, 1), device=x.device)  # 1 Hp Wp 1
        h_slices = (slice(0, -self.window_size[0]),
                    slice(-self.window_size[0], -self.shift_size[0]),
                    slice(-self.shift_size[0], None))
        w_slices = (slice(0, -self.window_size[1]),
                    slice(-self.window_size[1], -self.shift_size[1]),
                    slice(-self.shift_size[1], None))
        t_slices = (slice(0, -self.window_size[2]),
                    slice(-self.window_size[2], -self.shift_size[2]),
                    slice(-self.shift_size[2], None))
        cnt = 0
        for h in h_slices:
            for w in w_slices:
                for t in t_slices:
                    img_mask[:, h, w, t, :] = cnt
                    cnt += 1

        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
        mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2])
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

        for blk in self.blocks:
            blk.H, blk.W, blk.T = H, W, T
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x, attn_mask)
            else:
                x = blk(x, attn_mask)
        self.overlap_attn.H, self.overlap_attn.W, self.overlap_attn.T = H, W, T
        if self.use_checkpoint:
            x = checkpoint.checkpoint(self.overlap_attn, x, None)
        else:
            x = self.overlap_attn(x, None)

        if self.downsample is not None:
            x_down = self.downsample(x, H, W, T)
            Wh, Ww, Wt = (H + 1) // 2, (W + 1) // 2, (T + 1) // 2
            return x, H, W, T, x_down, Wh, Ww, Wt
        else:
            return x, H, W, T, x, H, W, T


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    Args:
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        patch_size = to_3tuple(patch_size)
        self.patch_size = patch_size

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        """Forward function."""
        # padding
        _, _, H, W, T = x.size()
        if T % self.patch_size[2] != 0:
            x = nnf.pad(x, (0, self.patch_size[2] - T % self.patch_size[2]))
        if W % self.patch_size[1] != 0:
            x = nnf.pad(x, (0, 0, 0, self.patch_size[1] - W % self.patch_size[1]))
        if H % self.patch_size[0] != 0:
            x = nnf.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))

        x = self.proj(x)  # B C Wh Ww Wt
        if self.norm is not None:
            Wh, Ww, Wt = x.size(2), x.size(3), x.size(4)
            x = x.flatten(2).transpose(1, 2)
            x = self.norm(x)
            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww, Wt)
        return x


class SinusoidalPositionEmbedding(nn.Module):
    '''
    Rotary Position Embedding
    '''
    def __init__(self,):
        super(SinusoidalPositionEmbedding, self).__init__()

    def forward(self, x):
        batch_sz, n_patches, hidden = x.shape
        position_ids = torch.arange(0, n_patches).float().cuda()
        indices = torch.arange(0, hidden//2).float().cuda()
        indices = torch.pow(10000.0, -2 * indices / hidden)
        embeddings = torch.einsum('b,d->bd', position_ids, indices)
        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)
        embeddings = torch.reshape(embeddings, (1, n_patches, hidden))
        return embeddings


class SinPositionalEncoding3D(nn.Module):
    def __init__(self, channels):
        """
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        """
        super(SinPositionalEncoding3D, self).__init__()
        channels = int(np.ceil(channels/6)*2)
        if channels % 2:
            channels += 1
        self.channels = channels
        self.inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))
        #self.register_buffer('inv_freq', inv_freq)

    def forward(self, tensor):
        """
        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)
        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)
        """
        tensor = tensor.permute(0, 2, 3, 4, 1)
        if len(tensor.shape) != 5:
            raise RuntimeError("The input tensor has to be 5d!")
        batch_size, x, y, z, orig_ch = tensor.shape
        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())
        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())
        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())
        sin_inp_x = torch.einsum("i,j->ij", pos_x, self.inv_freq)
        sin_inp_y = torch.einsum("i,j->ij", pos_y, self.inv_freq)
        sin_inp_z = torch.einsum("i,j->ij", pos_z, self.inv_freq)
        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1).unsqueeze(1)
        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1).unsqueeze(1)
        emb_z = torch.cat((sin_inp_z.sin(), sin_inp_z.cos()), dim=-1)
        emb = torch.zeros((x,y,z,self.channels*3),device=tensor.device).type(tensor.type())
        emb[:,:,:,:self.channels] = emb_x
        emb[:,:,:,self.channels:2*self.channels] = emb_y
        emb[:,:,:,2*self.channels:] = emb_z
        emb = emb[None,:,:,:,:orig_ch].repeat(batch_size, 1, 1, 1, 1)
        return emb.permute(0, 4, 1, 2, 3)


class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030
    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (tuple): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(self, pretrain_img_size=224,
                 patch_size=4,
                 in_chans=3,
                 embed_dim=96,
                 depths=[2, 2, 6, 2],
                 num_heads=[3, 6, 12, 24],
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.2,
                 norm_layer=nn.LayerNorm,
                 ape=False,
                 spe=False,
                 rpe=True,
                 patch_norm=True,
                 out_indices=(0, 1, 2, 3),
                 frozen_stages=-1,
                 use_checkpoint=False,
                 pat_merg_rf=2,):
        super().__init__()
        self.pretrain_img_size = pretrain_img_size
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.spe = spe
        self.rpe = rpe
        self.patch_norm = patch_norm
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)

        # absolute position embedding
        if self.ape:
            pretrain_img_size = to_3tuple(self.pretrain_img_size)
            patch_size = to_3tuple(patch_size)
            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1], pretrain_img_size[2] // patch_size[2]]

            self.absolute_pos_embed = nn.Parameter(
                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1], patches_resolution[2]))
            trunc_normal_(self.absolute_pos_embed, std=.02)
        elif self.spe:
            self.pos_embd = SinPositionalEncoding3D(embed_dim).cuda()
            #self.pos_embd = SinusoidalPositionEmbedding().cuda()
        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                                depth=depths[i_layer],
                                num_heads=num_heads[i_layer],
                                window_size=window_size,
                                mlp_ratio=mlp_ratio,
                                qkv_bias=qkv_bias,
                                rpe = rpe,
                                qk_scale=qk_scale,
                                drop=drop_rate,
                                attn_drop=attn_drop_rate,
                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                                norm_layer=norm_layer,
                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                                use_checkpoint=use_checkpoint,
                               pat_merg_rf=pat_merg_rf,)
            self.layers.append(layer)

        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
        self.num_features = num_features

        # add a norm layer for each output
        for i_layer in out_indices:
            layer = norm_layer(num_features[i_layer])
            layer_name = f'norm{i_layer}'
            self.add_module(layer_name, layer)

        self._freeze_stages()

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False

        if self.frozen_stages >= 1 and self.ape:
            self.absolute_pos_embed.requires_grad = False

        if self.frozen_stages >= 2:
            self.pos_drop.eval()
            for i in range(0, self.frozen_stages - 1):
                m = self.layers[i]
                m.eval()
                for param in m.parameters():
                    param.requires_grad = False

    def init_weights(self, pretrained=None):
        """Initialize the weights in backbone.
        Args:
            pretrained (str, optional): Path to pre-trained weights.
                Defaults to None.
        """

        def _init_weights(m):
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

        if isinstance(pretrained, str):
            self.apply(_init_weights)
        elif pretrained is None:
            self.apply(_init_weights)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, x):
        """Forward function."""
        x = self.patch_embed(x)

        Wh, Ww, Wt = x.size(2), x.size(3), x.size(4)

        if self.ape:
            # interpolate the position embedding to the corresponding size
            absolute_pos_embed = nnf.interpolate(self.absolute_pos_embed, size=(Wh, Ww, Wt), mode='trilinear')
            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww*Wt C
        elif self.spe:
            x = (x + self.pos_embd(x)).flatten(2).transpose(1, 2)
        else:
            x = x.flatten(2).transpose(1, 2)
        x = self.pos_drop(x)

        outs = []
        for i in range(self.num_layers):
            layer = self.layers[i]
            x_out, H, W, T, x, Wh, Ww, Wt = layer(x, Wh, Ww, Wt)
            if i in self.out_indices:
                norm_layer = getattr(self, f'norm{i}')
                x_out = norm_layer(x_out)

                out = x_out.view(-1, H, W, T, self.num_features[i]).permute(0, 4, 1, 2, 3).contiguous()
                outs.append(out)
        return outs

    def train(self, mode=True):
        """Convert the model into training mode while keep layers freezed."""
        super(SwinTransformer, self).train(mode)
        self._freeze_stages()


class Conv3dReLU(nn.Sequential):
    def __init__(
            self,
            in_channels,
            out_channels,
            kernel_size,
            padding=0,
            stride=1,
            use_batchnorm=True,
    ):
        conv = nn.Conv3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            bias=False,
        )
        relu = nn.LeakyReLU(inplace=True)
        if not use_batchnorm:
            nm = nn.InstanceNorm3d(out_channels)
        else:
            nm = nn.BatchNorm3d(out_channels)

        super(Conv3dReLU, self).__init__(conv, nm, relu)


class PixelShuffle3d(nn.Module):
    '''
    This class is a 3d version of pixelshuffle.
    '''
    def __init__(self, scale):
        '''
        :param scale: upsample scale
        '''
        super().__init__()
        self.scale = scale

    def forward(self, input):
        batch_size, channels, in_depth, in_height, in_width = input.size()
        nOut = channels // self.scale ** 3

        out_depth = in_depth * self.scale
        out_height = in_height * self.scale
        out_width = in_width * self.scale

        input_view = input.contiguous().view(batch_size, nOut, self.scale, self.scale, self.scale, in_depth, in_height, in_width)

        output = input_view.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()

        return output.view(batch_size, nOut, out_depth, out_height, out_width)


class ConvergeHead(nn.Module):
    def __init__(self, in_dim, up_ratio, kernel_size, padding):
        super().__init__()
        self.in_dim = in_dim
        self.up_ratio = up_ratio


        self.conv = nn.Conv3d(in_dim, (up_ratio**3)*in_dim, kernel_size, 1, padding, 1, in_dim)
        self.apply(self._init_weights)

    def forward(self, x):
        hp = self.conv(x)
        #hp = F.pixel_shuffle(hp, self.up_ratio)
        poxel = PixelShuffle3d(self.up_ratio)
        hp = poxel(hp)

        return hp
    def _init_weights(self, m):
        if isinstance(m, (nn.Conv3d, nn.Linear)):
            nn.init.normal_(m.weight, std=0.001)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm3d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)


class SR(nn.Module):
    def __init__(
            self,
            in_channels,
            out_channels,
            skip_channels=0,
            use_batchnorm=True,
    ):
        super().__init__()
        self.conv1 = Conv3dReLU(
            in_channels + skip_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            use_batchnorm=use_batchnorm,
        )
        self.conv2 = Conv3dReLU(
            out_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            use_batchnorm=use_batchnorm,
        )
        #self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)
        self.up = ConvergeHead(in_channels, 2, 3, 1)

    def forward(self, x, skip=None):
        x = self.up(x)
        if skip is not None:
            if skip.shape[2:] != x.shape[2:]:
                skip = nnf.interpolate(skip, size=x.shape[2:],  mode='trilinear', align_corners=False)
            x = torch.cat([x, skip], dim=1)
        x = self.conv1(x)
        x = self.conv2(x)
        return x


class RegistrationHead(nn.Sequential):
    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):
        conv3d = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)
        conv3d.weight = nn.Parameter(Normal(0, 1e-5).sample(conv3d.weight.shape))
        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))
        super().__init__(conv3d)


class SpatialTransformer(nn.Module):
    """
    N-D Spatial Transformer
    Obtained from https://github.com/voxelmorph/voxelmorph
    """
    def __init__(self, size, mode='bilinear'):
        super().__init__()

        self.mode = mode

        # create sampling grid
        vectors = [torch.arange(0, s) for s in size]
        grids = torch.meshgrid(vectors, indexing='ij')
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)

        # registering the grid as a buffer cleanly moves it to the GPU, but it also
        # adds it to the state dict. this is annoying since everything in the state dict
        # is included when saving weights to disk, so the model files are way bigger
        # than they need to be. so far, there does not appear to be an elegant solution.
        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict
        self.register_buffer('grid', grid)

    def forward(self, src, flow):
        # new locations
        new_locs = self.grid + flow
        shape = flow.shape[2:]

        # need to normalize grid values to [-1, 1] for resampler
        for i in range(len(shape)):
            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)

        # move channels dim to last position
        # also not sure why, but the channels need to be reversed
        if len(shape) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(shape) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return nnf.grid_sample(src, new_locs, align_corners=False, mode=self.mode)


class UTSRMorph(nn.Module):
    def __init__(self, config):
        '''
        UTSRMorph Model
        '''
        super(UTSRMorph, self).__init__()
        if_convskip = config.if_convskip
        self.if_convskip = if_convskip
        if_transskip = config.if_transskip
        self.if_transskip = if_transskip
        embed_dim = config.embed_dim
        self.transformer = SwinTransformer(patch_size=config.patch_size,
        
[... –æ–±—Ä–µ–∑–∞–Ω–æ ...]
==================================================

üìÑ models/__init__.py
--------------------------------------------------

==================================================

üìÑ utils/surface_distance/__init__.py
--------------------------------------------------
# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS-IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Surface distance module: https://github.com/deepmind/surface-distance ."""

from .metrics import *  # pylint: disable=wildcard-import
__version__ = "0.1"

==================================================

üìÑ utils/surface_distance/lookup_tables.py
--------------------------------------------------
# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS-IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Lookup tables used by surface distance metrics."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import numpy as np

ENCODE_NEIGHBOURHOOD_3D_KERNEL = np.array([[[128, 64], [32, 16]], [[8, 4],
                                                                   [2, 1]]])

# _NEIGHBOUR_CODE_TO_NORMALS is a lookup table.
# For every binary neighbour code
# (2x2x2 neighbourhood = 8 neighbours = 8 bits = 256 codes)
# it contains the surface normals of the triangles (called "surfel" for
# "surface element" in the following). The length of the normal
# vector encodes the surfel area.
#
# created using the marching_cube algorithm
# see e.g. https://en.wikipedia.org/wiki/Marching_cubes
# pylint: disable=line-too-long
_NEIGHBOUR_CODE_TO_NORMALS = [
    [[0, 0, 0]],
    [[0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125]],
    [[-0.5, 0.0, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[0.5, 0.0, 0.0], [0.5, 0.0, 0.0]],
    [[0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.5, 0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, 0.25], [-0.125, -0.125, -0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [0.125, -0.125, -0.125]],
    [[0.125, 0.125, 0.125], [0.375, 0.375, 0.375], [0.0, -0.25, 0.25], [-0.25, 0.0, 0.25]],
    [[0.125, -0.125, -0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.375, 0.375, 0.375], [0.0, 0.25, -0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.125, 0.125, 0.125]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25]],
    [[0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.375, -0.375, 0.375], [-0.0, 0.25, 0.25], [0.125, 0.125, -0.125], [-0.25, -0.0, -0.25]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.25, 0.25, -0.25], [0.25, 0.25, -0.25], [0.125, 0.125, -0.125], [-0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125], [0.125, -0.125, 0.125]],
    [[0.0, 0.25, -0.25], [0.375, -0.375, -0.375], [-0.125, 0.125, 0.125], [0.25, 0.25, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[0.0, 0.5, 0.0], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.0, 0.5, 0.0], [0.125, -0.125, 0.125], [-0.25, 0.25, -0.25]],
    [[0.0, 0.5, 0.0], [0.0, -0.5, 0.0]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0], [0.125, -0.125, 0.125]],
    [[-0.375, -0.375, -0.375], [-0.25, 0.0, 0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[0.125, 0.125, 0.125], [0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[-0.125, 0.125, 0.125], [0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.375, 0.375, -0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.375, -0.375, 0.375], [0.0, -0.25, -0.25], [-0.125, 0.125, -0.125], [0.25, 0.25, 0.0]],
    [[-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.25, 0.25, -0.25], [-0.25, 0.25, -0.25], [-0.125, 0.125, -0.125], [-0.125, 0.125, -0.125]],
    [[-0.25, 0.0, -0.25], [0.375, -0.375, -0.375], [0.0, 0.25, -0.25], [-0.125, 0.125, 0.125]],
    [[0.5, 0.0, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[-0.0, 0.0, 0.5], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[-0.25, -0.0, -0.25], [-0.375, 0.375, 0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, 0.125]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25], [-0.125, 0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[0.125, -0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[0.25, 0.0, 0.25], [-0.375, -0.375, 0.375], [-0.25, 0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.5, 0.0, -0.0], [0.25, -0.25, -0.25], [0.125, -0.125, -0.125]],
    [[-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.375, -0.375, 0.375], [0.0, 0.25, 0.25], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[0.0, -0.5, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.375, -0.375, 0.375], [0.25, -0.25, 0.0], [0.0, 0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.125, 0.125, 0.125], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[-0.375, -0.375, 0.375], [0.25, -0.25, 0.0], [0.0, 0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.0, -0.5, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[0.375, -0.375, 0.375], [0.0, 0.25, 0.25], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.5, 0.0, -0.0], [0.25, -0.25, -0.25], [0.125, -0.125, -0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[0.125, 0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.25, 0.0, 0.25], [-0.375, -0.375, 0.375], [-0.25, 0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25], [-0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[-0.0, 0.0, 0.5], [0.0, 0.0, 0.5]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [-0.375, 0.375, 0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[-0.0, 0.0, 0.5], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[0.5, 0.0, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, -0.25], [0.375, -0.375, -0.375], [0.0, 0.25, -0.25], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.25, -0.25], [-0.25, 0.25, -0.25], [-0.125, 0.125, -0.125], [-0.125, 0.125, -0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[0.375, -0.375, 0.375], [0.0, -0.25, -0.25], [-0.125, 0.125, -0.125], [0.25, 0.25, 0.0]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25]],
    [[-0.125, -0.125, 0.125], [-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[0.125, 0.125, 0.125], [-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.375, 0.375, -0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[0.125, 0.125, 0.125], [0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[-0.375, -0.375, -0.375], [-0.25, 0.0, 0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0], [0.125, -0.125, 0.125]],
    [[0.0, 0.5, 0.0], [0.0, -0.5, 0.0]],
    [[0.0, 0.5, 0.0], [0.125, -0.125, 0.125], [-0.25, 0.25, -0.25]],
    [[0.0, 0.5, 0.0], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.0, 0.25, -0.25], [0.375, -0.375, -0.375], [-0.125, 0.125, 0.125], [0.25, 0.25, 0.0]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.25, 0.25, -0.25], [0.25, 0.25, -0.25], [0.125, 0.125, -0.125], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.375, -0.375, 0.375], [-0.0, 0.25, 0.25], [0.125, 0.125, -0.125], [-0.25, -0.0, -0.25]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.125, 0.125, 0.125]],
    [[0.375, 0.375, 0.375], [0.0, 0.25, -0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[0.125, -0.125, -0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.125, 0.125, 0.125], [0.375, 0.375, 0.375], [0.0, -0.25, 0.25], [-0.25, 0.0, 0.25]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, 0.25], [-0.125, -0.125, -0.125]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.5, 0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25]],
    [[0.125, -0.125, -0.125]],
    [[0.5, 0.0, 0.0], [0.5, 0.0, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125]],
    [[0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.125, 0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125]],
    [[0, 0, 0]]]
# pylint: enable=line-too-long


def create_table_neighbour_code_to_surface_area(spacing_mm):
  """Returns an array mapping neighbourhood code to the surface elements area.

  Note that the normals encode the initial surface area. This function computes
  the area corresponding to the given `spacing_mm`.

  Args:
    spacing_mm: 3-element list-like structure. Voxel spacing in x0, x1 and x2
      direction.
  """
  # compute the area for all 256 possible surface elements
  # (given a 2x2x2 neighbourhood) according to the spacing_mm
  neighbour_code_to_surface_area = np.zeros([256])
  for code in range(256):
    normals = np.array(_NEIGHBOUR_CODE_TO_NORMALS[code])
    sum_area = 0
    for normal_idx in range(normals.shape[0]):
      # normal vector
      n = np.zeros([3])
      n[0] = normals[normal_idx, 0] * spacing_mm[1] * spacing_mm[2]
      n[1] = normals[normal_idx, 1] * spacing_mm[0] * spacing_mm[2]
      n[2] = normals[normal_idx, 2] * spacing_mm[0] * spacing_mm[1]
      area = np.linalg.norm(n)
      sum_area += area
    neighbour_code_to_surface_area[code] = sum_area

  return neighbour_code_to_surface_area


# In the neighbourhood, points are ordered: top left, top right, bottom left,
# bottom right.
ENCODE_NEIGHBOURHOOD_2D_KERNEL = np.array([[8, 4], [2, 1]])


def create_table_neighbour_code_to_contour_length(spacing_mm):
  """Returns an array mapping neighbourhood code to the contour length.

  For the list of possible cases and their figures, see page 38 from:
  https://nccastaff.bournemouth.ac.uk/jmacey/MastersProjects/MSc14/06/thesis.pdf

  In 2D, each point has 4 neighbors. Thus, are 16 configurations. A
  configuration is encoded with '1' meaning "inside the object" and '0' "outside
  the object". The points are ordered: top left, top right, bottom left, bottom
  right.

  The x0 axis is assumed vertical downward, and the x1 axis is horizontal to the
  right:
   (0, 0) --> (0, 1)
     |
   (1, 0)

  Args:
    spacing_mm: 2-element list-like structure. Voxel spacing in x0 and x1
      directions.
  """
  neighbour_code_to_contour_length = np.zeros([16])

  vertical = spacing_mm[0]
  horizontal = spacing_mm[1]
  diag = 0.5 * math.sqrt(spacing_mm[0]**2 + spacing_mm[1]**2)
  # pyformat: disable
  neighbour_code_to_contour_length[int("00"
                                       "01", 2)] = diag

  neighbour_code_to_contour_length[int("00"
                                       "10", 2)] = diag

  neighbour_code_to_contour_length[int("00"
                                       "11", 2)] = horizontal

  neighbour_code_to_contour_length[int("01"
                                       "00", 2)] = diag

  neighbour_code_to_contour_length[int("01"
                                       "01", 2)] = vertical

  neighbour_code_to_contour_length[int("01"
                                       "10", 2)] = 2*diag

  neighbour_code_to_contour_length[int("01"
                                       "11", 2)] = diag

  neighbour_code_to_contour_length[int("10"
                                       "00", 2)] = diag

  neighbour_code_to_contour_length[int("10"
                                       "01", 2)] = 2*diag

  neighbour_code_to_contour_length[int("10"
                                       "10", 2)] = vertical

  neighbour_code_to_contour_length[int("10"
                                       "11", 2)] = diag

  neighbour_code_to_contour_length[int("11"
                                       "00", 2)] = horizontal

  neighbour_code_to_contour_length[int("11"
                                       "01", 2)] = diag

  neighbour_code_to_contour_length[int("11"
                                       "10", 2)] = diag
  # pyformat: enable

  return neighbour_code_to_contour_length

==================================================

üìÑ utils/surface_distance/metrics.py
--------------------------------------------------
# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS-IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Module exposing surface distance based measures."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from . import lookup_tables  # pylint: disable=relative-beyond-top-level
import numpy as np
from scipy import ndimage


def _assert_is_numpy_array(name, array):
  """Raises an exception if `array` is not a numpy array."""
  if not isinstance(array, np.ndarray):
    raise ValueError("The argument {!r} should be a numpy array, not a "
                     "{}".format(name, type(array)))


def _check_nd_numpy_array(name, array, num_dims):
  """Raises an exception if `array` is not a `num_dims`-D numpy array."""
  if len(array.shape) != num_dims:
    raise ValueError("The argument {!r} should be a {}D array, not of "
                     "shape {}".format(name, num_dims, array.shape))


def _check_2d_numpy_array(name, array):
  _check_nd_numpy_array(name, array, num_dims=2)


def _check_3d_numpy_array(name, array):
  _check_nd_numpy_array(name, array, num_dims=3)


def _assert_is_bool_numpy_array(name, array):
  _assert_is_numpy_array(name, array)
  if array.dtype != np.bool:
    raise ValueError("The argument {!r} should be a numpy array of type bool, "
                     "not {}".format(name, array.dtype))


def _compute_bounding_box(mask):
  """Computes the bounding box of the masks.

  This function generalizes to arbitrary number of dimensions great or equal
  to 1.

  Args:
    mask: The 2D or 3D numpy mask, where '0' means background and non-zero means
      foreground.

  Returns:
    A tuple:
     - The coordinates of the first point of the bounding box (smallest on all
       axes), or `None` if the mask contains only zeros.
     - The coordinates of the second point of the bounding box (greatest on all
       axes), or `None` if the mask contains only zeros.
  """
  num_dims = len(mask.shape)
  bbox_min = np.zeros(num_dims, np.int64)
  bbox_max = np.zeros(num_dims, np.int64)

  # max projection to the x0-axis
  proj_0 = np.amax(mask, axis=tuple(range(num_dims))[1:])
  idx_nonzero_0 = np.nonzero(proj_0)[0]
  if len(idx_nonzero_0) == 0:  # pylint: disable=g-explicit-length-test
    return None, None

  bbox_min[0] = np.min(idx_nonzero_0)
  bbox_max[0] = np.max(idx_nonzero_0)

  # max projection to the i-th-axis for i in {1, ..., num_dims - 1}
  for axis in range(1, num_dims):
    max_over_axes = list(range(num_dims))  # Python 3 compatible
    max_over_axes.pop(axis)  # Remove the i-th dimension from the max
    max_over_axes = tuple(max_over_axes)  # numpy expects a tuple of ints
    proj = np.amax(mask, axis=max_over_axes)
    idx_nonzero = np.nonzero(proj)[0]
    bbox_min[axis] = np.min(idx_nonzero)
    bbox_max[axis] = np.max(idx_nonzero)

  return bbox_min, bbox_max


def _crop_to_bounding_box(mask, bbox_min, bbox_max):
  """Crops a 2D or 3D mask to the bounding box specified by `bbox_{min,max}`."""
  # we need to zeropad the cropped region with 1 voxel at the lower,
  # the right (and the back on 3D) sides. This is required to obtain the
  # "full" convolution result with the 2x2 (or 2x2x2 in 3D) kernel.
  # TODO:  This is correct only if the object is interior to the
  # bounding box.
  cropmask = np.zeros((bbox_max - bbox_min) + 2, np.uint8)

  num_dims = len(mask.shape)
  # pyformat: disable
  if num_dims == 2:
    cropmask[0:-1, 0:-1] = mask[bbox_min[0]:bbox_max[0] + 1,
                                bbox_min[1]:bbox_max[1] + 1]
  elif num_dims == 3:
    cropmask[0:-1, 0:-1, 0:-1] = mask[bbox_min[0]:bbox_max[0] + 1,
                                      bbox_min[1]:bbox_max[1] + 1,
                                      bbox_min[2]:bbox_max[2] + 1]
  # pyformat: enable
  else:
    assert False

  return cropmask


def _sort_distances_surfels(distances, surfel_areas):
  """Sorts the two list with respect to the tuple of (distance, surfel_area).

  Args:
    distances: The distances from A to B (e.g. `distances_gt_to_pred`).
    surfel_areas: The surfel areas for A (e.g. `surfel_areas_gt`).

  Returns:
    A tuple of the sorted (distances, surfel_areas).
  """
  sorted_surfels = np.array(sorted(zip(distances, surfel_areas)))
  return sorted_surfels[:, 0], sorted_surfels[:, 1]


def compute_surface_distances(mask_gt,
                              mask_pred,
                              spacing_mm):
  """Computes closest distances from all surface points to the other surface.

  This function can be applied to 2D or 3D tensors. For 2D, both masks must be
  2D and `spacing_mm` must be a 2-element list. For 3D, both masks must be 3D
  and `spacing_mm` must be a 3-element list. The description is done for the 2D
  case, and the formulation for the 3D case is present is parenthesis,
  introduced by "resp.".

  Finds all contour elements (resp surface elements "surfels" in 3D) in the
  ground truth mask `mask_gt` and the predicted mask `mask_pred`, computes their
  length in mm (resp. area in mm^2) and the distance to the closest point on the
  other contour (resp. surface). It returns two sorted lists of distances
  together with the corresponding contour lengths (resp. surfel areas). If one
  of the masks is empty, the corresponding lists are empty and all distances in
  the other list are `inf`.

  Args:
    mask_gt: 2-dim (resp. 3-dim) bool Numpy array. The ground truth mask.
    mask_pred: 2-dim (resp. 3-dim) bool Numpy array. The predicted mask.
    spacing_mm: 2-element (resp. 3-element) list-like structure. Voxel spacing
      in x0 anx x1 (resp. x0, x1 and x2) directions.

  Returns:
    A dict with:
    "distances_gt_to_pred": 1-dim numpy array of type float. The distances in mm
        from all ground truth surface elements to the predicted surface,
        sorted from smallest to largest.
    "distances_pred_to_gt": 1-dim numpy array of type float. The distances in mm
        from all predicted surface elements to the ground truth surface,
        sorted from smallest to largest.
    "surfel_areas_gt": 1-dim numpy array of type float. The length of the
      of the ground truth contours in mm (resp. the surface elements area in
      mm^2) in the same order as distances_gt_to_pred.
    "surfel_areas_pred": 1-dim numpy array of type float. The length of the
      of the predicted contours in mm (resp. the surface elements area in
      mm^2) in the same order as distances_gt_to_pred.

  Raises:
    ValueError: If the masks and the `spacing_mm` arguments are of incompatible
      shape or type. Or if the masks are not 2D or 3D.
  """
  # The terms used in this function are for the 3D case. In particular, surface
  # in 2D stands for contours in 3D. The surface elements in 3D correspond to
  # the line elements in 2D.

  _assert_is_bool_numpy_array("mask_gt", mask_gt)
  _assert_is_bool_numpy_array("mask_pred", mask_pred)

  if not len(mask_gt.shape) == len(mask_pred.shape) == len(spacing_mm):
    raise ValueError("The arguments must be of compatible shape. Got mask_gt "
                     "with {} dimensions ({}) and mask_pred with {} dimensions "
                     "({}), while the spacing_mm was {} elements.".format(
                         len(mask_gt.shape),
                         mask_gt.shape, len(mask_pred.shape), mask_pred.shape,
                         len(spacing_mm)))

  num_dims = len(spacing_mm)
  if num_dims == 2:
    _check_2d_numpy_array("mask_gt", mask_gt)
    _check_2d_numpy_array("mask_pred", mask_pred)

    # compute the area for all 16 possible surface elements
    # (given a 2x2 neighbourhood) according to the spacing_mm
    neighbour_code_to_surface_area = (
        lookup_tables.create_table_neighbour_code_to_contour_length(spacing_mm))
    kernel = lookup_tables.ENCODE_NEIGHBOURHOOD_2D_KERNEL
    full_true_neighbours = 0b1111
  elif num_dims == 3:
    _check_3d_numpy_array("mask_gt", mask_gt)
    _check_3d_numpy_array("mask_pred", mask_pred)

    # compute the area for all 256 possible surface elements
    # (given a 2x2x2 neighbourhood) according to the spacing_mm
    neighbour_code_to_surface_area = (
        lookup_tables.create_table_neighbour_code_to_surface_area(spacing_mm))
    kernel = lookup_tables.ENCODE_NEIGHBOURHOOD_3D_KERNEL
    full_true_neighbours = 0b11111111
  else:
    raise ValueError("Only 2D and 3D masks are supported, not "
                     "{}D.".format(num_dims))

  # compute the bounding box of the masks to trim the volume to the smallest
  # possible processing subvolume
  bbox_min, bbox_max = _compute_bounding_box(mask_gt | mask_pred)
  # Both the min/max bbox are None at the same time, so we only check one.
  if bbox_min is None:
    return {
        "distances_gt_to_pred": np.array([]),
        "distances_pred_to_gt": np.array([]),
        "surfel_areas_gt": np.array([]),
        "surfel_areas_pred": np.array([]),
    }

  # crop the processing subvolume.
  cropmask_gt = _crop_to_bounding_box(mask_gt, bbox_min, bbox_max)
  cropmask_pred = _crop_to_bounding_box(mask_pred, bbox_min, bbox_max)

  # compute the neighbour code (local binary pattern) for each voxel
  # the resulting arrays are spacially shifted by minus half a voxel in each
  # axis.
  # i.e. the points are located at the corners of the original voxels
  neighbour_code_map_gt = ndimage.filters.correlate(
      cropmask_gt.astype(np.uint8), kernel, mode="constant", cval=0)
  neighbour_code_map_pred = ndimage.filters.correlate(
      cropmask_pred.astype(np.uint8), kernel, mode="constant", cval=0)

  # create masks with the surface voxels
  borders_gt = ((neighbour_code_map_gt != 0) &
                (neighbour_code_map_gt != full_true_neighbours))
  borders_pred = ((neighbour_code_map_pred != 0) &
                  (neighbour_code_map_pred != full_true_neighbours))

  # compute the distance transform (closest distance of each voxel to the
  # surface voxels)
  if borders_gt.any():
    distmap_gt = ndimage.morphology.distance_transform_edt(
        ~borders_gt, sampling=spacing_mm)
  else:
    distmap_gt = np.Inf * np.ones(borders_gt.shape)

  if borders_pred.any():
    distmap_pred = ndimage.morphology.distance_transform_edt(
        ~borders_pred, sampling=spacing_mm)
  else:
    distmap_pred = np.Inf * np.ones(borders_pred.shape)

  # compute the area of each surface element
  surface_area_map_gt = neighbour_code_to_surface_area[neighbour_code_map_gt]
  surface_area_map_pred = neighbour_code_to_surface_area[
      neighbour_code_map_pred]

  # create a list of all surface elements with distance and area
  distances_gt_to_pred = distmap_pred[borders_gt]
  distances_pred_to_gt = distmap_gt[borders_pred]
  surfel_areas_gt = surface_area_map_gt[borders_gt]
  surfel_areas_pred = surface_area_map_pred[borders_pred]

  # sort them by distance
  if distances_gt_to_pred.shape != (0,):
    distances_gt_to_pred, surfel_areas_gt = _sort_distances_surfels(
        distances_gt_to_pred, surfel_areas_gt)

  if distances_pred_to_gt.shape != (0,):
    distances_pred_to_gt, surfel_areas_pred = _sort_distances_surfels(
        distances_pred_to_gt, surfel_areas_pred)

  return {
      "distances_gt_to_pred": distances_gt_to_pred,
      "distances_pred_to_gt": distances_pred_to_gt,
      "surfel_areas_gt": surfel_areas_gt,
      "surfel_areas_pred": surfel_areas_pred,
  }


def compute_average_surface_distance(surface_distances):
  """Returns the average surface distance.

  Computes the average surface distances by correctly taking the area of each
  surface element into account. Call compute_surface_distances(...) before, to
  obtain the `surface_distances` dict.

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
    "surfel_areas_gt", "surfel_areas_pred" created by
    compute_surface_distances()

  Returns:
    A tuple with two float values:
      - the average distance (in mm) from the ground truth surface to the
        predicted surface
      - the average distance from the predicted surface to the ground truth
        surface.
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  average_distance_gt_to_pred = (
      np.sum(distances_gt_to_pred * surfel_areas_gt) / np.sum(surfel_areas_gt))
  average_distance_pred_to_gt = (
      np.sum(distances_pred_to_gt * surfel_areas_pred) /
      np.sum(surfel_areas_pred))
  return (average_distance_gt_to_pred, average_distance_pred_to_gt)


def compute_robust_hausdorff(surface_distances, percent):
  """Computes the robust Hausdorff distance.

  Computes the robust Hausdorff distance. "Robust", because it uses the
  `percent` percentile of the distances instead of the maximum distance. The
  percentage is computed by correctly taking the area of each surface element
  into account.

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
      "surfel_areas_gt", "surfel_areas_pred" created by
      compute_surface_distances()
    percent: a float value between 0 and 100.

  Returns:
    a float value. The robust Hausdorff distance in mm.
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  if len(distances_gt_to_pred) > 0:  # pylint: disable=g-explicit-length-test
    surfel_areas_cum_gt = np.cumsum(surfel_areas_gt) / np.sum(surfel_areas_gt)
    idx = np.searchsorted(surfel_areas_cum_gt, percent/100.0)
    perc_distance_gt_to_pred = distances_gt_to_pred[
        min(idx, len(distances_gt_to_pred)-1)]
  else:
    perc_distance_gt_to_pred = np.Inf

  if len(distances_pred_to_gt) > 0:  # pylint: disable=g-explicit-length-test
    surfel_areas_cum_pred = (np.cumsum(surfel_areas_pred) /
                             np.sum(surfel_areas_pred))
    idx = np.searchsorted(surfel_areas_cum_pred, percent/100.0)
    perc_distance_pred_to_gt = distances_pred_to_gt[
        min(idx, len(distances_pred_to_gt)-1)]
  else:
    perc_distance_pred_to_gt = np.Inf

  return max(perc_distance_gt_to_pred, perc_distance_pred_to_gt)


def compute_surface_overlap_at_tolerance(surface_distances, tolerance_mm):
  """Computes the overlap of the surfaces at a specified tolerance.

  Computes the overlap of the ground truth surface with the predicted surface
  and vice versa allowing a specified tolerance (maximum surface-to-surface
  distance that is regarded as overlapping). The overlapping fraction is
  computed by correctly taking the area of each surface element into account.

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
      "surfel_areas_gt", "surfel_areas_pred" created by
      compute_surface_distances()
    tolerance_mm: a float value. The tolerance in mm

  Returns:
    A tuple of two float values. The overlap fraction in [0.0, 1.0] of the
    ground truth surface with the predicted surface and vice versa.
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  rel_overlap_gt = (
      np.sum(surfel_areas_gt[distances_gt_to_pred <= tolerance_mm]) /
      np.sum(surfel_areas_gt))
  rel_overlap_pred = (
      np.sum(surfel_areas_pred[distances_pred_to_gt <= tolerance_mm]) /
      np.sum(surfel_areas_pred))
  return (rel_overlap_gt, rel_overlap_pred)


def compute_surface_dice_at_tolerance(surface_distances, tolerance_mm):
  """Computes the _surface_ DICE coefficient at a specified tolerance.

  Computes the _surface_ DICE coefficient at a specified tolerance. Not to be
  confused with the standard _volumetric_ DICE coefficient. The surface DICE
  measures the overlap of two surfaces instead of two volumes. A surface
  element is counted as overlapping (or touching), when the closest distance to
  the other surface is less or equal to the specified tolerance. The DICE
  coefficient is in the range between 0.0 (no overlap) to 1.0 (perfect overlap).

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
      "surfel_areas_gt", "surfel_areas_pred" created by
      compute_surface_distances()
    tolerance_mm: a float value. The tolerance in mm

  Returns:
    A float value. The surface DICE coefficient in [0.0, 1.0].
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  overlap_gt = np.sum(surfel_areas_gt[distances_gt_to_pred <= tolerance_mm])
  overlap_pred = np.sum(surfel_areas_pred[distances_pred_to_gt <= tolerance_mm])
  surface_dice = (overlap_gt + overlap_pred) / (
      np.sum(surfel_areas_gt) + np.sum(surfel_areas_pred))
  return surface_dice


def compute_dice_coefficient(mask_gt, mask_pred):
  """Computes soerensen-dice coefficient.

  compute the soerensen-dice coefficient between the ground truth mask `mask_gt`
  and the predicted mask `mask_pred`.

  Args:
    mask_gt: 3-dim Numpy array of type bool. The ground truth mask.
    mask_pred: 3-dim Numpy array of type bool. The predicted mask.

  Returns:
    the dice coeffcient as float. If both masks are empty, the result is NaN.
  """
  volume_sum = mask_gt.sum() + mask_pred.sum()
  if volume_sum == 0:
    return np.NaN
  volume_intersect = (mask_gt & mask_pred).sum()
  return 2*volume_intersect / volume_sum

==================================================

üìÑ utils/__init__.py
--------------------------------------------------
# utils/__init__.py
# Public API of utils.


from utils.core import *
from utils.ctcf_losses import *
from utils.data import *
from utils.dice import *
from utils.field import *
from utils.losses import *
from utils.misc import *
from utils.rand import *
from utils.spatial import *
from utils.train import *
from utils.trans import *
from utils.validation import *
==================================================

üìÑ utils/core.py
--------------------------------------------------
import pickle
import numpy as np
import torch
import torch.nn.functional as F


def pkload(fname: str):
    with open(fname, "rb") as f:
        return pickle.load(f)


class AverageMeter:
    """Computes and stores the average and current value."""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0.0
        self.avg = 0.0
        self.sum = 0.0
        self.count = 0
        self.vals = []
        self.std = 0.0

    def update(self, val, n: int = 1):
        val = float(val)
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / max(1, self.count)
        self.vals.append(val)
        self.std = float(np.std(self.vals)) if len(self.vals) > 1 else 0.0


def pad_image(img: torch.Tensor, target_size):
    """
    Pad 3D image tensor [B,C,D,H,W] to at least target_size (D,H,W) with zeros.
    """
    rows_to_pad = max(target_size[0] - img.shape[2], 0)
    cols_to_pad = max(target_size[1] - img.shape[3], 0)
    slcs_to_pad = max(target_size[2] - img.shape[4], 0)
    return F.pad(img, (0, slcs_to_pad, 0, cols_to_pad, 0, rows_to_pad), "constant", 0)


def write2csv(line: str, name: str):
    with open(name + ".csv", "a", encoding="utf-8") as f:
        f.write(line)
        f.write("\n")
==================================================

üìÑ utils/ctcf_losses.py
--------------------------------------------------
import torch
from utils import field


def icon_loss(flow_ab: torch.Tensor, flow_ba: torch.Tensor) -> torch.Tensor:
    """
    ICON / inverse-consistency.
    Penalize composition phi_ab ‚àò phi_ba deviating from identity.
    """
    phi_ab_ba = field.compose_flows(flow_ab, flow_ba, mode='bilinear')
    phi_ba_ab = field.compose_flows(flow_ba, flow_ab, mode='bilinear')
    return phi_ab_ba.abs().mean() + phi_ba_ab.abs().mean()


def cycle_image_loss(model,
                     x: torch.Tensor, y: torch.Tensor,
                     x_warp: torch.Tensor, y_warp: torch.Tensor,
                     flow_xy: torch.Tensor, flow_yx: torch.Tensor) -> torch.Tensor:
    """
    Cycle-consistency in image space:
    x -> y -> x and y -> x -> y.
    """
    # Prefer spatial_trans_full if exists, else spatial_trans
    if hasattr(model, 'spatial_trans_down'):
        warp_fn = model.spatial_trans_down
    elif hasattr(model, 'spatial_trans_full'):
        warp_fn = model.spatial_trans_full
    else:
        warp_fn = model.spatial_trans

    x_cycle = warp_fn(x_warp, flow_yx)
    y_cycle = warp_fn(y_warp, flow_xy)
    return (x_cycle - x).abs().mean() + (y_cycle - y).abs().mean()


def percent_nonpositive_jacobian(flow: torch.Tensor) -> torch.Tensor:
    """
    Metric: percentage of voxels with detJ <= 0 (foldings).
    Returns a scalar tensor in [0,100].
    """
    detJ = field.jacobian_det(flow)  # [B,1,D,H,W]
    return (detJ <= 0.0).float().mean() * 100.0
==================================================

üìÑ utils/data.py
--------------------------------------------------
import random
import numpy as np
import torch
import re

M = 2 ** 32 - 1
_shape = (240, 240, 155)
_zero = torch.tensor([0])


def init_fn(worker: int):
    seed = torch.LongTensor(1).random_().item()
    seed = (seed + worker) % M
    np.random.seed(seed)
    random.seed(seed)


def add_mask(x: torch.Tensor, mask: torch.Tensor, dim: int = 1) -> torch.Tensor:
    mask = mask.unsqueeze(dim)
    shape = list(x.shape)
    shape[dim] += 21
    new_x = x.new_zeros(*shape)
    new_x = new_x.scatter_(dim, mask, 1.0)
    s = [slice(None)] * len(shape)
    s[dim] = slice(21, None)
    new_x[s] = x
    return new_x


def sample(x: np.ndarray, size: int) -> torch.Tensor:
    i = random.sample(range(x.shape[0]), size)
    return torch.tensor(x[i], dtype=torch.int16)


def get_all_coords(stride: int) -> torch.Tensor:
    return torch.tensor(
        np.stack(
            [v.reshape(-1) for v in np.meshgrid(
                *[stride // 2 + np.arange(0, s, stride) for s in _shape],
                indexing="ij"
            )],
            -1
        ),
        dtype=torch.int16
    )


def gen_feats() -> np.ndarray:
    x, y, z = _shape
    feats = np.stack(np.meshgrid(np.arange(x), np.arange(y), np.arange(z), indexing="ij"), -1).astype("float32")
    shape = np.array([x, y, z])
    feats -= shape / 2.0
    feats /= shape
    return feats


def process_label(label_info_path: str = "label_info.txt"):
    seg_table = [
        0, 2, 3, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 24, 26,
        28, 30, 31, 41, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 58,
        60, 62, 63, 72, 77, 80, 85, 251, 252, 253, 254, 255
    ]
    with open(label_info_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    label_dict = {}
    seg_i = 0
    for seg_label in seg_table:
        for line in lines:
            parts = re.sub(" +", " ", line).split(" ")
            try:
                int(parts[0])
            except Exception:
                continue
            if int(parts[0]) == seg_label:
                label_dict[seg_i] = parts[1]
        seg_i += 1
    return label_dict
==================================================

üìÑ utils/dice.py
--------------------------------------------------
import numpy as np
import torch
from torch import nn
from scipy.ndimage import gaussian_filter


def dice_val(y_pred: torch.Tensor, y_true: torch.Tensor, num_clus: int) -> torch.Tensor:
    y_pred = nn.functional.one_hot(y_pred, num_classes=num_clus)
    y_pred = torch.squeeze(y_pred, 1).permute(0, 4, 1, 2, 3).contiguous()
    y_true = nn.functional.one_hot(y_true, num_classes=num_clus)
    y_true = torch.squeeze(y_true, 1).permute(0, 4, 1, 2, 3).contiguous()

    intersection = (y_pred * y_true).sum(dim=[2, 3, 4])
    union = y_pred.sum(dim=[2, 3, 4]) + y_true.sum(dim=[2, 3, 4])
    dsc = (2.0 * intersection) / (union + 1e-5)
    return torch.mean(torch.mean(dsc, dim=1))


def dice_val_VOI(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
    VOI_lbls = list(range(1, 36))  # 1..35
    pred = y_pred.detach().cpu().numpy()[0, 0, ...]
    true = y_true.detach().cpu().numpy()[0, 0, ...]
    dscs = []
    for i in VOI_lbls:
        pred_i = (pred == i)
        true_i = (true == i)
        inter = np.sum(pred_i & true_i)
        union = np.sum(pred_i) + np.sum(true_i)
        dscs.append((2.0 * inter) / (union + 1e-5))
    return torch.tensor(float(np.mean(dscs)), device=y_pred.device)


def dice_val_substruct(y_pred: torch.Tensor, y_true: torch.Tensor, std_idx: int) -> str:
    with torch.no_grad():
        y_pred_oh = nn.functional.one_hot(y_pred, num_classes=46)
        y_pred_oh = torch.squeeze(y_pred_oh, 1).permute(0, 4, 1, 2, 3).contiguous()
        y_true_oh = nn.functional.one_hot(y_true, num_classes=46)
        y_true_oh = torch.squeeze(y_true_oh, 1).permute(0, 4, 1, 2, 3).contiguous()

    yp = y_pred_oh.detach().cpu().numpy()
    yt = y_true_oh.detach().cpu().numpy()

    line = f"p_{std_idx}"
    for i in range(46):
        pred_clus = yp[0, i, ...]
        true_clus = yt[0, i, ...]
        inter = (pred_clus * true_clus).sum()
        union = pred_clus.sum() + true_clus.sum()
        dsc = (2.0 * inter) / (union + 1e-5)
        line += "," + str(dsc)
    return line


def dice(y_pred: np.ndarray, y_true: np.ndarray) -> float:
    inter = float(np.sum(y_pred * y_true))
    union = float(np.sum(y_pred) + np.sum(y_true))
    return (2.0 * inter) / (union + 1e-5)


def smooth_seg(binary_img: np.ndarray, sigma: float = 1.5, thresh: float = 0.4) -> np.ndarray:
    sm = gaussian_filter(binary_img.astype(np.float32), sigma=sigma)
    return (sm > thresh)
==================================================

üìÑ utils/field.py
--------------------------------------------------
import torch
import torch.nn.functional as F


def _warp(tensor: torch.Tensor, flow: torch.Tensor, mode: str = "bilinear") -> torch.Tensor:
    """
    Warp tensor [B,C,D,H,W] by flow [B,3,D,H,W] (Voxelmorph-style).
    """
    b, c, d, h, w = tensor.shape
    device = tensor.device

    # base grid in ij indexing: z,y,x
    zz = torch.arange(d, device=device)
    yy = torch.arange(h, device=device)
    xx = torch.arange(w, device=device)
    grid = torch.stack(torch.meshgrid(zz, yy, xx, indexing="ij"), dim=0).float()  # [3,D,H,W]
    grid = grid.unsqueeze(0)  # [1,3,D,H,W]

    new_locs = grid + flow
    # normalize to [-1,1]
    new_locs[:, 0] = 2.0 * (new_locs[:, 0] / (d - 1) - 0.5)
    new_locs[:, 1] = 2.0 * (new_locs[:, 1] / (h - 1) - 0.5)
    new_locs[:, 2] = 2.0 * (new_locs[:, 2] / (w - 1) - 0.5)

    # grid_sample expects (..., x,y,z) i.e. reverse
    grid_sample_grid = new_locs.permute(0, 2, 3, 4, 1)[..., [2, 1, 0]]
    return F.grid_sample(tensor, grid_sample_grid, mode=mode, align_corners=False)


def compose_flows(flow_ab: torch.Tensor, flow_bc: torch.Tensor, mode: str = "bilinear") -> torch.Tensor:
    """
    Compose displacement fields:
      phi_ac = phi_ab ‚àò phi_bc  (displacement convention)
    For displacements: flow_ac = flow_ab + warp(flow_bc, flow_ab)
    """
    return flow_ab + _warp(flow_bc, flow_ab, mode=mode)


def jacobian_det(flow: torch.Tensor) -> torch.Tensor:
    """
    det(J) for transformation (Id + flow) using finite differences in torch.
    flow: [B,3,D,H,W]
    returns: [B,1,D,H,W]
    """
    # gradients w.r.t. z,y,x (D,H,W)
    dz = flow[:, :, 2:, :, :] - flow[:, :, :-2, :, :]
    dy = flow[:, :, :, 2:, :] - flow[:, :, :, :-2, :]
    dx = flow[:, :, :, :, 2:] - flow[:, :, :, :, :-2]

    # pad back to original size (central diff approx)
    dz = F.pad(dz, (0, 0, 0, 0, 1, 1)) * 0.5
    dy = F.pad(dy, (0, 0, 1, 1, 0, 0)) * 0.5
    dx = F.pad(dx, (1, 1, 0, 0, 0, 0)) * 0.5

    # J = I + grad(flow)
    fz, fy, fx = flow[:, 0], flow[:, 1], flow[:, 2]

    # partials:
    fz_z, fz_y, fz_x = dz[:, 0], dy[:, 0], dx[:, 0]
    fy_z, fy_y, fy_x = dz[:, 1], dy[:, 1], dx[:, 1]
    fx_z, fx_y, fx_x = dz[:, 2], dy[:, 2], dx[:, 2]

    J00 = 1.0 + fz_z
    J01 = fz_y
    J02 = fz_x

    J10 = fy_z
    J11 = 1.0 + fy_y
    J12 = fy_x

    J20 = fx_z
    J21 = fx_y
    J22 = 1.0 + fx_x

    det = (
        J00 * (J11 * J22 - J12 * J21)
        - J01 * (J10 * J22 - J12 * J20)
        + J02 * (J10 * J21 - J11 * J20)
    )
    return det.unsqueeze(1)


def neg_jacobian_penalty(flow: torch.Tensor) -> torch.Tensor:
    detJ = jacobian_det(flow)
    return torch.relu(-detJ).mean()
==================================================

üìÑ utils/losses.py
--------------------------------------------------
import torch
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
from math import exp
import math
import torch.nn as nn


class NCC_gauss(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss via Gaussian kernel
    """
    def __init__(self, win=9):
        super(NCC_gauss, self).__init__()
        self.win = [win]*3
        self.filt = self.create_window_3D(win, 1).to("cuda")

    def gaussian(self, window_size, sigma):
        gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])
        return gauss / gauss.sum()

    def create_window_3D(self, window_size, channel):
        _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
        _2D_window = _1D_window.mm(_1D_window.t())
        _3D_window = _1D_window.mm(_2D_window.reshape(1, -1)).reshape(window_size, window_size,
                                                                      window_size).float().unsqueeze(0).unsqueeze(0)
        window = Variable(_3D_window.expand(channel, 1, window_size, window_size, window_size).contiguous())
        return window

    def forward(self, y_true, y_pred):

        Ii = y_true
        Ji = y_pred

        ndims = len(list(Ii.size())) - 2
        assert ndims in [1, 2, 3], "volumes should be 1 to 3 dimensions. found: %d" % ndims
        pad_no = math.floor(self.win[0] / 2)
        conv_fn = getattr(F, 'conv%dd' % ndims)

        mu1 = conv_fn(Ii, self.filt, padding=pad_no)
        mu2 = conv_fn(Ji, self.filt, padding=pad_no)

        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2

        sigma1_sq = conv_fn(Ii * Ii, self.filt, padding=pad_no) - mu1_sq
        sigma2_sq = conv_fn(Ji * Ji, self.filt, padding=pad_no) - mu2_sq
        sigma12 = conv_fn(Ii * Ji, self.filt, padding=pad_no) - mu1_mu2

        cc = (sigma12 * sigma12 + 1e-5)/(sigma1_sq * sigma2_sq + 1e-5)
        return 1-torch.mean(cc)


def gaussian(window_size, sigma):
    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])
    return gauss / gauss.sum()


def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
    return window


def create_window_3D(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t())
    _3D_window = _1D_window.mm(_2D_window.reshape(1, -1)).reshape(window_size, window_size,
                                                                  window_size).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_3D_window.expand(channel, 1, window_size, window_size, window_size).contiguous())
    return window


def _ssim(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2

    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)


def _ssim_3D(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv3d(img1, window, padding=window_size // 2, groups=channel)
    mu2 = F.conv3d(img2, window, padding=window_size // 2, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)

    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv3d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq
    sigma2_sq = F.conv3d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq
    sigma12 = F.conv3d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2

    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)


class SSIM(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True):
        super(SSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window(window_size, self.channel)

    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()

        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window(self.window_size, channel)

            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)

            self.window = window
            self.channel = channel

        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)


class SSIM3D(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True):
        super(SSIM3D, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window_3D(window_size, self.channel)

    def forward(self, img1, img2):
        (_, channel, _, _, _) = img1.size()

        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window_3D(self.window_size, channel)

            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)

            self.window = window
            self.channel = channel

        return 1-_ssim_3D(img1, img2, window, self.window_size, channel, self.size_average)


def ssim(img1, img2, window_size=11, size_average=True):
    (_, channel, _, _) = img1.size()
    window = create_window(window_size, channel)

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)

    return _ssim(img1, img2, window, window_size, channel, size_average)


def ssim3D(img1, img2, window_size=11, size_average=True):
    (_, channel, _, _, _) = img1.size()
    window = create_window_3D(window_size, channel)

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)

    return _ssim_3D(img1, img2, window, window_size, channel, size_average)


class Grad(torch.nn.Module):
    """
    N-D gradient loss.
    """
    def __init__(self, penalty='l1', loss_mult=None):
        super(Grad, self).__init__()
        self.penalty = penalty
        self.loss_mult = loss_mult

    def forward(self, y_pred, y_true):
        dy = torch.abs(y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :])
        dx = torch.abs(y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1])

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx

        d = torch.mean(dx) + torch.mean(dy)
        grad = d / 2.0

        if self.loss_mult is not None:
            grad *= self.loss_mult
        return grad


class Grad3d(torch.nn.Module):
    """
    N-D gradient loss.
    """
    def __init__(self, penalty='l1', loss_mult=None):
        super(Grad3d, self).__init__()
        self.penalty = penalty
        self.loss_mult = loss_mult

    def forward(self, y_pred, y_true=None):
        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])
        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])
        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx
            dz = dz * dz

        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)
        grad = d / 3.0

        if self.loss_mult is not None:
            grad *= self.loss_mult
        return grad


class Grad3DiTV(torch.nn.Module):
    """
    N-D gradient loss.
    """
    def __init__(self):
        super(Grad3DiTV, self).__init__()
        a = 1

    def forward(self, y_pred, y_true):
        dy = torch.abs(y_pred[:, :, 1:, 1:, 1:] - y_pred[:, :, :-1, 1:, 1:])
        dx = torch.abs(y_pred[:, :, 1:, 1:, 1:] - y_pred[:, :, 1:, :-1, 1:])
        dz = torch.abs(y_pred[:, :, 1:, 1:, 1:] - y_pred[:, :, 1:, 1:, :-1])
        dy = dy * dy
        dx = dx * dx
        dz = dz * dz
        d = torch.mean(torch.sqrt(dx+dy+dz+1e-6))
        grad = d / 3.0
        return grad
    

class WeightedGrad3d(nn.Module):
    """
    Weighted smoothness: mean( w(x) * ||‚àáflow||^2 )
    flow:   (B,3,D,H,W)
    weight: (B,1,D,H,W) or (B,D,H,W)
    """
    def __init__(self, penalty='l2'):
        super().__init__()
        assert penalty in ['l1', 'l2']
        self.penalty = penalty

    def forward(self, flow, weight):
        if weight.dim() == 4:
            weight = weight.unsqueeze(1)  # (B,1,D,H,W)

        dy = flow[:, :, 1:, :, :] - flow[:, :, :-1, :, :]
        dx = flow[:, :, :, 1:, :] - flow[:, :, :, :-1, :]
        dz = flow[:, :, :, :, 1:] - flow[:, :, :, :, :-1]

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx
            dz = dz * dz
        else:
            dy = torch.abs(dy)
            dx = torch.abs(dx)
            dz = torch.abs(dz)

        wy = weight[:, :, 1:, :, :]
        wx = weight[:, :, :, 1:, :]
        wz = weight[:, :, :, :, 1:]

        return ((wy * dy).mean() + (wx * dx).mean() + (wz * dz).mean()) / 3.0    


class Grad1ch3d(nn.Module):
    """
    Smoothness for 1-channel map (for lambda map).
    x: (B,1,D,H,W) or (B,D,H,W)
    """
    def __init__(self, penalty='l2'):
        super().__init__()
        assert penalty in ['l1', 'l2']
        self.penalty = penalty

    def forward(self, x):
        if x.dim() == 4:
            x = x.unsqueeze(1)

        dy = x[:, :, 1:, :, :] - x[:, :, :-1, :, :]
        dx = x[:, :, :, 1:, :] - x[:, :, :, :-1, :]
        dz = x[:, :, :, :, 1:] - x[:, :, :, :, :-1]

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx
            dz = dz * dz
        else:
            dy = torch.abs(dy)
            dx = torch.abs(dx)
            dz = torch.abs(dz)

        return (dy.mean() + dx.mean() + dz.mean()) / 3.0


class DisplacementRegularizer(torch.nn.Module):
    def __init__(self, energy_type):
        super().__init__()
        self.energy_type = energy_type

    def gradient_dx(self, fv): return (fv[:, 2:, 1:-1, 1:-1] - fv[:, :-2, 1:-1, 1:-1]) / 2

    def gradient_dy(self, fv): return (fv[:, 1:-1, 2:, 1:-1] - fv[:, 1:-1, :-2, 1:-1]) / 2

    def gradient_dz(self, fv): return (fv[:, 1:-1, 1:-1, 2:] - fv[:, 1:-1, 1:-1, :-2]) / 2

    def gradient_txyz(self, Txyz, fn):
        return torch.stack([fn(Txyz[:,i,...]) for i in [0, 1, 2]], dim=1)

    def compute_gradient_norm(self, displacement, flag_l1=False):
        dTdx = self.gradient_txyz(displacement, self.gradient_dx)
        dTdy = self.gradient_txyz(displacement, self.gradient_dy)
        dTdz = self.gradient_txyz(displacement, self.gradient_dz)
        if flag_l1:
            norms = torch.abs(dTdx) + torch.abs(dTdy) + torch.abs(dTdz)
        else:
            norms = dTdx**2 + dTdy**2 + dTdz**2
        return torch.mean(norms)/3.0

    def compute_bending_energy(self, displacement):
        dTdx = self.gradient_txyz(displacement, self.gradient_dx)
        dTdy = self.gradient_txyz(displacement, self.gradient_dy)
        dTdz = self.gradient_txyz(displacement, self.gradient_dz)
        dTdxx = self.gradient_txyz(dTdx, self.gradient_dx)
        dTdyy = self.gradient_txyz(dTdy, self.gradient_dy)
        dTdzz = self.gradient_txyz(dTdz, self.gradient_dz)
        dTdxy = self.gradient_txyz(dTdx, self.gradient_dy)
        dTdyz = self.gradient_txyz(dTdy, self.gradient_dz)
        dTdxz = self.gradient_txyz(dTdx, self.gradient_dz)
        return torch.mean(dTdxx**2 + dTdyy**2 + dTdzz**2 + 2*dTdxy**2 + 2*dTdxz**2 + 2*dTdyz**2)

    def forward(self, disp, _):
        if self.energy_type == 'bending':
            energy = self.compute_bending_energy(disp)
        elif self.energy_type == 'gradient-l2':
            energy = self.compute_gradient_norm(disp)
        elif self.energy_type == 'gradient-l1':
            energy = self.compute_gradient_norm(disp, flag_l1=True)
        else:
            raise Exception('Not recognised local regulariser!')
        return energy


class DiceLoss(nn.Module):
    """Dice and Xentropy loss"""
    def __init__(self, num_class=36, if_onehot=False):
        super().__init__()
        self.num_class = num_class
        self.if_onehot = if_onehot

    def forward(self, y_pred, y_true):
        if self.if_onehot:
            y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)
            y_true = torch.squeeze(y_true, 1)
            y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()
        intersection = y_pred * y_true
        intersection = intersection.sum(dim=[2, 3, 4])
        union = torch.pow(y_pred, 1).sum(dim=[2, 3, 4]) + torch.pow(y_true, 1).sum(dim=[2, 3, 4])
        dsc = (2.*intersection) / (union + 1e-5)
        dsc = (1-torch.mean(dsc))
        return dsc


class NCC_vxm(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss.
    """
    def __init__(self, win=None):
        super(NCC_vxm, self).__init__()
        self.win = win

    def forward(self, y_true, y_pred):

        Ii = y_true
        Ji = y_pred

        ndims = len(list(Ii.size())) - 2
        assert ndims in [1, 2, 3], "volumes should be 1 to 3 dimensions. found: %d" % ndims
        win = [9] * ndims if self.win is None else self.win
        sum_filt = torch.ones([1, 1, *win]).to("cuda")
        pad_no = math.floor(win[0] / 2)

        if ndims == 1:
            stride = (1)
            padding = (pad_no)
        elif ndims == 2:
            stride = (1, 1)
            padding = (pad_no, pad_no)
        else:
            stride = (1, 1, 1)
            padding = (pad_no, pad_no, pad_no)

        conv_fn = getattr(F, 'conv%dd' % ndims)

        I2 = Ii * Ii
        J2 = Ji * Ji
        IJ = Ii * Ji

        I_sum = conv_fn(Ii, sum_filt, stride=stride, padding=padding)
        J_sum = conv_fn(Ji, sum_filt, stride=stride, padding=padding)
        I2_sum = conv_fn(I2, sum_filt, stride=stride, padding=padding)
        J2_sum = conv_fn(J2, sum_filt, stride=stride, padding=padding)
        IJ_sum = conv_fn(IJ, sum_filt, stride=stride, padding=padding)

        win_size = np.prod(win)
        u_I = I_sum / win_size
        u_J = J_sum / win_size

        cross = IJ_sum - u_J * I_sum - u_I * J_sum + u_I * u_J * win_size
        I_var = I2_sum - 2 * u_I * I_sum + u_I * u_I * win_size
        J_var = J2_sum - 2 * u_J * J_sum + u_J * u_J * win_size

        cc = cross * cross / (I_var * J_var + 1e-5)

        return -torch.mean(cc)


class NCC(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss.
    """
    def __init__(self, win=None):
        super(NCC, self).__init__()
        self.win = win

    def forward(self, y_true, y_pred):
        Ii = y_true#/100
        Ji = y_pred#/100

        ndims = len(list(Ii.size())) - 2
        assert ndims in [1, 2, 3], "volumes should be 1 to 3 dimensions. found: %d" % ndims
        win = [9] * ndims if self.win is None else [self.win] * ndims
        sum_filt = torch.ones([1, 1, *win]).to("cuda")/float(np.prod(win))
        pad_no = win[0] // 2

        if ndims == 1:
            stride = (1)
            padding = (pad_no)
        elif ndims == 2:
            stride = (1, 1)
            padding = (pad_no, pad_no)
        else:
            stride = (1, 1, 1)
            padding = (pad_no, pad_no, pad_no)

        conv_fn = getattr(F, 'conv%dd' % ndims)

        mu1 = conv_fn(Ii, sum_filt, padding=padding, stride=stride)
        mu2 = conv_fn(Ji, sum_filt, padding=padding, stride=stride)

        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2

        sigma1_sq = conv_fn(Ii * Ii, sum_filt, padding=padding, stride=stride) - mu1_sq
        sigma2_sq = conv_fn(Ji * Ji, sum_filt, padding=padding, stride=stride) - mu2_sq
        sigma12 = conv_fn(Ii * Ji, sum_filt, padding=padding, stride=stride) - mu1_mu2

        cc = (sigma12 * sigma12) / (sigma1_sq * sigma2_sq + 1e-5)
        return - torch.mean(cc)


class MIND_loss(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss.
    """
    def __init__(self, win=None):
        super(MIND_loss, self).__init__()
        self.win = win

    def pdist_squared(self, x):
        xx = (x ** 2).sum(dim=1).unsqueeze(2)
        yy = xx.permute(0, 2, 1)
        dist = xx + yy - 2.0 * torch.bmm(x.permute(0, 2, 1), x)
        dist[dist != dist] = 0
        dist = torch.clamp(dist, 0.0, np.inf)
        return dist

    def MINDSSC(self, img, radius=2, dilation=2):
        # see http://mpheinrich.de/pub/miccai2013_943_mheinrich.pdf for details on the MIND-SSC descriptor

        # kernel size
        kernel_size = radius * 2 + 1

        # define start and end locations for self-similarity pattern
        six_neighbourhood = torch.Tensor([[0, 1, 1],
                                          [1, 1, 0],
                                          [1, 0, 1],
                                          [1, 1, 2],
                                          [2, 1, 1],
                                          [1, 2, 1]]).long()

        # squared distances
        dist = self.pdist_squared(six_neighbourhood.t().unsqueeze(0)).squeeze(0)

        # define comparison mask
        x, y = torch.meshgrid(torch.arange(6), torch.arange(6), indexing='ij')
        mask = ((x > y).view(-1) & (dist == 2).view(-1))

        # build kernel
        idx_shift1 = six_neighbourhood.unsqueeze(1).repeat(1, 6, 1).view(-1, 3)[mask, :]
        idx_shift2 = six_neighbourhood.unsqueeze(0).repeat(6, 1, 1).view(-1, 3)[mask, :]
        mshift1 = torch.zeros(12, 1, 3, 3, 3).cuda()
        mshift1.view(-1)[torch.arange(12) * 27 + idx_shift1[:, 0] * 9 + idx_shift1[:, 1] * 3 + idx_shift1[:, 2]] = 1
        mshift2 = torch.zeros(12, 1, 3, 3, 3).cuda()
        mshift2.view(-1)[torch.arange(12) * 27 + idx_shift2[:, 0] * 9 + idx_shift2[:, 1] * 3 + idx_shift2[:, 2]] = 1
        rpad1 = nn.ReplicationPad3d(dilation)
        rpad2 = nn.ReplicationPad3d(radius)

        # compute patch-ssd
        ssd = F.avg_pool3d(rpad2(
            (F.conv3d(rpad1(img), mshift1, dilation=dilation) - F.conv3d(rpad1(img), mshift2, dilation=dilation)) ** 2),
                           kernel_size, stride=1)

        # MIND equation
        mind = ssd - torch.min(ssd, 1, keepdim=True)[0]
        mind_var = torch.mean(mind, 1, keepdim=True)
        mind_var = torch.clamp(mind_var, (mind_var.mean() * 0.001).item(), (mind_var.mean() * 1000).item())
        mind /= mind_var
        mind = torch.exp(-mind)

        # permute to have same ordering as C++ code
        mind = mind[:, torch.Tensor([6, 8, 1, 11, 2, 10, 0, 7, 9, 4, 5, 3]).long(), :, :, :]

        return mind

    def forward(self, y_pred, y_true):
        return torch.mean((self.MINDSSC(y_pred) - self.MINDSSC(y_true)) ** 2)


class MutualInformation(torch.nn.Module):
    """
    Mutual Information
    """
    def __init__(self, sigma_ratio=1, minval=0., maxval=1., num_bin=32):
        super(MutualInformation, self).__init__()

        """Create bin centers"""
        bin_centers = np.linspace(minval, maxval, num=num_bin)
        vol_bin_centers = Variable(torch.linspace(minval, maxval, num_bin), requires_grad=False).cuda()
        num_bins = len(bin_centers)

        """Sigma for Gaussian approx."""
        sigma = np.mean(np.diff(bin_centers)) * sigma_ratio
        print(sigma)

        self.preterm = 1 / (2 * sigma ** 2)
        self.bin_centers = bin_centers
        self.max_clip = maxval
        self.num_bins = num_bins
        self.vol_bin_centers = vol_bin_centers

    def mi(self, y_true, y_pred):
        y_pred = torch.clamp(y_pred, 0., self.max_clip)
        y_true = torch.clamp(y_true, 0, self.max_clip)

        y_true = y_true.view(y_true.shape[0], -1)
        y_true = torch.unsqueeze(y_true, 2)
        y_pred = y_pred.view(y_pred.shape[0], -1)
        y_pred = torch.unsqueeze(y_pred, 2)

        nb_voxels = y_pred.shape[1]  # total num of voxels

        """Reshape bin centers"""
        o = [1, 1, np.prod(self.vol_bin_centers.shape)]
        vbc = torch.reshape(self.vol_bin_centers, o).cuda()

        """compute image terms by approx. Gaussian dist."""
        I_a = torch.exp(- self.preterm * torch.square(y_true - vbc))
        I_a = I_a / torch.sum(I_a, dim=-1, keepdim=True)

        I_b = torch.exp(- self.preterm * torch.square(y_pred - vbc))
        I_b = I_b / torch.sum(I_b, dim=-1, keepdim=True)

        # compute probabilities
        pab = torch.bmm(I_a.permute(0, 2, 1), I_b)
        pab = pab / nb_voxels
        pa = torch.mean(I_a, dim=1, keepdim=True)
        pb = torch.mean(I_b, dim=1, keepdim=True)

        papb = torch.bmm(pa.permute(0, 2, 1), pb) + 1e-6
        mi = torch.sum(torch.sum(pab * torch.log(pab / papb + 1e-6), dim=1), dim=1)
        return mi.mean()  # average across batch

    def forward(self, y_true, y_pred):
        return -self.mi(y_true, y_pred)


class localMutualInformation(torch.nn.Module):
    """
    Local Mutual Information for non-overlapping patches
    """
    def __init__(self, sigma_ratio=1, minval=0., maxval=1., num_bin=32, patch_size=5):
        super(localMutualInformation, self).__init__()

        """Create bin centers"""
        bin_centers = np.linspace(minval, maxval, num=num_bin)
        vol_bin_centers = Variable(torch.linspace(minval, maxval, num_bin), requires_grad=False).cuda()
        num_bins = len(bin_centers)

        """Sigma for Gaussian approx."""
        sigma = np.mean(np.diff(bin_centers)) * sigma_ratio

        self.preterm = 1 / (2 * sigma ** 2)
        self.bin_centers = bin_centers
        self.max_clip = maxval
        self.num_bins = num_bins
        self.vol_bin_centers = vol_bin_centers
        self.patch_size = patch_size

    def local_mi(self, y_true, y_pred):
        y_pred = torch.clamp(y_pred, 0., self.max_clip)
        y_true = torch.clamp(y_true, 0, self.max_clip)

        """Reshape bin centers"""
        o = [1, 1, np.prod(self.vol_bin_centers.shape)]
        vbc = torch.reshape(self.vol_bin_centers, o).cuda()

        """Making image paddings"""
        if len(list(y_pred.size())[2:]) == 3:
            ndim = 3
            x, y, z = list(y_pred.size())[2:]
            # compute padding sizes
            x_r = -x % self.patch_size
            y_r = -y % self.patch_size
            z_r = -z % self.patch_size
            padding = (z_r // 2, z_r - z_r // 2, y_r // 2, y_r - y_r // 2, x_r // 2, x_r - x_r // 2, 0, 0, 0, 0)
        elif len(list(y_pred.size())[2:]) == 2:
            ndim = 2
            x, y = list(y_pred.size())[2:]
            # compute padding sizes
            x_r = -x % self.patch_size
            y_r = -y % self.patch_size
            padding = (y_r // 2, y_r - y_r // 2, x_r // 2, x_r - x_r // 2, 0, 0, 0, 0)
        else:
            raise Exception('Supports 2D and 3D but not {}'.format(list(y_pred.size())))
        y_true = F.pad(y_true, padding, "constant", 0)
        y_pred = F.pad(y_pred, padding, "constant", 0)

        """Reshaping images into non-overlapping patches"""
        if ndim == 3:
            y_true_patch = torch.reshape(y_true, (y_true.shape[0], y_true.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size,
                                                  (z + z_r) // self.patch_size, self.patch_size))
            y_true_patch = y_true_patch.permute(0, 1, 2, 4, 6, 3, 5, 7)
            y_true_patch = torch.reshape(y_true_patch, (-1, self.patch_size ** 3, 1))

            y_pred_patch = torch.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size,
                                                  (z + z_r) // self.patch_size, self.patch_size))
            y_pred_patch = y_pred_patch.permute(0, 1, 2, 4, 6, 3, 5, 7)
            y_pred_patch = torch.reshape(y_pred_patch, (-1, self.patch_size ** 3, 1))
        else:
            y_true_patch = torch.reshape(y_true, (y_true.shape[0], y_true.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size))
            y_true_patch = y_true_patch.permute(0, 1, 2, 4, 3, 5)
            y_true_patch = torch.reshape(y_true_patch, (-1, self.patch_size ** 2, 1))

            y_pred_patch = torch.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size))
            y_pred_patch = y_pred_patch.permute(0, 1, 2, 4, 3, 5)
            y_pred_patch = torch.reshape(y_pred_patch, (-1, self.patch_size ** 2, 1))

        """Compute MI"""
        I_a_patch = torch.exp(- self.preterm * torch.square(y_true_patch - vbc))
        I_a_patch = I_a_patch / torch.sum(I_a_patch, dim=-1, keepdim=True)

        I_b_patch = torch.exp(- self.preterm * torch.square(y_pred_patch - vbc))
        I_b_patch = I_b_patch / torch.sum(I_b_patch, dim=-1, keepdim=True)

        pab = torch.bmm(I_a_patch.permute(0, 2, 1), I_b_patch)
        pab = pab / self.patch_size ** ndim
        pa = torch.mean(I_a_patch, dim=1, keepdim=True)
        pb = torch.mean(I_b_patch, dim=1, keepdim=True)

        papb = torch.bmm(pa.permute(0, 2, 1), pb) + 1e-6
        mi = torch.sum(torch.sum(pab * torch.log(pab / papb + 1e-6), dim=1), dim=1)
        return mi.mean()

    def forward(self, y_true, y_pred):
        return -self.local_mi(y_true, y_pred)
==================================================

üìÑ utils/misc.py
--------------------------------------------------
import torch
from torch import nn


def get_mc_preds(net, inputs, mc_iter: int = 25):
    img_list, flow_list = [], []
    with torch.no_grad():
        for _ in range(mc_iter):
            img, flow = net(inputs)
            img_list.append(img)
            flow_list.append(flow)
    return img_list, flow_list


def calc_error(tar, img_list):
    sqr_diffs = [(img - tar) ** 2 for img in img_list]
    return torch.mean(torch.cat(sqr_diffs, dim=0), dim=0, keepdim=True)


def get_mc_preds_w_errors(net, inputs, target, mc_iter: int = 25):
    img_list, flow_list, err = [], [], []
    mse = nn.MSELoss()
    with torch.no_grad():
        for _ in range(mc_iter):
            img, flow = net(inputs)
            img_list.append(img)
            flow_list.append(flow)
            err.append(mse(img, target).item())
    return img_list, flow_list, err


def get_diff_mc_preds(net, inputs, mc_iter: int = 25):
    img_list, flow_list, disp_list = [], [], []
    with torch.no_grad():
        for _ in range(mc_iter):
            img, _, flow, disp = net(inputs)
            img_list.append(img)
            flow_list.append(flow)
            disp_list.append(disp)
    return img_list, flow_list, disp_list


def uncert_regression_gal(img_list, reduction="mean"):
    img_list = torch.cat(img_list, dim=0)
    ale = img_list[:, -1:].mean(dim=0, keepdim=True)
    epi = torch.var(img_list[:, :-1], dim=0, keepdim=True).mean(dim=1, keepdim=True)
    uncert = ale + epi
    if reduction == "mean":
        return ale.mean().item(), epi.mean().item(), uncert.mean().item()
    if reduction == "sum":
        return ale.sum().item(), epi.sum().item(), uncert.sum().item()
    return ale.detach(), epi.detach(), uncert.detach()


def uceloss(errors, uncert, n_bins=15, outlier=0.0, range=None):
    device = errors.device
    if range is None:
        bin_boundaries = torch.linspace(uncert.min().item(), uncert.max().item(), n_bins + 1, device=device)
    else:
        bin_boundaries = torch.linspace(range[0], range[1], n_bins + 1, device=device)

    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]

    uce = torch.zeros(1, device=device)
    errors_in_bin_list, avg_uncert_in_bin_list, prop_in_bin_list = [], [], []

    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = uncert.gt(bin_lower.item()) * uncert.le(bin_upper.item())
        prop_in_bin = in_bin.float().mean()
        prop_in_bin_list.append(prop_in_bin)
        if prop_in_bin.item() > outlier:
            errors_in_bin = errors[in_bin].float().mean()
            avg_uncert_in_bin = uncert[in_bin].mean()
            uce += torch.abs(avg_uncert_in_bin - errors_in_bin) * prop_in_bin
            errors_in_bin_list.append(errors_in_bin)
            avg_uncert_in_bin_list.append(avg_uncert_in_bin)

    err_in_bin = torch.tensor(errors_in_bin_list, device=device)
    avg_uncert_in_bin = torch.tensor(avg_uncert_in_bin_list, device=device)
    prop_in_bin = torch.tensor(prop_in_bin_list, device=device)
    return uce, err_in_bin, avg_uncert_in_bin, prop_in_bin
==================================================

üìÑ utils/rand.py
--------------------------------------------------
import random


class Uniform(object):
    def __init__(self, a, b):
        self.a = a
        self.b = b

    def sample(self):
        return random.uniform(self.a, self.b)


class Gaussian(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def sample(self):
        return random.gauss(self.mean, self.std)


class Constant(object):
    def __init__(self, val):
        self.val = val

    def sample(self):
        return self.val
==================================================

üìÑ utils/spatial.py
--------------------------------------------------
import numpy as np
import torch
import torch.nn.functional as F
from torch import nn
import pystrum.pynd.ndutils as nd


class SpatialTransformer(nn.Module):
    """
    Double of ST from TM-DCA model.
    """
    def __init__(self, size, mode: str = "bilinear"):
        super().__init__()
        self.mode = mode

        vectors = [torch.arange(0, s) for s in size]
        grids = torch.meshgrid(vectors, indexing="ij")
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)
        self.register_buffer("grid", grid, persistent=False)

    def forward(self, src: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:
        new_locs = self.grid + flow
        shape = flow.shape[2:]

        # normalize to [-1, 1]
        for i in range(len(shape)):
            new_locs[:, i, ...] = 2.0 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)

        if len(shape) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(shape) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return F.grid_sample(src, new_locs, align_corners=False, mode=self.mode)


class register_model(nn.Module):
    """
    Wrapper used in old trainers: forward([img, flow]) -> warped_img.
    """
    def __init__(self, img_size=(64, 256, 256), mode="bilinear"):
        super().__init__()
        self.spatial_trans = SpatialTransformer(img_size, mode)

    def forward(self, x):
        img, flow = x[0], x[1]
        return self.spatial_trans(img, flow)


def jacobian_determinant_vxm(disp: np.ndarray) -> np.ndarray:
    """
    Jacobian determinant of a displacement field (numpy version, VXM-style).
    disp: [*vol_shape, nb_dims] OR with transpose needed.
    """
    disp = disp.transpose(1, 2, 3, 0)
    volshape = disp.shape[:-1]
    nb_dims = len(volshape)
    assert nb_dims in (2, 3), "flow has to be 2D or 3D"

    grid_lst = nd.volsize2ndgrid(volshape)
    grid = np.stack(grid_lst, len(volshape))
    J = np.gradient(disp + grid)

    if nb_dims == 3:
        dx, dy, dz = J[0], J[1], J[2]
        Jdet0 = dx[..., 0] * (dy[..., 1] * dz[..., 2] - dy[..., 2] * dz[..., 1])
        Jdet1 = dx[..., 1] * (dy[..., 0] * dz[..., 2] - dy[..., 2] * dz[..., 0])
        Jdet2 = dx[..., 2] * (dy[..., 0] * dz[..., 1] - dy[..., 1] * dz[..., 0])
        return Jdet0 - Jdet1 + Jdet2
    else:
        dfdx, dfdy = J[0], J[1]
        return dfdx[..., 0] * dfdy[..., 1] - dfdy[..., 0] * dfdx[..., 1]
==================================================

üìÑ utils/train.py
--------------------------------------------------
from __future__ import annotations

import os
import sys
import glob
import time
from dataclasses import dataclass
from typing import Optional, Dict, Any

import numpy as np
import torch
from torch import optim
import matplotlib.pyplot as plt
from natsort import natsorted


class Logger:
    """
    Mirrors stdout to both console and logs/<exp>/logfile.log
    """
    def __init__(self, log_dir: str, filename: str = "logfile.log"):
        self.terminal = sys.stdout
        os.makedirs(log_dir, exist_ok=True)
        self.log_path = os.path.join(log_dir, filename)
        self.log = open(self.log_path, "a", encoding="utf-8")

    def write(self, message: str):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        # required for python logging compatibility
        self.log.flush()


def attach_stdout_logger(log_dir: str) -> Logger:
    """
    Redirect sys.stdout to Logger(log_dir). Returns created logger (so you can keep ref if needed).
    """
    logger = Logger(log_dir)
    sys.stdout = logger
    return logger


@dataclass(frozen=True)
class DeviceInfo:
    device: torch.device
    gpu_id: int
    gpu_name: Optional[str]


def setup_device(gpu_id: int = 0, seed: int = 0, deterministic: bool = False) -> DeviceInfo:
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.set_device(gpu_id)
        name = torch.cuda.get_device_name(gpu_id)
        if deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
        else:
            torch.backends.cudnn.benchmark = True
        print(f"Number of GPU: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"     GPU #{i}: {torch.cuda.get_device_name(i)}")
        print(f"Currently using: {name}")
        print(f"If the GPU is available? True")
        return DeviceInfo(device=torch.device("cuda", gpu_id), gpu_id=gpu_id, gpu_name=name)
    else:
        print("CUDA not available, using CPU.")
        return DeviceInfo(device=torch.device("cpu"), gpu_id=-1, gpu_name=None)


@dataclass(frozen=True)
class ExperimentPaths:
    exp_dir: str
    log_dir: str


def make_exp_dirs(exp_name: str) -> ExperimentPaths:
    exp_root = exp_name.rstrip("/\\")
    exp_dir = os.path.join("results", exp_root)
    log_dir = os.path.join("logs", exp_root)
    os.makedirs(exp_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    return ExperimentPaths(exp_dir=exp_dir, log_dir=log_dir)


def adjust_learning_rate_poly(
    optimizer: optim.Optimizer,
    epoch: int,
    max_epochs: int,
    init_lr: float,
    power: float = 0.9
) -> float:
    """
    Polynomial LR schedule: lr = init_lr * (1 - epoch/max_epochs)^power
    """
    new_lr = float(np.round(init_lr * np.power(1 - (epoch / max_epochs), power), 8))
    for pg in optimizer.param_groups:
        pg["lr"] = new_lr
    return new_lr


def save_checkpoint(state: Dict[str, Any], save_dir: str, filename: str, max_model_num: int = 8) -> None:
    os.makedirs(save_dir, exist_ok=True)
    ckpt_path = os.path.join(save_dir, filename)
    torch.save(state, ckpt_path)

    model_lists = natsorted(glob.glob(os.path.join(save_dir, "*")))
    while len(model_lists) > max_model_num:
        os.remove(model_lists[0])
        model_lists = natsorted(glob.glob(os.path.join(save_dir, "*")))


def load_checkpoint_if_exists(
    ckpt_path: str,
    model: torch.nn.Module,
    optimizer: Optional[optim.Optimizer] = None,
    map_location: str | torch.device = "cpu",
) -> Optional[Dict[str, Any]]:
    if not os.path.exists(ckpt_path):
        return None
    ckpt = torch.load(ckpt_path, map_location=map_location)
    if "state_dict" in ckpt:
        model.load_state_dict(ckpt["state_dict"])
    else:
        model.load_state_dict(ckpt)
    if optimizer is not None and "optimizer" in ckpt:
        optimizer.load_state_dict(ckpt["optimizer"])
    return ckpt


@dataclass(frozen=True)
class PerfInfo:
    epoch_time_sec: float
    mean_iter_time_ms: float
    peak_gpu_mem_gib: Optional[float]


def perf_epoch_start() -> float:
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
    return time.perf_counter()


def perf_epoch_end(t0: float, iters: int, iter_time_sum: float) -> PerfInfo:
    epoch_time = time.perf_counter() - t0
    mean_iter_ms = (iter_time_sum / max(1, iters)) * 1000.0
    peak = None
    if torch.cuda.is_available():
        peak = torch.cuda.max_memory_reserved() / (1024 ** 3)
    return PerfInfo(epoch_time_sec=epoch_time, mean_iter_time_ms=mean_iter_ms, peak_gpu_mem_gib=peak)


def comput_fig(img: torch.Tensor) -> plt.Figure:
    """
    16 axial slices from [B, C, D, H, W], assumes B=1, C=1.
    """
    arr = img.detach().float().cpu().numpy()[0, 0]
    z0 = min(48, max(0, arr.shape[0] - 16))
    arr = arr[z0:z0 + 16]

    fig = plt.figure(figsize=(12, 12), dpi=180)
    for i in range(arr.shape[0]):
        plt.subplot(4, 4, i + 1)
        plt.axis("off")
        plt.imshow(arr[i, :, :], cmap="gray")
    fig.subplots_adjust(wspace=0, hspace=0)
    return fig


def mk_grid_img(grid_step: int, line_thickness: int = 1, grid_sz=(160, 192, 224), device: Optional[torch.device] = None) -> torch.Tensor:
    """
    Binary 3D grid [1, 1, D, H, W] for deformation visualization.
    grid_sz must match the image spatial shape.
    """
    d, h, w = grid_sz
    grid_img = np.zeros((d, h, w), dtype=np.float32)

    for j in range(0, h, grid_step):
        jj = min(h - 1, j + line_thickness - 1)
        grid_img[:, jj, :] = 1.0
    for i in range(0, w, grid_step):
        ii = min(w - 1, i + line_thickness - 1)
        grid_img[:, :, ii] = 1.0

    out = torch.from_numpy(grid_img[None, None, ...])
    if device is not None:
        out = out.to(device, non_blocking=True)
    return out
==================================================

üìÑ utils/trans.py
--------------------------------------------------
# import math
import random
from collections.abc import Sequence
import numpy as np
import torch, random, math
from scipy import ndimage

from .rand import Constant
from scipy.ndimage import rotate
from skimage.transform import resize


class Base(object):
    def sample(self, *shape):
        return shape

    def tf(self, img, k=0):
        return img

    def __call__(self, img, dim=3, reuse=False): # class -> func()
        # image: nhwtc
        # shape: no first dims
        if not reuse:
            im = img if isinstance(img, np.ndarray) else img[0]
            shape = im.shape[1:dim+1]
            self.sample(*shape)

        if isinstance(img, Sequence):
            return [self.tf(x, k) for k, x in enumerate(img)] # img:k=0,label:k=1

        return self.tf(img)

    def __str__(self):
        return 'Identity()'


Identity = Base


# gemetric transformations, need a buffers
# first axis is N
class Rot90(Base):
    def __init__(self, axes=(0, 1)):
        self.axes = axes

        for a in self.axes:
            assert a > 0

    def sample(self, *shape):
        shape = list(shape)
        i, j = self.axes

        # shape: no first dim
        i, j = i-1, j-1
        shape[i], shape[j] = shape[j], shape[i]

        return shape

    def tf(self, img, k=0):
        return np.rot90(img, axes=self.axes)

    def __str__(self):
        return 'Rot90(axes=({}, {})'.format(*self.axes)


class RandomRotion(Base):
    def __init__(self,angle_spectrum=10):
        assert isinstance(angle_spectrum,int)
        axes = [(1, 0), (2, 1),(2, 0)]
        self.angle_spectrum = angle_spectrum
        self.axes = axes

    def sample(self,*shape):
        self.axes_buffer = self.axes[np.random.choice(list(range(len(self.axes))))] # choose the random direction
        self.angle_buffer = np.random.randint(-self.angle_spectrum, self.angle_spectrum) # choose the random direction
        return list(shape)

    def tf(self, img, k=0):
        """ Introduction: The rotation function supports the shape [H,W,D,C] or shape [H,W,D]
        :param img: if x, shape is [1,H,W,D,c]; if label, shape is [1,H,W,D]
        :param k: if x, k=0; if label, k=1
        """
        bsize = img.shape[0]

        for bs in range(bsize):
            if k == 0:
                channels = [rotate(img[bs,:,:,:,c], self.angle_buffer, axes=self.axes_buffer, reshape=False, order=0, mode='constant', cval=-1) for c in
                            range(img.shape[4])]
                img[bs,...] = np.stack(channels, axis=-1)

            if k == 1:
                img[bs,...] = rotate(img[bs,...], self.angle_buffer, axes=self.axes_buffer, reshape=False, order=0, mode='constant', cval=-1)

        return img

    def __str__(self):
        return 'RandomRotion(axes={},Angle:{}'.format(self.axes_buffer,self.angle_buffer)


class Flip(Base):
    def __init__(self, axis=0):
        self.axis = axis

    def tf(self, img, k=0):
        return np.flip(img, self.axis)

    def __str__(self):
        return 'Flip(axis={})'.format(self.axis)


class RandomFlip(Base):
    # mirror flip across all x,y,z
    def __init__(self,axis=0):
        # assert axis == (1,2,3) # For both data and label, it has to specify the axis.
        self.axis = (1,2,3)
        self.x_buffer = None
        self.y_buffer = None
        self.z_buffer = None

    def sample(self, *shape):
        self.x_buffer = np.random.choice([True,False])
        self.y_buffer = np.random.choice([True,False])
        self.z_buffer = np.random.choice([True,False])
        return list(shape) # the shape is not changed

    def tf(self,img,k=0): # img shape is (1, 240, 240, 155, 4)
        if self.x_buffer:
            img = np.flip(img,axis=self.axis[0])
        if self.y_buffer:
            img = np.flip(img,axis=self.axis[1])
        if self.z_buffer:
            img = np.flip(img,axis=self.axis[2])
        return img


class RandSelect(Base):
    def __init__(self, prob=0.5, tf=None):
        self.prob = prob
        self.ops  = tf if isinstance(tf, Sequence) else (tf, )
        self.buff = False

    def sample(self, *shape):
        self.buff = random.random() < self.prob

        if self.buff:
            for op in self.ops:
                shape = op.sample(*shape)

        return shape

    def tf(self, img, k=0):
        if self.buff:
            for op in self.ops:
                img = op.tf(img, k)
        return img

    def __str__(self):
        if len(self.ops) == 1:
            ops = str(self.ops[0])
        else:
            ops = '[{}]'.format(', '.join([str(op) for op in self.ops]))
        return 'RandSelect({}, {})'.format(self.prob, ops)


class CenterCrop(Base):
    def __init__(self, size):
        self.size = size
        self.buffer = None

    def sample(self, *shape):
        size = self.size
        start = [(s -size)//2 for s in shape]
        self.buffer = [slice(None)] + [slice(s, s+size) for s in start]
        return [size] * len(shape)

    def tf(self, img, k=0):
        return img[tuple(self.buffer)]

    def __str__(self):
        return 'CenterCrop({})'.format(self.size)


class CenterCropBySize(CenterCrop):
    def sample(self, *shape):
        assert len(self.size) == 3  # random crop [H,W,T] from img [240,240,155]
        if not isinstance(self.size, list):
            size = list(self.size)
        else:
            size = self.size
        start = [(s-i)//2 for i, s in zip(size, shape)]
        self.buffer = [slice(None)] + [slice(s, s+i) for i, s in zip(size, start)]
        return size

    def __str__(self):
        return 'CenterCropBySize({})'.format(self.size)


class RandCrop(CenterCrop):
    def sample(self, *shape):
        size = self.size
        start = [random.randint(0, s-size) for s in shape]
        self.buffer = [slice(None)] + [slice(s, s+size) for s in start]
        return [size]*len(shape)

    def __str__(self):
        return 'RandCrop({})'.format(self.size)


class RandCrop3D(CenterCrop):
    def sample(self, *shape): # shape : [240,240,155]
        assert len(self.size)==3 # random crop [H,W,T] from img [240,240,155]
        if not isinstance(self.size,list):
            size = list(self.size)
        else:
            size = self.size
        start = [random.randint(0, s-i) for i,s in zip(size,shape)]
        self.buffer = [slice(None)] + [slice(s, s+k) for s,k in zip(start,size)]
        return size

    def __str__(self):
        return 'RandCrop({})'.format(self.size)


# for data only
class RandomIntensityChange(Base):
    def __init__(self,factor):
        shift,scale = factor
        assert (shift >0) and (scale >0)
        self.shift = shift
        self.scale = scale

    def tf(self,img,k=0):
        if k==1:
            return img

        shift_factor = np.random.uniform(-self.shift,self.shift,size=[1,img.shape[1],1,1,img.shape[4]]) # [-0.1,+0.1]
        scale_factor = np.random.uniform(1.0 - self.scale, 1.0 + self.scale,size=[1,img.shape[1],1,1,img.shape[4]]) # [0.9,1.1)
        return img * scale_factor + shift_factor

    def __str__(self):
        return 'random intensity shift per channels on the input image, including'


class RandomGammaCorrection(Base):
    def __init__(self,factor):
        lower, upper = factor
        assert (lower >0) and (upper >0)
        self.lower = lower
        self.upper = upper

    def tf(self,img,k=0):
        if k==1:
            return img
        img = img + np.min(img)
        img_max = np.max(img)
        img = img/img_max
        factor = random.choice(np.arange(self.lower, self.upper, 0.1))
        gamma = random.choice([1, factor])
        if gamma == 1:
            return img
        img = img ** gamma * img_max
        img = (img - img.mean())/img.std()
        return img

    def __str__(self):
        return 'random intensity shift per channels on the input image, including'


class MinMax_norm(Base):
    def __init__(self, ):
        a = None

    def tf(self, img, k=0):
        if k == 1:
            return img
        img = (img - img.min()) / (img.max()-img.min())
        return img


class Seg_norm(Base):
    def __init__(self, ):
        a = None
        self.seg_table = np.array([0, 2, 3, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 24, 26,
                          28, 30, 31, 41, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 58, 60, 62,
                          63, 72, 77, 80, 85, 251, 252, 253, 254, 255])
    def tf(self, img, k=0):
        if k == 0:
            return img
        img_out = np.zeros_like(img)
        for i in range(len(self.seg_table)):
            img_out[img == self.seg_table[i]] = i
        return img_out


class Resize_img(Base):
    def __init__(self, shape):
        self.shape = shape

    def tf(self, img, k=0):
        if k == 1:
            img = resize(img, (img.shape[0], self.shape[0], self.shape[1], self.shape[2]),
                         anti_aliasing=False, order=0)
        else:
            img = resize(img, (img.shape[0], self.shape[0], self.shape[1], self.shape[2]),
                         anti_aliasing=False, order=3)
        return img


class Pad(Base):
    def __init__(self, pad): # [0,0,0,5,0]
        self.pad = pad
        self.px = tuple(zip([0]*len(pad), pad))

    def sample(self, *shape):

        shape = list(shape)

        # shape: no first dim
        for i in range(len(shape)):
            shape[i] += self.pad[i+1]

        return shape

    def tf(self, img, k=0):
        #nhwtc, nhwt
        dim = len(img.shape)
        return np.pad(img, self.px[:dim], mode='constant')

    def __str__(self):
        return 'Pad(({}, {}, {}))'.format(*self.pad)


class Pad3DIfNeeded(Base):
    def __init__(self, shape, value=0, mask_value=0): # [0,0,0,5,0]
        self.shape = shape
        self.value = value
        self.mask_value = mask_value

    def tf(self, img, k=0):
        pad = [(0,0)]
        if k==0:
            img_shape = img.shape[1:-1]
        else:
            img_shape = img.shape[1:]
        for i, t in zip(img_shape, self.shape):
            if i < t:
                diff = t-i
                pad.append((math.ceil(diff/2),math.floor(diff/2)))
            else:
                pad.append((0,0))
        if k == 0:
            pad.append((0,0))
        pad = tuple(pad)
        if k==0:
            return np.pad(img, pad, mode='constant', constant_values=img.min())
        else:
            return np.pad(img, pad, mode='constant', constant_values=self.mask_value)

    def __str__(self):
        return 'Pad(({}, {}, {}))'.format(*self.pad)


class Noise(Base):
    def __init__(self, dim, sigma=0.1, channel=True, num=-1):
        self.dim = dim
        self.sigma = sigma
        self.channel = channel
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img

        if self.channel:
            #nhwtc, hwtc, hwt
            shape = [1] if len(img.shape) < self.dim+2 else [img.shape[-1]]
        else:
            shape = img.shape
        return img * np.exp(self.sigma * torch.randn(shape, dtype=torch.float32).numpy())

    def __str__(self):
        return 'Noise()'


# dim could come from shape
class GaussianBlur(Base):
    def __init__(self, dim, sigma=Constant(1.5), app=-1):
        # 1.5 pixel
        self.dim = dim
        self.sigma = sigma
        self.eps   = 0.001
        self.app = app

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img

        # image is nhwtc
        for n in range(img.shape[0]):
            sig = self.sigma.sample()
            # sample each channel saperately to avoid correlations
            if sig > self.eps:
                if len(img.shape) == self.dim+2:
                    C = img.shape[-1]
                    for c in range(C):
                        img[n,..., c] = ndimage.gaussian_filter(img[n, ..., c], sig)
                elif len(img.shape) == self.dim+1:
                    img[n] = ndimage.gaussian_filter(img[n], sig)
                else:
                    raise ValueError('image shape is not supported')

        return img

    def __str__(self):
        return 'GaussianBlur()'


class ToNumpy(Base):
    def __init__(self, num=-1):
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        return img.numpy()

    def __str__(self):
        return 'ToNumpy()'


class ToTensor(Base):
    def __init__(self, num=-1):
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img

        return torch.from_numpy(img)

    def __str__(self):
        return 'ToTensor'


class TensorType(Base):
    def __init__(self, types, num=-1):
        self.types = types # ('torch.float32', 'torch.int64')
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        # make this work with both Tensor and Numpy
        return img.type(self.types[k])

    def __str__(self):
        s = ', '.join([str(s) for s in self.types])
        return 'TensorType(({}))'.format(s)


class NumpyType(Base):
    def __init__(self, types, num=-1):
        self.types = types # ('float32', 'int64')
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        # make this work with both Tensor and Numpy
        return img.astype(self.types[k])

    def __str__(self):
        s = ', '.join([str(s) for s in self.types])
        return 'NumpyType(({}))'.format(s)


class Normalize(Base):
    def __init__(self, mean=0.0, std=1.0, num=-1):
        self.mean = mean
        self.std = std
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        img -= self.mean
        img /= self.std
        return img

    def __str__(self):
        return 'Normalize()'


class Compose(Base):
    def __init__(self, ops):
        if not isinstance(ops, Sequence):
            ops = ops,
        self.ops = ops

    def sample(self, *shape):
        for op in self.ops:
            shape = op.sample(*shape)

    def tf(self, img, k=0):

        for op in self.ops:
            img = op.tf(img, k) # do not use op(img) here

        return img

    def __str__(self):
        ops = ', '.join([str(op) for op in self.ops])
        return 'Compose([{}])'.format(ops)
==================================================

üìÑ utils/validation.py
--------------------------------------------------
from __future__ import annotations
from dataclasses import dataclass
from typing import Callable, Dict, Optional, Any
import torch

from utils import AverageMeter, jacobian_det


@dataclass
class ValResult:
    dsc: float
    fold_percent: float
    last_vis: Dict[str, Any]


@torch.no_grad()
def validate_oasis(
    *,
    model: torch.nn.Module,
    val_loader,
    device: torch.device,
    forward_flow_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
    dice_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
    register_model_cls,
    mk_grid_img_fn: Optional[Callable[..., torch.Tensor]] = None,
    grid_step: int = 8,
    line_thickness: int = 1,
) -> ValResult:
    """
    Universal OASIS validation:
      - computes DSC on VOI labels using warped segmentation (nearest)
      - computes folding percent (detJ <= 0)
      - optionally returns a deformed grid for visualization

    Contract:
      forward_flow_fn(x, y) -> flow_full of shape [B,3,D,H,W] on `device`
    """
    model.eval()

    reg_nearest = None
    reg_bilin = None

    dsc_meter = AverageMeter()
    fold_meter = AverageMeter()

    last_vis: Dict[str, Any] = {}

    for batch in val_loader:
        # expected: x, y, x_seg, y_seg
        x, y, x_seg, y_seg = [t.to(device, non_blocking=True) for t in batch]
        vol_shape = tuple(x.shape[2:])  # (D,H,W) from tensor

        if reg_nearest is None:
            reg_nearest = register_model_cls(vol_shape, mode="nearest").to(device)
            reg_bilin = register_model_cls(vol_shape, mode="bilinear").to(device)

        flow = forward_flow_fn(x, y)  # [B,3,D,H,W]

        # warp seg (nearest)
        def_seg = reg_nearest([x_seg.float(), flow.float()])
        dsc = dice_fn(def_seg.long(), y_seg.long())
        dsc_meter.update(float(dsc), x.size(0))

        # fold %
        detJ = jacobian_det(flow.float())  # [B,1,D,H,W]
        fold = (detJ <= 0.0).float().mean() * 100.0
        fold_meter.update(float(fold), x.size(0))

        # optional grid for visuals (only keep last batch)
        def_grid = None
        if mk_grid_img_fn is not None:
            grid_img = mk_grid_img_fn(grid_step, line_thickness, vol_shape, device=device)
            def_grid = reg_bilin([grid_img.float(), flow.float()])

        last_vis = {
            "x_seg": x_seg,
            "y_seg": y_seg,
            "def_seg": def_seg,
            "def_grid": def_grid,
            "flow": flow,
        }

    return ValResult(
        dsc=dsc_meter.avg,
        fold_percent=fold_meter.avg,
        last_vis=last_vis,
    )
==================================================

üìÑ .gitignore
--------------------------------------------------
[–ë–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª]
==================================================

üìÑ environment.yml
--------------------------------------------------
—è—én a m e :   o a s i s - c t c f 
 
 c h a n n e l s : 
 
     -   c o n d a - f o r g e 
 
     -   d e f a u l t s 
 
 d e p e n d e n c i e s : 
 
     -   b z i p 2 = 1 . 0 . 8 
 
     -   c a - c e r t i f i c a t e s = 2 0 2 5 . 1 1 . 4 
 
     -   e x p a t = 2 . 7 . 3 
 
     -   l i b f f i = 3 . 4 . 4 
 
     -   l i b z l i b = 1 . 3 . 1 
 
     -   o p e n s s l = 3 . 0 . 1 8 
 
     -   p i p = 2 5 . 2 
 
     -   p y t h o n = 3 . 1 0 . 1 9 
 
     -   s e t u p t o o l s = 8 0 . 9 . 0 
 
     -   s q l i t e = 3 . 5 1 . 0 
 
     -   t k = 8 . 6 . 1 5 
 
     -   t z d a t a = 2 0 2 5 b 
 
     -   u c r t = 1 0 . 0 . 2 2 6 2 1 . 0 
 
     -   v c = 1 4 . 3 
 
     -   v c 1 4 _ r u n t i m e = 1 4 . 4 4 . 3 5 2 0 8 
 
     -   v s 2 0 1 5 _ r u n t i m e = 1 4 . 4 4 . 3 5 2 0 8 
 
     -   w h e e l = 0 . 4 5 . 1 
 
     -   x z = 5 . 6 . 4 
 
     -   z l i b = 1 . 3 . 1 
 
     -   p i p : 
 
             -   a b s l - p y = = 2 . 3 . 1 
 
             -   a n y i o = = 4 . 1 1 . 0 
 
             -   c e r t i f i = = 2 0 2 5 . 1 0 . 5 
 
             -   c l i c k = = 8 . 3 . 0 
 
             -   c o l o r a m a = = 0 . 4 . 6 
 
             -   c o n t o u r p y = = 1 . 3 . 2 
 
             -   c y c l e r = = 0 . 1 2 . 1 
 
             -   e x c e p t i o n g r o u p = = 1 . 3 . 0 
 
             -   f i l e l o c k = = 3 . 1 9 . 1 
 
             -   f o n t t o o l s = = 4 . 6 0 . 1 
 
             -   f s s p e c = = 2 0 2 5 . 9 . 0 
 
             -   g r p c i o = = 1 . 7 6 . 0 
 
             -   h 1 1 = = 0 . 1 6 . 0 
 
             -   h f - x e t = = 1 . 2 . 0 
 
             -   h t t p c o r e = = 1 . 0 . 9 
 
             -   h t t p x = = 0 . 2 8 . 1 
 
             -   h u g g i n g f a c e - h u b = = 1 . 1 . 2 
 
             -   i d n a = = 3 . 1 1 
 
             -   i m a g e i o = = 2 . 3 7 . 2 
 
             -   j i n j a 2 = = 3 . 1 . 6 
 
             -   k i w i s o l v e r = = 1 . 4 . 9 
 
             -   l a z y - l o a d e r = = 0 . 4 
 
             -   m a r k d o w n = = 3 . 1 0 
 
             -   m a r k u p s a f e = = 2 . 1 . 5 
 
             -   m a t p l o t l i b = = 3 . 1 0 . 7 
 
             -   m l - c o l l e c t i o n s = = 1 . 1 . 0 
 
             -   m p m a t h = = 1 . 3 . 0 
 
             -   n a t s o r t = = 8 . 4 . 0 
 
             -   n e t w o r k x = = 3 . 3 
 
             -   n u m p y = = 2 . 1 . 2 
 
             -   p a c k a g i n g = = 2 5 . 0 
 
             -   p i l l o w = = 1 1 . 3 . 0 
 
             -   p r o t o b u f = = 6 . 3 3 . 0 
 
             -   p y p a r s i n g = = 3 . 2 . 5 
 
             -   p y s t r u m = = 0 . 4 
 
             -   p y t h o n - d a t e u t i l = = 2 . 9 . 0 . p o s t 0 
 
             -   p y y a m l = = 6 . 0 . 3 
 
             -   s a f e t e n s o r s = = 0 . 6 . 2 
 
             -   s c i k i t - i m a g e = = 0 . 2 5 . 2 
 
             -   s c i p y = = 1 . 1 5 . 3 
 
             -   s h e l l i n g h a m = = 1 . 5 . 4 
 
             -   s i x = = 1 . 1 7 . 0 
 
             -   s n i f f i o = = 1 . 3 . 1 
 
             -   s y m p y = = 1 . 1 4 . 0 
 
             -   t e n s o r b o a r d = = 2 . 2 0 . 0 
 
             -   t e n s o r b o a r d - d a t a - s e r v e r = = 0 . 7 . 2 
 
             -   t i f f f i l e = = 2 0 2 5 . 5 . 1 0 
 
             -   t i m m = = 1 . 0 . 2 2 
 
             -   t o r c h = = 2 . 9 . 0 + c u 1 2 8 
 
             -   t o r c h a u d i o = = 2 . 9 . 0 + c u 1 2 8 
 
             -   t o r c h v i s i o n = = 0 . 2 4 . 0 + c u 1 2 8 
 
             -   t q d m = = 4 . 6 7 . 1 
 
             -   t y p e r - s l i m = = 0 . 2 0 . 0 
 
             -   t y p i n g - e x t e n s i o n s = = 4 . 1 5 . 0 
 
             -   w e r k z e u g = = 3 . 1 . 3 
 
 
==================================================

üìÑ environment_clear.yml
--------------------------------------------------
—è—én a m e :   o a s i s - c t c f 
 
 c h a n n e l s : 
 
     -   c o n d a - f o r g e 
 
     -   d e f a u l t s 
 
 d e p e n d e n c i e s : 
 
     -   p y t h o n = 3 . 1 0 
 
     -   p i p > = 2 4 
 
     -   s e t u p t o o l s 
 
     -   w h e e l 
 
     -   n u m p y 
 
     -   s c i p y 
 
     -   s c i k i t - i m a g e 
 
     -   m a t p l o t l i b 
 
     -   i m a g e i o 
 
     -   t i f f f i l e 
 
     -   m l - c o l l e c t i o n s 
 
     -   p i p : 
 
             -   n a t s o r t 
 
             -   t e n s o r b o a r d 
 
             -   t q d m 
 
             -   e i n o p s 
 
             -   t i m m 
 
             -   p y s t r u m 
 
             -   s a f e t e n s o r s 
 
             -   h u g g i n g f a c e - h u b 
 
             -   m a r k d o w n 
 
             -   w e r k z e u g 
==================================================

üìÑ guide.md
--------------------------------------------------
# OASIS-CTCF
–§–∞–π–ª—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã:
- `TransMorph/train_CTCF_v2.py`
- `TransMorph/train_TransMorph_v2.py`

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞
```bash
conda env create -f environment.yml -n oasis-ctcf
conda activate oasis-ctcf
```

–õ–ò–ë–û –ª—É—á—à–µ:

```bash
conda env create -f environment_clear.yml -n oasis-ctcf
conda activate oasis-ctcf
pip install --index-url https://download.pytorch.org/whl/cu128 torch torchvision torchaudio
```

## –ó–∞–ø—É—Å–∫

1. –ù–∞ Windows –Ω–∞ –º–æ–µ–π –º–∞—à–∏–Ω–µ:
–û–±—â–µ–µ: `python -m experiments.OASIS.`*–∏–º—è —Ñ–∞–π–ª–∞*` --train_dir "C:/Users/user/Documents/Education/MasterWork/datasets/OASIS_L2R_2021_task03/All" --val_dir "C:/Users/user/Documents/Education/MasterWork/datasets/OASIS_L2R_2021_task03/Test"`
- train_CTCF
- train_TM-DCA
- train_UTSRMorph


2. –î–ª—è Linux –Ω–∞ –≤–∞—à–µ–π –º–∞—à–∏–Ω–µ:
–û–±—â–µ–µ: `python -m experiments.OASIS.`*–∏–º—è —Ñ–∞–π–ª–∞*` --train_dir "/home/roman/P/OASIS_L2R_2021_task03/All" --val_dir "/home/roman/P/OASIS_L2R_2021_task03/Test"`
- train_CTCF
- train_TM-DCA
- train_UTSRMorph
==================================================

üìÑ Pasenko_CN_2_–°—Ç–∞—Ç—å—è.doc
--------------------------------------------------
[–ë–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª]
==================================================

üìÑ Pasenko_CN_2_–°—Ç–∞—Ç—å—è.pdf
--------------------------------------------------
[–ë–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª]
==================================================

üìÑ README.md
--------------------------------------------------
‚Ä£Èø∞ÍÇß‰º†ÂçÅÂçâ‰å≠‰çî‡µÜ‡¥ä‚®ä‰º™ÂçÅÂçâ‰å≠‰çî‚©Ü‚Ä™ËÉ¢‚ÇîÎ£êËáëËáëÎØêÎóêÎìêÎªêÎãêÎÉêËãëÎóêÎØêË≥ëËáëÎ´êÎ£êÎßêÌÄ†ÌÜøÌÇÄÌÇæÌÇµÌÜ∫‚ÇÇÎãêÌÄ†ÌÇæÌÇ±ÌÇªÌÜ∞ÌÜÅÌÇÇ‚Ç∏Î≥êÎóêÎìêÎ£êËõëÎ£êÎ∑êËáëÎ´êÎªêÎßêÌÑ†ÌÇÄÌÇµÌÇ≥ÌÜ∏ÌÜÅÌÜÇÌÇÄÌÜ∞ÌÇÜÌÇ∏‚Ç∏Î£êÎüêÎªêÎáêËÉëÎÉêÎõêÎóêÎ∑êÎ£êÎßê‚Ä¨‡¥†ÌÑäÌÇÄÌÇµÌÇ∞ÌÇªÌÇ∏ÌÜ∑ÌÜÉÌÜéÌÇâÌÇ∏‚ÇπÎÉêËÉëËóëÎ£êËãëÎóêÎ´êËãëËèëËÉëËèë‚®†‰å™ÊçπÊï¨Âê≠ÊÖ≤ÁçÆÊΩçÁÅ≤‚µ®‰ôÉ‚††ÂëÉ‰ôÉ‚®©‚Ä™ËÉ¢‚ÇîÎ´êÎÉêËáëÎ´êÎÉêÎìêÎ∑êËèëËªëÌÑ†ÌÜÇÌÇÄÌÇ∞ÌÜΩÌÜÅÌÇÑÌÜæÌÇÄÌÇºÌÜµ‚∂ÄÎ≥êÎªêÎìêÎóêÎØêË≥ë‚Ä†‡®çÎìêÎØêËøë‚®†ÊπµÁï≥Êï∞Áô≤Áç©Êë•Êê†Êô•ÁâØÊÖ≠Ê±¢‚Å•Êï≠Ê•§ÊÖ£‚Å¨Êµ©Êù°‚Å•Êï≤Ê•ßÁë≥ÊÖ≤Ê•¥ÊπØ‚∏™‡®ç‡®ç‚¥≠‡¥≠‡¥ä‚åä‚Ä£Èø∞Í≤îÌÄ†ÌÇöÌÇæÌÜΩÌÇÜÌÇµÌÜøÌÇÜÌÜ∏‡∂è‡¥äÌÄäÌÜüÌÇÄÌÇæÌÇµÌÜ∫‚ÇÇÎªêËáëÎ∑êÎªêÎãêÎÉêÎ∑êÌÄ†ÌÇΩ‚Ç∞‚®™ÁâîÊπ°‰µ≥ÁâØÊ°∞‚®™ÌÄ†‚Ç∏ÎªêÎáêË´ëÎóêÎìêÎ£êÎ∑êËøëÎóêËãëÌÄ†ÌÇ∏ÌÇ¥ÌÇµ„™∏‡®ç‡®ç‚Ä≠‚®™Á•ÉÊ±£‰µ•ÁâØÊ°∞‚®™Óà†ÈíÄ‚®†Á•£Ê±£‚µ•ÊΩ£ÁçÆÁç©Êï¥ÊçÆ‚ÅπÊΩ¨Áç≥‚Ä™‡¥†‚¥ä‚®†‰§™‰ΩÉ‚Åé‚ÄØÁâáÊë°‰çâ‰πè‚®™Óà†ÈíÄ‚®†Êπ©Êï∂Áç≤‚µ•ÊΩ£ÁçÆÁç©Êï¥ÊçÆ‚ÅπÊ±¶ÁùØÁà†Êù•Ê±µÁâ°Á©©Áë°ÊΩ©‚©Æ‚Ä†‡®ç‚Ä≠‚®™ÊÖÉÊç≥Êë°‚Å•Êï≤Ê•ßÁë≥ÊÖ≤Ê•¥ÊπØ‚®™Óà†ÈíÄÌÄ†ÌÇºÌÇΩÌÇæÌÇ≥ÌÜæÌÜÉÌÇÄÌÇæÌÇ≤ÌÇΩÌÇµÌÇ≤ÌÜ∞‚ÇèÊå™ÊÖØÁç≤‚µ•ÊΩ¥Êò≠Êπ©‚©•ÌÄ†ÌÜ∞ÌÜÄÌÇÖÌÜ∏ÌÇÇÌÇµÌÜ∫ÌÜÇÌÜÉÌÇÄ‚Ç∞‡¥†‚¥ä‚®†‰å™ÊΩ≤Áç≥‚º†ÂÄ†ÊÖ¨ÊïÆÊÑ†Áë¥Êπ•Ê•¥ÊπØ‚®™Óà†ÈíÄÌÑ†ÌÇÉÌÜªÌÜÉÌÜáÌÇàÌÇµÌÇΩÌÇΩÌÇæ‚ÇµËáëÎªêÎøêÎªêËáëËãëÎÉêÎãêÎØêÎóêÎ∑êÎ£êÎóêÌÄ†ÌÜøÌÇÄÌÇ∏ÌÇ∑ÌÇΩÌÇ∞ÌÇ∫ÌÇæ‚Ç≤‡¥†‚¥äÌÄ†ÌÇëÌÇ∞ÌÇªÌÇ∞ÌÜΩÌÇÅÌÜ∏ÌÇÄÌÇæÌÇ≤ÌÜ∫‚ÇÉÎØêÎªêËáëËáëÎªêÎãê‚®†‰∞®ÁçüÊµ©‚Ä¨ÂΩåÊï≤‚±ß‰∞†Ê©üÊç°‚Ä¨ÂΩåÊç©ÊπØ‚Ä¨ÂΩåÁ•£‚•£‚Ä™‡¥†‚ÄäÌÄ†ÌÇ¥ÌÜª‚ÇèÎªêÎøêËãëÎ£êÎ≥êÎÉêÎØêË≥ëÎ∑êÎªêÎèêÎªêÌÑ†ÌÇÅÌÇæÌÜæÌÇÇÌÇΩÌÜæÌÇàÌÇµÌÇΩÌÜ∏‚ÇèÎ´êÎÉêËüëÎóêËáëËãëÎãêÎÉê‚Ä¨ÎèêÎØêÎÉêÎìêÎ´êÎªêËáëËãëÎ£êÌÄ†‚Ç∏ËãëÎªêÎøêÎªêÎØêÎªêÎèêÎ£êÎ£ê‡®ç‡®ç‚¥≠‡¥≠‡¥ä‚åä‚Ä£Èø∞ËäìÌÄ†ÌÜ°ÌÜÇÌÜÄÌÇÉÌÜ∫ÌÜÇÌÜÉÌÇÄ‚Ç∞ËÉëÎóêÎøêÎªêÎüêÎ£êËãëÎªêËÉëÎ£êËøë‡®ç‡®ç‚ÅºÍìêÎÉêÎßêÎØê‚º†ÌÄ†ÌÇøÌÇ∞ÌÇøÌÇ∫‚Ç∞‚ÅºÈ∑êÎÉêÎüêÎ∑êÎÉêËüëÎóêÎ∑êÎ£êÎóêÁ∞†‡®ç‚µº‚¥≠‚¥≠‚¥≠‚¥≠‚¥≠‚¥≠‚¥≠‚µº‚¥≠‚¥≠‚¥≠‚¥≠‚¥≠Á∞≠‡®ç‚ÅºÂë†ÊÖ≤ÁçÆÊΩçÁÅ≤‚Ω®Áâ¥Ê•°ÂΩÆÂëÉ‰ôÉÁÄÆÊÅπÁ∞†ÌÄ†ÌÇæÌÜ±ÌÜÉÌÇáÌÇµÌÇΩÌÇ∏‚ÇµÎ´êÎÉêËáëÎ´êÎÉêÎìêÎ∑êÎªêÎßêÌÄ†ÌÇºÌÇæÌÇ¥ÌÇµÌÇª‚Ç∏‚®™ÂëÉ‰ôÉ‚®™Á∞†‡®ç‚ÅºÂë†ÊÖ≤ÁçÆÊΩçÁÅ≤‚Ω®Áâ¥Ê•°ÂΩÆ‰µîÊâüÁç°Ê±•Êπ©‚π•Á•∞‚Å†‚ÅºÎªêÎáêËèëËüëÎóêÎ∑êÎ£êÎóêÌÑ†ÌÇáÌÜ∏ÌÜÅÌÇÇÌÇæÌÇ≥‚Çæ‚®™ÁâîÊπ°‰µ≥ÁâØÊ°∞Êà≠Áç°Ê±•Êπ©‚©•‚Ä™ÎìêÎØêËøëÌÑ†ÌÜÅÌÇÄÌÇ∞ÌÇ≤ÌÇΩÌÇµÌÇΩÌÜ∏‚Çè‡µºÁ∞äÊÄ†ÁâîÊπ°‰µ≥ÁâØÊ°∞Ê¥ØÊëØÊ±•‚Ω≥‚Å†‚ÅºÎÉêËÉëËóëÎ£êËãëÎóêÎ´êËãëËèëËÉëËØë‚Ä¨ÎØêÎªêËáëËáëËØë‚Ä¨Î´êÎÉêËáëÎ´êÎÉêÎìê‚Ä¨Áë°Êï¥ÁëÆÊΩ©‚µÆÎ≥êÎªêÎìêËèëÎØêÎ£ê‚Ä¨ËèëËãëÎ£êÎØêÎ£êËãëËØëÁ∞†‡®ç‚ÅºÂë†ÊÖ≤ÁçÆÊΩçÁÅ≤‚Ω®ÊΩ≠Êï§Áç¨ÊåØÊπØÊ•¶Áçß‰çü‰çî‚πÜÁ•∞‚Å†‚ÅºÂÅá‚µïÎøêËÉëÎªêËìëÎ£êÎØêÎ£ê‚®†Âå®ÊÖ≠Ê±¨‚º†‰¥†Êë©‚º†‰∞†Áâ°Êïß‚®©Á∞†‡®ç‚ÅºÂë†ÊÖ≤ÁçÆÊΩçÁÅ≤‚Ω®ÊΩ≠Êï§Áç¨ÁîØÊ•¥Áç¨ÁëüÁâØÊ°£ÁÄÆÊÅπ‚Ä¨Áï†Ê•¥Áç¨ÁëüÊÖ≤Êπ©ÁÄÆÊÅπÁ∞†‰Ñ†ÂÅç‚Ä¨‰ôî„à≥‚Ä¨ÎØêÎªêÎèêÎèêÎóêËÉëËØë‚Ä¨ÎãêÎÉêÎØêÎ£êÎìêÎÉêËõëÎ£êËøëÁ∞†‡®ç‚ÅºÊï†ÊÖ∂Áï¨Áë°ÊΩ©‚πÆÁ•∞‚Å†‚ÅºÎªêËìëËìëÎØêÎÉêÎßêÎ∑êÌÄ≠ÌÜæÌÇÜÌÇµÌÇΩÌÇ∫‚Ç∞Î£êÌÑ†ÌÇçÌÜ∫ÌÇÅÌÇøÌÜæÌÜÄ‚ÇÇÎøêÎªêÎØêÎóêÎßêÌÄ†ÌÇ¥ÌÜµÌÇÑÌÜæÌÇÄÌÇºÌÜ∞ÌÇÜÌÇ∏‚Çπ‡µº‡¥ä‚¥ä‚¥≠‡®ç‡®ç‚å£Óà†È¶öÎ£Ø‚ÇèÈªêËáëÎªêÎáêÎóêÎ∑êÎ∑êÎªêËáëËãëÎ£ê‡®ç‡®ç‚Ä≠ÈøêÎªêÎØêÎ∑êÎÉêËøëÌÑ†ÌÇÅÌÇæÌÇ≤ÌÇºÌÜµÌÜÅÌÇÇÌÇ∏ÌÇºÌÜæÌÜÅÌÜÇ‚ÇåËáë‚®†ÂÄ™ÂëπÁâØÊ°£„à†„§Æ‚¨†‰å†‰ëï‚ÅÅ„à±„†Æ‚®™‚Ä†‡®ç‚Ä≠ÈøêÎªêÎìêÎìêÎóêËÉëÎõêÎ´êÎÉê‚®†Âú™Êπ©ÊΩ§Áç∑‚º†‰∞†Êπ©Á°µ‚®™‚Ä†‡®ç‚Ä≠‚®™‰µÅ‚µêËèëËáëÎ´êÎªêËÉëÎóêÎ∑êÎ£êÎóê‚®™ÌÄ†‚Ç∏‚®™‰ôî„à≥ÌÑ≠ÌÇÄÌÇµÌÇ∂ÌÇ∏‚™º‚Ä™ÎìêÎØêËøëÌÑ†ÌÇÅÌÇæÌÜ≤ÌÇÄÌÇµÌÇºÌÇµÌÇΩÌÜΩÌÜã‚ÇÖÂÅá‚Åï‡¥†‚¥äÌÄ†ÌÇêÌÜ≤ÌÇÇÌÇæÌÇºÌÜ∞ÌÇÇÌÜ∏ÌÇáÌÜµÌÇÅÌÇ∫ÌÇæ‚ÇµËáëÎªêËóëËÉëÎÉêÎ∑êÎóêÎ∑êÎ£êÎóêÌÄ†ÌÇªÌÇæÌÇ≥ÌÇæ‚Ç≤Î£êÌÑ†ÌÇáÌÇµÌÇ∫ÌÇøÌÇæÌÇπÌÜΩÌÇÇÌÇæ‚Ç≤ÊÄ®ÊΩ¨ÁçßÊÄØ‚Ä¨Êï†ÁÅ∏Áâ•Êµ©Êπ•Áç¥ÊÄØ‚Ä©‡¥†‚¥äÌÄ†ÌÇìÌÜæÌÇÇÌÇæ‚Ç≤Î´êÌÄ†ÌÇ≤ÌÜæÌÇÅÌÜøÌÇÄÌÇæÌÇ∏ÌÇ∑ÌÇ≤ÌÇæÌÇ¥ÌÇ∏ÌÜºÌÇã‚ÇºË∑ëÎ´êËáëÎøêÎóêËÉëÎ£êÎ≥êÎóêÎ∑êËãëÎÉêÎ≥êÌÄ†‚Ç∏ÎøêËèëÎáêÎØêÎ£êÎ´êÎÉêËõëÎ£êÎ£êÌÑ†ÌÇÄÌÇµÌÜ∑ÌÇÉÌÜªÌÜåÌÇÇÌÜ∞ÌÇÇÌÇæ‚Ç≤‡¥†‚Ää‚®†‰î®‰ç¨ÊπØ‰å†ÊπØÊï¶Êï≤ÊçÆ‚Å•„Ä≤„ò≤‚Ä¨Ê•ÇÊµØÊë•Êç©Ê±°‰î†ÊùÆÊπ©Êï•Ê•≤ÊùÆÁê†ÊÖ≤Ê≠£‚®©
==================================================
