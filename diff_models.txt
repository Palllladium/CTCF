PROJECT ROOT: C:/Users/user/Documents/Education/MasterWork/repos/CTCF
ALLOW-LIST:
  DIR : experiments
  DIR : models
  DIR : utils

================================================================================
TREE (allowed only)
================================================================================
├── experiments
│   ├── IXI
│   │   ├── __init__.py
│   │   ├── datasets.py
│   │   ├── train_CTCF.py
│   │   ├── train_TransMorphDCA.py
│   │   └── train_UTSRMorph.py
│   ├── OASIS
│   │   ├── __init__.py
│   │   ├── datasets.py
│   │   ├── inference.py
│   │   ├── train_CTCF.py
│   │   ├── train_TransMorphDCA.py
│   │   └── train_UTSRMorph.py
│   ├── __init__.py
│   └── engine.py
├── models
│   ├── CTCF
│   │   ├── __init__.py
│   │   ├── cascade_nets.py
│   │   ├── configs.py
│   │   ├── model.py
│   │   └── ut_blocks.py
│   ├── TransMorph_DCA
│   │   ├── __init__.py
│   │   ├── configs.py
│   │   └── model.py
│   ├── UTSRMorph
│   │   ├── __init__.py
│   │   ├── configs.py
│   │   └── model.py
│   └── __init__.py
└── utils
    ├── __init__.py
    ├── core.py
    ├── ctcf_losses.py
    ├── data.py
    ├── dice.py
    ├── field.py
    ├── losses.py
    ├── misc.py
    ├── rand.py
    ├── spatial.py
    ├── surface_distance
    │   ├── __init__.py
    │   ├── lookup_tables.py
    │   └── metrics.py
    ├── train.py
    ├── trans.py
    └── validation.py

================================================================================
CONTENTS
================================================================================


################################################################################
# FILE: experiments/IXI/__init__.py
# SIZE: 0 bytes
################################################################################


################################################################################
# FILE: experiments/IXI/datasets.py
# SIZE: 2820 bytes
################################################################################

import os, glob
import torch, sys
from torch.utils.data import Dataset
from utils import pkload
import matplotlib.pyplot as plt

import numpy as np


class IXIBrainDataset(Dataset):
    def __init__(self, data_path, atlas_path, transforms):
        self.paths = data_path
        self.atlas_path = atlas_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        x, x_seg = pkload(self.atlas_path)
        y, y_seg = pkload(path)
        #print(x.shape)
        #print(x.shape)
        #print(np.unique(y))
        # print(x.shape, y.shape)#(240, 240, 155) (240, 240, 155)
        # transforms work with nhwtc
        x, y = x[None, ...], y[None, ...]
        # print(x.shape, y.shape)#(1, 240, 240, 155) (1, 240, 240, 155)
        x,y = self.transforms([x, y])
        #y = self.one_hot(y, 2)
        #print(y.shape)
        #sys.exit(0)
        x = np.ascontiguousarray(x)# [Bsize,channelsHeight,,Width,Depth]
        y = np.ascontiguousarray(y)
        #plt.figure()
        #plt.subplot(1, 2, 1)
        #plt.imshow(x[0, :, :, 8], cmap='gray')
        #plt.subplot(1, 2, 2)
        #plt.imshow(y[0, :, :, 8], cmap='gray')
        #plt.show()
        #sys.exit(0)
        #y = np.squeeze(y, axis=0)
        x, y = torch.from_numpy(x), torch.from_numpy(y)
        return x, y

    def __len__(self):
        return len(self.paths)


class IXIBrainInferDataset(Dataset):
    def __init__(self, data_path, atlas_path, transforms):
        self.atlas_path = atlas_path
        self.paths = data_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        x, x_seg = pkload(self.atlas_path)
        y, y_seg = pkload(path)
        x, y = x[None, ...], y[None, ...]
        x_seg, y_seg= x_seg[None, ...], y_seg[None, ...]
        x, x_seg = self.transforms([x, x_seg])
        y, y_seg = self.transforms([y, y_seg])
        x = np.ascontiguousarray(x)# [Bsize,channelsHeight,,Width,Depth]
        y = np.ascontiguousarray(y)
        x_seg = np.ascontiguousarray(x_seg)  # [Bsize,channelsHeight,,Width,Depth]
        y_seg = np.ascontiguousarray(y_seg)
        x, y, x_seg, y_seg = torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(x_seg), torch.from_numpy(y_seg)
        return x, y, x_seg, y_seg

    def __len__(self):
        return len(self.paths)
################################################################################
# FILE: experiments/IXI/train_CTCF.py
# SIZE: 4191 bytes
################################################################################

import os, glob, argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim

from experiments.IXI import datasets
from utils import trans as trans_utils
from experiments.engine import add_engine_args, apply_paths, run_train

from models.CTCF.model import CTCF_CascadeA, CONFIGS as CONFIGS_CTCF
from utils import (
    NCC_vxm, 
    Grad3d, 
    register_model, 
    icon_loss, 
    neg_jacobian_penalty, 
    ctcf_schedule, 
    adjust_lr_ctcf_schedule
)


CTX = {}


@torch.no_grad()
def forward_flow_fn(model, x, y, device, args):
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        _, flow = model(x, y)
    return flow


def train_step(model, batch, device, args, epoch):
    x, y = batch
    x, y = x.to(device).float(), y.to(device).float()

    alpha_l1, alpha_l3, warm = ctcf_schedule(epoch, args.max_epoch)
    W_icon, W_cyc, W_jac = args.w_icon * warm, args.w_cyc * warm, args.w_jac * warm

    def_xy, flow_xy = model(x, y, return_all=False, alpha_l1=alpha_l1, alpha_l3=alpha_l3)
    def_yx, flow_yx = model(y, x, return_all=False, alpha_l1=alpha_l1, alpha_l3=alpha_l3)

    with torch.autocast(device_type="cuda", enabled=False):
        L_ncc = 0.5 * (CTX["ncc"](def_xy.float(), y.float()) + CTX["ncc"](def_yx.float(), x.float())) * args.w_ncc

    L_icon = icon_loss(flow_xy, flow_yx) * W_icon
    L_reg = 0.5 * (CTX["reg"](flow_xy) + CTX["reg"](flow_yx)) * args.w_reg
    L_jac = 0.5 * (neg_jacobian_penalty(flow_xy) + neg_jacobian_penalty(flow_yx)) * W_jac
    L_cyc = ((CTX["reg_model"]((def_xy, flow_yx)) - x).abs().mean() + (CTX["reg_model"]((def_yx, flow_xy)) - y).abs().mean()) * W_cyc

    loss = L_ncc + L_icon + L_reg + L_jac + L_cyc
    logs = {"all": loss.item(), "ncc": L_ncc.item(), "reg": L_reg.item(), "icon": L_icon.item(), "cyc": L_cyc.item(), "jac": L_jac.item()}
    return loss, logs


def build_model(device, args):
    cfg = CONFIGS_CTCF["CTCF-CascadeA"]
    cfg.time_steps = int(args.time_steps)
    model = CTCF_CascadeA(cfg).to(device)
    opt = optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)

    CTX["config"] = cfg
    CTX["ncc"] = NCC_vxm(win=(9, 9, 9))
    CTX["reg"] = Grad3d(penalty="l2")
    CTX["reg_model"] = register_model(cfg.img_size, mode="bilinear").to(device)
    return model, opt


def build_loaders(args):
    train_tfm = transforms.Compose([trans_utils.RandomFlip(0), trans_utils.NumpyType((np.float32, np.float32))])
    val_tfm = transforms.Compose([trans_utils.Seg_norm(), trans_utils.NumpyType((np.float32, np.int16))])
    tr = datasets.IXIBrainDataset(glob.glob(os.path.join(args.train_dir, "*.pkl")), args.atlas_path, transforms=train_tfm)
    va = datasets.IXIBrainInferDataset(glob.glob(os.path.join(args.val_dir, "*.pkl")), args.atlas_path, transforms=val_tfm)
    train_loader = DataLoader(tr, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)
    val_loader = DataLoader(va, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)
    return train_loader, val_loader


def lr_step(optimizer, epoch, args):
    return adjust_lr_ctcf_schedule(optimizer, epoch, args.max_epoch, args.lr)


def parse_args():
    p = argparse.ArgumentParser()
    add_engine_args(p, dataset="IXI")
    p.set_defaults(exp="CTCF_IXI")
    p.add_argument("--w_ncc", type=float, default=1.0)
    p.add_argument("--w_reg", type=float, default=1.0)
    p.add_argument("--w_icon", type=float, default=0.05)
    p.add_argument("--w_cyc", type=float, default=0.02)
    p.add_argument("--w_jac", type=float, default=0.005)
    p.add_argument("--time_steps", type=int, default=12)
    return p.parse_args()


def main():
    args = parse_args()
    apply_paths(args, dataset="IXI")
    run_train(dataset="IXI", args=args, build_model=build_model, build_loaders=build_loaders,
              train_step=train_step, forward_flow_fn=forward_flow_fn, lr_step=lr_step)


if __name__ == "__main__":
    main()
################################################################################
# FILE: experiments/IXI/train_TransMorphDCA.py
# SIZE: 3805 bytes
################################################################################

import os, glob, argparse
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim

from experiments.IXI import datasets
from utils import trans as trans_utils
from experiments.engine import add_engine_args, apply_paths, run_train

from models.TransMorph_DCA.model import TransMorphCascadeAd, CONFIGS as CONFIGS_TM

from utils import (
    NCC_vxm, 
    Grad3d, 
    register_model, 
    adjust_learning_rate_poly
)


CTX = {}


@torch.no_grad()
def forward_flow_fn(model, x, y, device, args):
    xh, yh = F.avg_pool3d(x, 2), F.avg_pool3d(y, 2)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        flow_h = model((xh, yh))
    return F.interpolate(flow_h.float(), scale_factor=2, mode="trilinear", align_corners=False) * 2.0

def train_step(model, batch, device, args, epoch):
    x, y = batch
    x, y = x.to(device).float(), y.to(device).float()

    xh, yh = F.avg_pool3d(x, 2), F.avg_pool3d(y, 2)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        flow_h = model((xh, yh))
    flow = F.interpolate(flow_h.float(), scale_factor=2, mode="trilinear", align_corners=False) * 2.0

    def_x = CTX["reg_model"]((x, flow))

    with torch.autocast(device_type="cuda", enabled=False):
        L_ncc = CTX["ncc"](def_x.float(), y.float()) * args.w_ncc
    L_reg = CTX["reg"](flow) * args.w_reg

    loss = L_ncc + L_reg
    return loss, {"all": loss.item(), "ncc": L_ncc.item(), "reg": L_reg.item()}


def build_model(device, args):
    D,H,W = args.vol_size
    cfg = CONFIGS_TM["TransMorph-3-LVL"]; 
    cfg.img_size = (D//2, H//2, W//2)

    model = TransMorphCascadeAd(cfg, int(args.time_steps)).to(device)
    opt = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0, amsgrad=True)

    CTX["ncc"] = NCC_vxm()
    CTX["reg"] = Grad3d(penalty="l2")
    CTX["reg_model"] = register_model((D, H, W), mode="bilinear").to(device)
    return model, opt

def build_loaders(args):
    train_tfm = transforms.Compose([trans_utils.RandomFlip(0), trans_utils.NumpyType((np.float32, np.float32))])
    val_tfm = transforms.Compose([trans_utils.Seg_norm(), trans_utils.NumpyType((np.float32, np.int16))])
    tr = datasets.IXIBrainDataset(glob.glob(os.path.join(args.train_dir, "*.pkl")), args.atlas_path, transforms=train_tfm)
    va = datasets.IXIBrainInferDataset(glob.glob(os.path.join(args.val_dir, "*.pkl")), args.atlas_path, transforms=val_tfm)
    train_loader = DataLoader(tr, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)
    val_loader = DataLoader(va, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)
    return train_loader, val_loader


def lr_step(optimizer, epoch, args):
    return adjust_learning_rate_poly(optimizer, epoch, args.max_epoch, args.lr)


def parse_args():
    p = argparse.ArgumentParser()
    add_engine_args(p, dataset="IXI")
    p.set_defaults(exp="TM_DCA_IXI")
    p.add_argument("--w_ncc", type=float, default=1.0)
    p.add_argument("--w_reg", type=float, default=1.0)
    p.add_argument("--time_steps", type=int, default=12)
    p.add_argument("--vol_size", type=int, nargs=3, default=[160, 192, 224])
    return p.parse_args()


def main():
    args = parse_args()
    apply_paths(args, dataset="IXI")
    run_train(dataset="IXI", args=args, build_model=build_model, build_loaders=build_loaders,
              train_step=train_step, forward_flow_fn=forward_flow_fn, lr_step=lr_step)

if __name__ == "__main__":
    main()
################################################################################
# FILE: experiments/IXI/train_UTSRMorph.py
# SIZE: 3136 bytes
################################################################################

import os, glob, argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim

from experiments.IXI import datasets
from utils import trans as trans_utils
from experiments.engine import add_engine_args, apply_paths, run_train

from models.UTSRMorph.model import UTSRMorph, CONFIGS as CONFIGS_UM
from utils import (
    NCC_vxm, 
    Grad3d, 
    adjust_learning_rate_poly
)


CTX = {}


@torch.no_grad()
def forward_flow_fn(model, x, y, device, args):
    inp = torch.cat((x, y), dim=1)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        _, flow = model(inp)
    return flow


def train_step(model, batch, device, args, epoch):
    x, y = batch
    x, y = x.to(device).float(), y.to(device).float()

    inp = torch.cat((x, y), dim=1)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        def_x, flow = model(inp)

    with torch.autocast(device_type="cuda", enabled=False):
        L_ncc = CTX["ncc"](def_x.float(), y.float()) * args.w_ncc
    L_reg = CTX["reg"](flow) * args.w_reg

    loss = L_ncc + L_reg
    return loss, {"all": loss.item(), "ncc": L_ncc.item(), "reg": L_reg.item()}


def build_model(device, args):
    cfg = CONFIGS_UM["UTSRMorph-Large"]
    model = UTSRMorph(cfg).to(device)
    opt = optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)
    CTX["ncc"] = NCC_vxm()
    CTX["reg"] = Grad3d(penalty="l2")
    return model, opt


def build_loaders(args):
    train_tfm = transforms.Compose([trans_utils.RandomFlip(0), trans_utils.NumpyType((np.float32, np.float32))])
    val_tfm = transforms.Compose([trans_utils.Seg_norm(), trans_utils.NumpyType((np.float32, np.int16))])
    tr = datasets.IXIBrainDataset(glob.glob(os.path.join(args.train_dir, "*.pkl")), args.atlas_path, transforms=train_tfm)
    va = datasets.IXIBrainInferDataset(glob.glob(os.path.join(args.val_dir, "*.pkl")), args.atlas_path, transforms=val_tfm)
    train_loader = DataLoader(tr, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)
    val_loader = DataLoader(va, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)
    return train_loader, val_loader


def lr_step(optimizer, epoch, args):
    return adjust_learning_rate_poly(optimizer, epoch, args.max_epoch, args.lr)


def parse_args():
    p = argparse.ArgumentParser()
    add_engine_args(p, dataset="IXI")
    p.set_defaults(exp="UTSRMorph_IXI")
    p.add_argument("--w_ncc", type=float, default=1.0)
    p.add_argument("--w_reg", type=float, default=1.0)
    return p.parse_args()


def main():
    args = parse_args()
    apply_paths(args, dataset="IXI")
    run_train(dataset="IXI", args=args, build_model=build_model, build_loaders=build_loaders,
              train_step=train_step, forward_flow_fn=forward_flow_fn, lr_step=lr_step)


if __name__ == "__main__":
    main()
################################################################################
# FILE: experiments/OASIS/__init__.py
# SIZE: 0 bytes
################################################################################


################################################################################
# FILE: experiments/OASIS/datasets.py
# SIZE: 2908 bytes
################################################################################

import torch
from torch.utils.data import Dataset
import random
import numpy as np

from utils import pkload


class OASISBrainDataset(Dataset):
    def __init__(self, data_path, transforms):
        self.paths = data_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        tar_list = self.paths.copy()
        tar_list.remove(path)
        random.shuffle(tar_list)
        tar_file = tar_list[0]
        data = pkload(path)
        x, x_seg = data if len(data) == 2 else (data[0], data[2])
        data = pkload(tar_file)
        y, y_seg = data if len(data) == 2 else (data[0], data[2])
        x, y = x[None, ...], y[None, ...]
        x_seg, y_seg = x_seg[None, ...], y_seg[None, ...]
        x, x_seg = self.transforms([x, x_seg])
        y, y_seg = self.transforms([y, y_seg])
        x = np.ascontiguousarray(x)
        y = np.ascontiguousarray(y)
        x_seg = np.ascontiguousarray(x_seg)
        y_seg = np.ascontiguousarray(y_seg)
        x, y, x_seg, y_seg = torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(x_seg), torch.from_numpy(y_seg)

        # images must be float
        x = x.float()
        y = y.float()

        # labels must be Long for one_hot and for any index-based ops
        x_seg = x_seg.long()
        y_seg = y_seg.long()

        return x, y, x_seg, y_seg

    def __len__(self):
        return len(self.paths)


class OASISBrainInferDataset(Dataset):
    def __init__(self, data_path, transforms):
        self.paths = data_path
        self.transforms = transforms

    def one_hot(self, img, C):
        out = np.zeros((C, img.shape[1], img.shape[2], img.shape[3]))
        for i in range(C):
            out[i,...] = img == i
        return out

    def __getitem__(self, index):
        path = self.paths[index]
        x, y, x_seg, y_seg = pkload(path)
        x, y = x[None, ...], y[None, ...]
        x_seg, y_seg= x_seg[None, ...], y_seg[None, ...]
        x, x_seg = self.transforms([x, x_seg])
        y, y_seg = self.transforms([y, y_seg])
        x = np.ascontiguousarray(x)
        y = np.ascontiguousarray(y)
        x_seg = np.ascontiguousarray(x_seg)
        y_seg = np.ascontiguousarray(y_seg)
        x, y, x_seg, y_seg = torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(x_seg), torch.from_numpy(y_seg)

        # images must be float
        x = x.float()
        y = y.float()   

        # labels must be Long for one_hot and for any index-based ops
        x_seg = x_seg.long()
        y_seg = y_seg.long()

        return x, y, x_seg, y_seg

    def __len__(self):
        return len(self.paths)
################################################################################
# FILE: experiments/OASIS/inference.py
# SIZE: 20968 bytes
################################################################################

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Unified inference for OASIS Task-3 for 3 models in this repo:
  - TM-DCA    (models.TransMorph_DCA.model)
  - UTSRMorph (models.UTSRMorph.model)
  - CTCF      (models.CTCF.model)

Outputs (per run):
  out_dir/
    per_case.csv
    summary.json
    summary.csv
    png/   (optional visuals, NOW includes deformed grid)
    flows/ (optional displacement fields)

IMPORTANT FIX:
  - Previously def_grid was computed but never drawn into the PNG figure.
    Now it is rendered in an extra row ("Deformed grid").
"""

from __future__ import annotations

import os
import glob
import json
import time
import csv
import math
import argparse
from dataclasses import dataclass
from typing import Dict, Tuple, Optional

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms

from experiments.OASIS import datasets

from utils import (
    NumpyType,
    register_model,
    dice_val_VOI,
    jacobian_det,
    setup_device,
)

def make_grid_img(vol_shape, step=8, thickness=1, device="cuda"):
    D, H, W = vol_shape
    g = torch.zeros((1, 1, D, H, W), device=device, dtype=torch.float32)
    g[:, :, ::step, :, :] = 1
    g[:, :, :, ::step, :] = 1
    g[:, :, :, :, ::step] = 1
    if thickness > 1:
        g = F.max_pool3d(g, kernel_size=thickness, stride=1, padding=thickness//2)
    return g

# ------------------------------ Helpers ------------------------------ #

def ensure_dir(path: str) -> str:
    os.makedirs(path, exist_ok=True)
    return path


def resolve_checkpoint(path_or_dir: str, prefer_best: bool = True) -> str:
    if os.path.isfile(path_or_dir):
        return path_or_dir
    if not os.path.isdir(path_or_dir):
        raise FileNotFoundError(f"Checkpoint path not found: {path_or_dir}")

    exts = (".pth", ".pt", ".pth.tar")
    files = [os.path.join(path_or_dir, f) for f in os.listdir(path_or_dir) if f.endswith(exts)]
    if not files:
        raise RuntimeError(f"No checkpoint files found in directory: {path_or_dir}")

    if prefer_best:
        best = [f for f in files if "best" in os.path.basename(f).lower()]
        if best:
            best.sort()
            return best[-1]

    files.sort(key=lambda p: os.path.getmtime(p))
    return files[-1]


def load_state_dict(model: torch.nn.Module, ckpt_path: str) -> Dict:
    ckpt = torch.load(ckpt_path, map_location="cpu")
    if isinstance(ckpt, dict):
        if "state_dict" in ckpt:
            sd = ckpt["state_dict"]
        elif "model" in ckpt:
            sd = ckpt["model"]
        else:
            sd = ckpt
    else:
        sd = ckpt

    missing, unexpected = model.load_state_dict(sd, strict=False)
    if missing:
        print(f"[WARN] Missing keys: {len(missing)} (up to 10): {missing[:10]}")
    if unexpected:
        print(f"[WARN] Unexpected keys: {len(unexpected)} (up to 10): {unexpected[:10]}")
    return ckpt if isinstance(ckpt, dict) else {"state_dict": sd}


def case_id_from_path(pkl_path: str) -> str:
    base = os.path.basename(pkl_path)
    stem = os.path.splitext(base)[0]
    return stem[2:] if stem.startswith("p_") else stem


def dice_per_label_1to35(def_seg: torch.Tensor, y_seg: torch.Tensor) -> np.ndarray:
    pred = def_seg.detach().cpu().numpy()[0, 0]
    true = y_seg.detach().cpu().numpy()[0, 0]
    out = np.zeros((35,), dtype=np.float64)
    for i, lbl in enumerate(range(1, 36)):
        p = (pred == lbl)
        t = (true == lbl)
        inter = np.sum(p & t)
        union = np.sum(p) + np.sum(t)
        out[i] = (2.0 * inter) / (union + 1e-5)
    return out


def fold_percent_from_flow(flow: torch.Tensor) -> float:
    detJ = jacobian_det(flow.float())  # [B,1,D,H,W]
    return float((detJ <= 0.0).float().mean().item() * 100.0)


def logdet_std_from_flow(flow: torch.Tensor) -> float:
    detJ = jacobian_det(flow.float())
    detJ = torch.clamp(detJ, min=1e-9, max=1e9)
    logdet = torch.log(detJ)
    return float(torch.std(logdet).item())


def compute_ci95(mean: float, std: float, n: int) -> float:
    if n <= 1:
        return 0.0
    sem = std / math.sqrt(n)
    return 1.96 * sem


def require_surface_distance():
    try:
        from surface_distance import compute_robust_hausdorff, compute_surface_distances  # noqa
    except Exception as e:
        raise RuntimeError(
            "HD95 requested but 'surface-distance' is not installed.\n"
            "Install in your env:\n"
            "  pip install surface-distance\n"
        ) from e


def hd95_mean_1to35(def_seg: torch.Tensor, y_seg: torch.Tensor, spacing=(1.0, 1.0, 1.0)) -> float:
    require_surface_distance()
    from surface_distance import compute_robust_hausdorff, compute_surface_distances

    pred = def_seg.detach().cpu().numpy()[0, 0]
    true = y_seg.detach().cpu().numpy()[0, 0]

    hds = []
    for lbl in range(1, 36):
        p = (pred == lbl)
        t = (true == lbl)
        if p.sum() == 0 or t.sum() == 0:
            hds.append(0.0)
        else:
            sd = compute_surface_distances(t, p, spacing)
            hd = compute_robust_hausdorff(sd, 95.0)
            hds.append(float(hd))
    return float(np.mean(hds))


def save_flow_npz(flow: torch.Tensor, path: str):
    arr = flow.detach().cpu().numpy().astype(np.float16)
    np.savez_compressed(path, flow=arr)


def save_png_triplet(
    out_png: str,
    x: torch.Tensor,
    y: torch.Tensor,
    x_seg: torch.Tensor,
    y_seg: torch.Tensor,
    def_seg: torch.Tensor,
    def_grid: Optional[torch.Tensor] = None,
):
    """
    Paper-useful visualization:
      - fixed (3 orthogonal)
      - moving (3 orthogonal)
      - fixed seg (3 orthogonal)
      - warped seg (3 orthogonal)
      - deformed grid (3 orthogonal)  <-- FIXED: now actually rendered
    """
    import matplotlib.pyplot as plt

    x = x.detach().cpu().numpy()[0, 0]
    y = y.detach().cpu().numpy()[0, 0]
    ys = y_seg.detach().cpu().numpy()[0, 0]
    ds = def_seg.detach().cpu().numpy()[0, 0]

    dg = None
    if def_grid is not None:
        # usually [B,1,D,H,W]
        dg = def_grid.detach().cpu().numpy()[0, 0]

    D, H, W = y.shape
    cz, cy, cx = D // 2, H // 2, W // 2

    def slices(vol):
        return (vol[cz, :, :], vol[:, cy, :], vol[:, :, cx])

    y_ax, y_cor, y_sag = slices(y)
    x_ax, x_cor, x_sag = slices(x)
    ys_ax, ys_cor, ys_sag = slices(ys)
    ds_ax, ds_cor, ds_sag = slices(ds)

    n_rows = 5 if dg is not None else 4
    fig = plt.figure(figsize=(12, 12 if dg is not None else 10))
    axs = []

    # Row 1: fixed
    axs.append(fig.add_subplot(n_rows, 3, 1)); axs[-1].imshow(y_ax, cmap="gray"); axs[-1].set_title("Fixed (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 2)); axs[-1].imshow(y_cor, cmap="gray"); axs[-1].set_title("Fixed (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 3)); axs[-1].imshow(y_sag, cmap="gray"); axs[-1].set_title("Fixed (sag)")

    # Row 2: moving
    axs.append(fig.add_subplot(n_rows, 3, 4)); axs[-1].imshow(x_ax, cmap="gray"); axs[-1].set_title("Moving (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 5)); axs[-1].imshow(x_cor, cmap="gray"); axs[-1].set_title("Moving (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 6)); axs[-1].imshow(x_sag, cmap="gray"); axs[-1].set_title("Moving (sag)")

    # Row 3: fixed seg
    axs.append(fig.add_subplot(n_rows, 3, 7)); axs[-1].imshow(ys_ax); axs[-1].set_title("Fixed seg (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 8)); axs[-1].imshow(ys_cor); axs[-1].set_title("Fixed seg (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 9)); axs[-1].imshow(ys_sag); axs[-1].set_title("Fixed seg (sag)")

    # Row 4: warped seg
    axs.append(fig.add_subplot(n_rows, 3, 10)); axs[-1].imshow(ds_ax); axs[-1].set_title("Warped seg (ax)")
    axs.append(fig.add_subplot(n_rows, 3, 11)); axs[-1].imshow(ds_cor); axs[-1].set_title("Warped seg (cor)")
    axs.append(fig.add_subplot(n_rows, 3, 12)); axs[-1].imshow(ds_sag); axs[-1].set_title("Warped seg (sag)")

    # Row 5: deformed grid
    if dg is not None:
        dg_ax, dg_cor, dg_sag = slices(dg)
        axs.append(fig.add_subplot(n_rows, 3, 13)); axs[-1].imshow(dg_ax, cmap="gray"); axs[-1].set_title("Deformed grid (ax)")
        axs.append(fig.add_subplot(n_rows, 3, 14)); axs[-1].imshow(dg_cor, cmap="gray"); axs[-1].set_title("Deformed grid (cor)")
        axs.append(fig.add_subplot(n_rows, 3, 15)); axs[-1].imshow(dg_sag, cmap="gray"); axs[-1].set_title("Deformed grid (sag)")

    for a in axs:
        a.axis("off")

    fig.tight_layout()
    ensure_dir(os.path.dirname(out_png))
    fig.savefig(out_png, dpi=200)
    plt.close(fig)


# ------------------------------ Model builders + forward adapters ------------------------------ #

@dataclass
class ModelBundle:
    name: str
    model: torch.nn.Module
    forward_flow_fn: callable


def build_tm_dca(time_steps: int, vol_size=(160, 192, 224), dwin=(7, 5, 3)) -> ModelBundle:
    from models.TransMorph_DCA.model import CONFIGS as CONFIGS_TM
    import models.TransMorph_DCA.model as TransMorph

    D, H, W = vol_size
    half_size = (D // 2, H // 2, W // 2)

    config = CONFIGS_TM["TransMorph-3-LVL"]
    config.img_size = half_size
    config.dwin_kernel_size = tuple(dwin)
    config.window_size = (D // 32, H // 32, W // 32)

    model = TransMorph.TransMorphCascadeAd(config, time_steps)

    @torch.no_grad()
    def forward_flow(model_, x, y):
        x_half = F.avg_pool3d(x, 2)
        y_half = F.avg_pool3d(y, 2)
        use_amp = torch.cuda.is_available()
        with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
            flow_half = model_((x_half, y_half))
        flow_full = F.interpolate(flow_half.float(), scale_factor=2, mode="trilinear", align_corners=False) * 2.0
        return flow_full

    return ModelBundle("tm-dca", model, forward_flow)


def build_ctcf(time_steps: int, vol_size=(160, 192, 224), dwin=(7, 5, 3), config_key="CTCF-CascadeA") -> ModelBundle:
    from models.CTCF.model import CONFIGS as CONFIGS_CTCF
    import models.CTCF.model as CTCF

    D, H, W = vol_size
    half_size = (D // 2, H // 2, W // 2)

    if config_key not in CONFIGS_CTCF:
        raise KeyError(f"Unknown CTCF config '{config_key}'. Available: {list(CONFIGS_CTCF.keys())}")

    config = CONFIGS_CTCF[config_key]
    config.img_size = half_size
    if hasattr(config, "dwin_kernel_size"):
        config.dwin_kernel_size = tuple(dwin)
    if hasattr(config, "window_size"):
        config.window_size = (D // 32, H // 32, W // 32)

    if hasattr(CTCF, "CTCF_CascadeA"):
        model = CTCF.CTCF_CascadeA(config)
    else:
        raise AttributeError("models.CTCF.model does not expose CTCF_DCA_SR; check your model module.")

    @torch.no_grad()
    def forward_flow(model_, x, y):
        use_amp = torch.cuda.is_available()
        with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
            _, flow_full = model_(x, y)
        return flow_full

    return ModelBundle("ctcf", model, forward_flow)


def build_utsrmorph(config_key: str = "UTSRMorph-Large") -> ModelBundle:
    from models.UTSRMorph.model import CONFIGS as CONFIGS_UM, UTSRMorph

    if config_key not in CONFIGS_UM:
        raise KeyError(f"Unknown UTSRMorph config '{config_key}'. Available: {list(CONFIGS_UM.keys())}")

    config = CONFIGS_UM[config_key]
    model = UTSRMorph(config)

    @torch.no_grad()
    def forward_flow(model_, x, y):
        inp = torch.cat((x, y), dim=1)
        use_amp = torch.cuda.is_available()
        with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
            _, flow = model_(inp)
        return flow

    return ModelBundle("utsrmorph", model, forward_flow)


def build_model(model_name: str, args) -> ModelBundle:
    model_name = model_name.lower().strip()
    if model_name in ("tm-dca", "tm_dca", "tmdca"):
        return build_tm_dca(time_steps=args.time_steps, vol_size=args.vol_size, dwin=args.dwin)
    if model_name in ("ctcf",):
        return build_ctcf(time_steps=args.time_steps, vol_size=args.vol_size, dwin=args.dwin, config_key=args.ctcf_config)
    if model_name in ("utsrmorph", "utsr"):
        return build_utsrmorph(config_key=args.utsr_config)
    raise ValueError("Unknown --model. Use one of: tm-dca, utsrmorph, ctcf")


# ------------------------------ Inference ------------------------------ #

def run_inference(args):
    dev = setup_device(args.gpu, seed=args.seed, deterministic=args.deterministic)
    device = dev.device

    test_dir = args.test_dir
    if not test_dir.endswith(os.sep):
        test_dir += os.sep

    out_dir = ensure_dir(args.out_dir)
    png_dir = ensure_dir(os.path.join(out_dir, "png")) if args.save_pngs else None
    flow_dir = ensure_dir(os.path.join(out_dir, "flows")) if args.save_flow else None

    test_files = sorted(glob.glob(test_dir + "*.pkl"))
    if not test_files:
        raise RuntimeError(f"No .pkl files found in test_dir: {test_dir}")

    test_tf = transforms.Compose([NumpyType((np.float32, np.int16))])
    test_set = datasets.OASISBrainInferDataset(test_files, transforms=test_tf)
    test_loader = DataLoader(
        test_set,
        batch_size=1,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True,
    )

    bundle = build_model(args.model, args)
    model = bundle.model.to(device)
    forward_flow_fn = bundle.forward_flow_fn

    ckpt_path = resolve_checkpoint(args.ckpt, prefer_best=not args.prefer_last)
    print(f"[INFO] Model: {bundle.name}")
    print(f"[INFO] Checkpoint: {ckpt_path}")
    _ = load_state_dict(model, ckpt_path)
    model.eval()

    reg_nearest = None
    reg_bilin = None

    per_case_path = os.path.join(out_dir, "per_case.csv")
    header = (
        ["case_id", "dice_mean", "fold_percent", "logdet_std", "time_sec"]
        + (["hd95_mean"] if args.hd95 else [])
        + [f"dice_lbl_{i}" for i in range(1, 36)]
    )

    rows = []

    with torch.no_grad():
        for idx, batch in enumerate(test_loader):
            x, y, x_seg, y_seg = [t.to(device, non_blocking=True) for t in batch]
            vol_shape = tuple(x.shape[2:])

            if reg_nearest is None:
                reg_nearest = register_model(vol_shape, mode="nearest").to(device)
                reg_bilin = register_model(vol_shape, mode="bilinear").to(device)

            cid = case_id_from_path(test_files[idx])

            t0 = time.perf_counter()
            flow = forward_flow_fn(model, x, y)
            def_seg = reg_nearest([x_seg.float(), flow.float()])
            dt = time.perf_counter() - t0

            dice_mean = float(dice_val_VOI(def_seg.long(), y_seg.long()).item())
            dice_lbl = dice_per_label_1to35(def_seg.long(), y_seg.long())
            foldp = fold_percent_from_flow(flow)
            logdet_std = logdet_std_from_flow(flow)

            row = {
                "case_id": cid,
                "dice_mean": dice_mean,
                "fold_percent": foldp,
                "logdet_std": logdet_std,
                "time_sec": dt,
            }

            if args.hd95:
                row["hd95_mean"] = hd95_mean_1to35(def_seg.long(), y_seg.long(), spacing=(1.0, 1.0, 1.0))

            for i in range(35):
                row[f"dice_lbl_{i+1}"] = float(dice_lbl[i])

            rows.append(row)

            if args.save_flow:
                save_flow_npz(flow, os.path.join(flow_dir, f"flow_{cid}.npz"))

            if args.save_pngs and (args.png_limit < 0 or idx < args.png_limit):
                grid_img = make_grid_img(vol_shape, step=args.grid_step, thickness=args.line_thickness, device=device)
                def_grid = reg_bilin([grid_img.float(), flow.float()])

                save_png_triplet(
                    out_png=os.path.join(png_dir, f"{cid}.png"),
                    x=x, y=y, x_seg=x_seg, y_seg=y_seg, def_seg=def_seg, def_grid=def_grid,
                )

            if (idx + 1) % max(1, args.print_every) == 0:
                msg = f"[{idx+1:03d}/{len(test_loader):03d}] {cid} dice={dice_mean:.4f} fold%={foldp:.4f} time={dt:.3f}s"
                if args.hd95:
                    msg += f" hd95={row['hd95_mean']:.4f}"
                print(msg)

    with open(per_case_path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=header)
        w.writeheader()
        for r in rows:
            w.writerow(r)

    def agg(key: str) -> Tuple[float, float]:
        arr = np.array([r[key] for r in rows], dtype=np.float64)
        return float(arr.mean()), float(arr.std(ddof=1)) if len(arr) > 1 else 0.0

    n = len(rows)
    summary = {
        "model": bundle.name,
        "ckpt_path": ckpt_path,
        "test_dir": args.test_dir,
        "n_cases": n,
        "metrics": {},
    }

    keys = ["dice_mean", "fold_percent", "logdet_std", "time_sec"] + (["hd95_mean"] if args.hd95 else [])
    for key in keys:
        m, s = agg(key)
        sem = s / math.sqrt(n) if n > 1 else 0.0
        ci95 = compute_ci95(m, s, n)
        summary["metrics"][key] = {"mean": m, "std": s, "sem": sem, "ci95": ci95}

    summary_path = os.path.join(out_dir, "summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    summary_csv_path = os.path.join(out_dir, "summary.csv")
    with open(summary_csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["metric", "mean", "std", "sem", "ci95"])
        for k, v in summary["metrics"].items():
            w.writerow([k, f"{v['mean']:.6f}", f"{v['std']:.6f}", f"{v['sem']:.6f}", f"{v['ci95']:.6f}"])

    print(f"\n[SAVED] per-case:  {per_case_path}")
    print(f"[SAVED] summary:   {summary_path}")
    print(f"[SAVED] summary:   {summary_csv_path}")
    if args.save_pngs:
        print(f"[SAVED] png dir:   {png_dir}")
    if args.save_flow:
        print(f"[SAVED] flow dir:  {flow_dir}")


# ------------------------------ CLI ------------------------------ #

def build_parser():
    p = argparse.ArgumentParser(description="Unified OASIS inference for TM-DCA / UTSRMorph / CTCF")
    sub = p.add_subparsers(dest="cmd", required=True)

    pinf = sub.add_parser("infer", help="Run inference for one model and export metrics/visuals")
    pinf.add_argument("--model", required=True, choices=["tm-dca", "utsrmorph", "ctcf"])
    pinf.add_argument("--ckpt", required=True, help="Checkpoint FILE or directory containing checkpoints")
    pinf.add_argument("--test_dir", required=True, help="Path to OASIS Test/*.pkl")
    pinf.add_argument("--out_dir", required=True, help="Output directory (will be created)")
    pinf.add_argument("--gpu", type=int, default=0)
    pinf.add_argument("--seed", type=int, default=0)
    pinf.add_argument("--deterministic", action="store_true")
    pinf.add_argument("--num_workers", type=int, default=4)
    pinf.add_argument("--print_every", type=int, default=1)
    pinf.add_argument("--prefer_last", action="store_true", help="Prefer last/newest instead of best")
    pinf.add_argument("--save_flow", action="store_true", help="Save flow fields as .npz")
    pinf.add_argument("--save_pngs", action="store_true", help="Save PNG visuals (requires matplotlib)")
    pinf.add_argument("--png_limit", type=int, default=5, help="How many cases to save PNG for (-1 = all)")
    pinf.add_argument("--grid_step", type=int, default=8)
    pinf.add_argument("--line_thickness", type=int, default=1)
    pinf.add_argument("--hd95", action="store_true", help="Compute HD95 mean over labels 1..35 (requires surface-distance)")

    pinf.add_argument("--time_steps", type=int, default=12, help="TM-DCA/CTCF cascade steps")
    pinf.add_argument("--vol_size", type=int, nargs=3, default=[160, 192, 224], help="Volume size D H W")
    pinf.add_argument("--dwin", type=int, nargs=3, default=[7, 5, 3], help="TM-DCA/CTCF dwin kernel size")
    pinf.add_argument("--utsr_config", type=str, default="UTSRMorph-Large", help="UTSRMorph config key")
    pinf.add_argument("--ctcf_config", type=str, default="CTCF-CascadeA", help="CTCF config key")

    return p


def main():
    parser = build_parser()
    args = parser.parse_args()
    if args.cmd == "infer":
        run_inference(args)
    else:
        raise RuntimeError("Unknown command")


if __name__ == "__main__":
    main()

################################################################################
# FILE: experiments/OASIS/train_CTCF.py
# SIZE: 4140 bytes
################################################################################

import os, glob, argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim

from experiments.OASIS import datasets
from experiments.engine import add_engine_args, apply_paths, run_train
from models.CTCF.model import CTCF_CascadeA, CONFIGS as CONFIGS_CTCF

from utils import (
    NumpyType, 
    NCC_vxm, 
    Grad3d, 
    register_model, 
    icon_loss,
    neg_jacobian_penalty, 
    ctcf_schedule, 
    adjust_lr_ctcf_schedule
)


CTX = {}  # minimal shared context


@torch.no_grad()
def forward_flow_fn(model, x, y, device, args):
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        _, flow_full = model(x, y)
    return flow_full


def train_step(model, batch, device, args, epoch):
    x, y, *_ = batch
    x, y = x.to(device).float(), y.to(device).float()

    alpha_l1, alpha_l3, warm = ctcf_schedule(epoch, args.max_epoch)
    W_icon, W_cyc, W_jac = args.w_icon * warm, args.w_cyc * warm, args.w_jac * warm

    def_xy, flow_xy = model(x, y, return_all=False, alpha_l1=alpha_l1, alpha_l3=alpha_l3)
    def_yx, flow_yx = model(y, x, return_all=False, alpha_l1=alpha_l1, alpha_l3=alpha_l3)

    with torch.autocast(device_type="cuda", enabled=False):
        L_ncc = 0.5 * (CTX["ncc"](def_xy.float(), y.float()) + CTX["ncc"](def_yx.float(), x.float())) * args.w_ncc

    L_icon = icon_loss(flow_xy, flow_yx) * W_icon
    L_reg = 0.5 * (CTX["reg"](flow_xy) + CTX["reg"](flow_yx)) * args.w_reg
    L_jac = 0.5 * (neg_jacobian_penalty(flow_xy) + neg_jacobian_penalty(flow_yx)) * W_jac
    L_cyc = ((CTX["reg_model"]((def_xy, flow_yx)) - x).abs().mean() + (CTX["reg_model"]((def_yx, flow_xy)) - y).abs().mean()) * W_cyc

    loss = L_ncc + L_icon + L_reg + L_jac + L_cyc
    logs = {"all": loss.item(), "ncc": L_ncc.item(), "reg": L_reg.item(), "icon": L_icon.item(), "cyc": L_cyc.item(), "jac": L_jac.item()}
    return loss, logs


def build_model(device, args):
    config = CONFIGS_CTCF["CTCF-CascadeA"]
    config.time_steps = int(args.time_steps)
    model = CTCF_CascadeA(config).to(device)
    opt = optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)

    CTX["config"] = config
    CTX["ncc"] = NCC_vxm(win=(9, 9, 9))
    CTX["reg"] = Grad3d(penalty="l2")
    CTX["reg_model"] = register_model(config.img_size, mode="bilinear").to(device)
    return model, opt


def build_loaders(args):
    tfm = transforms.Compose([NumpyType((np.float32, np.int16))])
    train_files = glob.glob(os.path.join(args.train_dir, "*.pkl"))
    val_files = glob.glob(os.path.join(args.val_dir, "*.pkl"))
    train_set = datasets.OASISBrainDataset(train_files, transforms=tfm)
    val_set = datasets.OASISBrainDataset(val_files, transforms=tfm)
    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=False)
    val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)
    return train_loader, val_loader


def lr_step(optimizer, epoch, args):
    return adjust_lr_ctcf_schedule(optimizer, epoch, args.max_epoch, args.lr)


def parse_args():
    p = argparse.ArgumentParser()
    add_engine_args(p, dataset="OASIS")
    p.set_defaults(exp="CTCF")
    p.add_argument("--w_ncc", type=float, default=1.0)
    p.add_argument("--w_reg", type=float, default=1.0)
    p.add_argument("--w_icon", type=float, default=0.05)
    p.add_argument("--w_cyc", type=float, default=0.02)
    p.add_argument("--w_jac", type=float, default=0.005)
    p.add_argument("--time_steps", type=int, default=12)
    return p.parse_args()


def main():
    args = parse_args()
    apply_paths(args, dataset="OASIS")
    run_train(dataset="OASIS", args=args, build_model=build_model, build_loaders=build_loaders,
              train_step=train_step, forward_flow_fn=forward_flow_fn, lr_step=lr_step)


if __name__ == "__main__":
    main()
################################################################################
# FILE: experiments/OASIS/train_TransMorphDCA.py
# SIZE: 3624 bytes
################################################################################

import os, glob, argparse
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim

from experiments.OASIS import datasets
from experiments.engine import add_engine_args, apply_paths, run_train
from models.TransMorph_DCA.model import TransMorphCascadeAd, CONFIGS as CONFIGS_TM

from utils import (
    NumpyType, 
    NCC_vxm, 
    Grad3d, 
    register_model, 
    adjust_learning_rate_poly
)     


CTX = {}


@torch.no_grad()
def forward_flow_fn(model, x, y, device, args):
    xh, yh = F.avg_pool3d(x, 2), F.avg_pool3d(y, 2)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        flow_h = model((xh, yh))
    return F.interpolate(flow_h.float(), scale_factor=2, mode="trilinear", align_corners=False) * 2.0


def train_step(model, batch, device, args, epoch):
    x, y, *_ = batch
    x, y = x.to(device).float(), y.to(device).float()

    xh, yh = F.avg_pool3d(x, 2), F.avg_pool3d(y, 2)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        flow_h = model((xh, yh))
    flow = F.interpolate(flow_h.float(), scale_factor=2, mode="trilinear", align_corners=False) * 2.0
    def_x = CTX["reg_model"]((x, flow))

    with torch.autocast(device_type="cuda", enabled=False):
        L_ncc = CTX["ncc"](def_x.float(), y.float()) * args.w_ncc
    L_reg = CTX["reg"](flow) * args.w_reg
    loss = L_ncc + L_reg
    return loss, {"all": loss.item(), "ncc": L_ncc.item(), "reg": L_reg.item()}


def build_model(device, args):
    D,H,W = args.vol_size
    cfg = CONFIGS_TM["TransMorph-3-LVL"]; 
    cfg.img_size = (D//2, H//2, W//2)

    model = TransMorphCascadeAd(cfg, int(args.time_steps)).to(device)
    opt = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0, amsgrad=True)

    CTX["ncc"] = NCC_vxm()
    CTX["reg"] = Grad3d(penalty="l2")
    CTX["reg_model"] = register_model((D, H, W), mode="bilinear").to(device)
    return model, opt


def build_loaders(args):
    tfm = transforms.Compose([NumpyType((np.float32, np.int16))])
    tr = datasets.OASISBrainDataset(glob.glob(os.path.join(args.train_dir, "*.pkl")), transforms=tfm)
    va = datasets.OASISBrainInferDataset(glob.glob(os.path.join(args.val_dir, "*.pkl")), transforms=tfm)
    train_loader = DataLoader(tr, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=False)
    val_loader = DataLoader(va, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)
    return train_loader, val_loader


def lr_step(optimizer, epoch, args):
    return adjust_learning_rate_poly(optimizer, epoch, args.max_epoch, args.lr)


def parse_args():
    p = argparse.ArgumentParser()
    add_engine_args(p, dataset="OASIS")
    p.set_defaults(exp="TM_DCA_OASIS")
    p.add_argument("--w_ncc", type=float, default=1.0)
    p.add_argument("--w_reg", type=float, default=1.0)
    p.add_argument("--time_steps", type=int, default=12)
    p.add_argument("--vol_size", type=int, nargs=3, default=[160, 192, 224])
    return p.parse_args()


def main():
    args = parse_args()
    apply_paths(args, dataset="OASIS")
    run_train(dataset="OASIS", args=args, build_model=build_model, build_loaders=build_loaders,
              train_step=train_step, forward_flow_fn=forward_flow_fn, lr_step=lr_step)


if __name__ == "__main__":
    main()
################################################################################
# FILE: experiments/OASIS/train_UTSRMorph.py
# SIZE: 2950 bytes
################################################################################

import os, glob, argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torch import optim

from experiments.OASIS import datasets
from experiments.engine import add_engine_args, apply_paths, run_train
from models.UTSRMorph.model import UTSRMorph, CONFIGS as CONFIGS_UM

from utils import (
    NumpyType, 
    NCC_vxm, 
    Grad3d, 
    adjust_learning_rate_poly
)


CTX = {}


@torch.no_grad()
def forward_flow_fn(model, x, y, device, args):
    inp = torch.cat((x, y), dim=1)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        _, flow = model(inp)
    return flow


def train_step(model, batch, device, args, epoch):
    x, y, *_ = batch
    x, y = x.to(device).float(), y.to(device).float()

    inp = torch.cat((x, y), dim=1)
    use_amp = torch.cuda.is_available()
    with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
        def_x, flow = model(inp)

    with torch.autocast(device_type="cuda", enabled=False):
        L_ncc = CTX["ncc"](def_x.float(), y.float()) * args.w_ncc
    L_reg = CTX["reg"](flow) * args.w_reg

    loss = L_ncc + L_reg
    return loss, {"all": loss.item(), "ncc": L_ncc.item(), "reg": L_reg.item()}


def build_model(device, args):
    cfg = CONFIGS_UM["UTSRMorph-Large"]
    model = UTSRMorph(cfg).to(device)
    opt = optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)
    CTX["ncc"] = NCC_vxm()
    CTX["reg"] = Grad3d(penalty="l2")
    return model, opt


def build_loaders(args):
    tfm = transforms.Compose([NumpyType((np.float32, np.int16))])
    tr = datasets.OASISBrainDataset(glob.glob(os.path.join(args.train_dir, "*.pkl")), transforms=tfm)
    va = datasets.OASISBrainInferDataset(glob.glob(os.path.join(args.val_dir, "*.pkl")), transforms=tfm)
    train_loader = DataLoader(tr, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=False)
    val_loader = DataLoader(va, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)
    return train_loader, val_loader


def lr_step(optimizer, epoch, args):
    return adjust_learning_rate_poly(optimizer, epoch, args.max_epoch, args.lr)


def parse_args():
    p = argparse.ArgumentParser()
    add_engine_args(p, dataset="OASIS")
    p.set_defaults(exp="UTSRMorph_OASIS")
    p.add_argument("--w_ncc", type=float, default=1.0)
    p.add_argument("--w_reg", type=float, default=1.0)
    return p.parse_args()


def main():
    args = parse_args()
    apply_paths(args, dataset="OASIS")
    run_train(dataset="OASIS", args=args, build_model=build_model, build_loaders=build_loaders,
              train_step=train_step, forward_flow_fn=forward_flow_fn, lr_step=lr_step)


if __name__ == "__main__":
    main()
################################################################################
# FILE: experiments/__init__.py
# SIZE: 0 bytes
################################################################################


################################################################################
# FILE: experiments/engine.py
# SIZE: 8764 bytes
################################################################################

import os, time, argparse
import torch
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter

from utils import (
    AverageMeter, 
    setup_device, 
    make_exp_dirs, 
    attach_stdout_logger, 
    load_checkpoint_if_exists,
    perf_epoch_start, 
    perf_epoch_end, 
    dice_val_VOI, 
    register_model, 
    mk_grid_img, 
    comput_fig,
    validate, 
    save_checkpoint
)


PATHS = {
    1: {
        "OASIS": {"train_dir": "C:/Users/user/Documents/Education/MasterWork/datasets/OASIS_L2R_2021_task03/All",
                 "val_dir": "C:/Users/user/Documents/Education/MasterWork/datasets/OASIS_L2R_2021_task03/Test"},
        "IXI":  {"train_dir": "C:/Users/user/Documents/Education/MasterWork/datasets/IXI_data/Train",
                 "val_dir": "C:/Users/user/Documents/Education/MasterWork/datasets/IXI_data/Val",
                 "atlas_path": "C:/Users/user/Documents/Education/MasterWork/datasets/IXI_data/atlas.pkl"},
    },
    2: {
        "OASIS": {"train_dir": "/home/roman/P/OASIS_L2R_2021_task03/All",
                 "val_dir": "/home/roman/P/OASIS_L2R_2021_task03/Test"},
        "IXI":  {"train_dir": "/home/roman/P/IXI_data/Train",
                 "val_dir": "/home/roman/P/IXI_data/Val",
                 "atlas_path":"/home/roman/P/IXI_data/atlas.pkl"},
    },
}


def _norm_dir(p: str) -> str: return p.rstrip("/\\") + os.sep


def add_engine_args(p: argparse.ArgumentParser, *, dataset: str):
    p.add_argument("--1", dest="paths", action="store_const", const=1, help="Use path profile #1")
    p.add_argument("--2", dest="paths", action="store_const", const=2, help="Use path profile #2")
    p.add_argument("--paths", type=int, default=1, help="Path profile id (1/2/...)")
    p.add_argument("--train_dir", default="")
    p.add_argument("--val_dir", default="")
    if dataset.upper() == "IXI": p.add_argument("--atlas_path", default="")
    p.add_argument("--exp", default="")
    p.add_argument("--gpu", type=int, default=0)
    p.add_argument("--max_epoch", type=int, default=400)
    p.add_argument("--batch_size", type=int, default=1)
    p.add_argument("--lr", type=float, default=1e-4)
    p.add_argument("--num_workers", type=int, default=4)
    p.add_argument("--resume", default="")
    p.add_argument("--tb_images_every", type=int, default=5)
    p.add_argument("--grid_step", type=int, default=8)
    p.add_argument("--line_thickness", type=int, default=1)
    return p


def apply_paths(args, *, dataset: str):
    ds = dataset.upper()
    prof = int(getattr(args, "paths", 1) or 1)
    prof_map = PATHS.get(prof, {}).get(ds, {})
    if not args.train_dir: args.train_dir = prof_map.get("train_dir", "")
    if not args.val_dir:   args.val_dir   = prof_map.get("val_dir", "")
    if ds == "IXI" and hasattr(args, "atlas_path") and not args.atlas_path:
        args.atlas_path = prof_map.get("atlas_path", "")
    args.train_dir, args.val_dir = _norm_dir(args.train_dir), _norm_dir(args.val_dir)
    if ds == "IXI" and hasattr(args, "atlas_path"): args.atlas_path = args.atlas_path.rstrip("/\\")
    return args


def write_tb_images(writer: SummaryWriter, last_vis: dict, epoch: int):
    if not last_vis: return
    def_out, def_grid, x_vis, y_vis = last_vis.get("def_seg"), last_vis.get("def_grid"), last_vis.get("x_seg"), last_vis.get("y_seg")
    if def_out is None or def_grid is None or x_vis is None or y_vis is None: return
    plt.switch_backend("agg")
    pred_fig, grid_fig, x_fig, tar_fig = comput_fig(def_out), comput_fig(def_grid), comput_fig(x_vis), comput_fig(y_vis)
    writer.add_figure("Grid", grid_fig, epoch); plt.close(grid_fig)
    writer.add_figure("input", x_fig, epoch); plt.close(x_fig)
    writer.add_figure("ground truth", tar_fig, epoch); plt.close(tar_fig)
    writer.add_figure("prediction", pred_fig, epoch); plt.close(pred_fig)


def run_train(
    *, dataset: str, args,
    build_model, build_loaders,
    train_step, forward_flow_fn,
    lr_step=None, extra_state=None
):
    device = setup_device(gpu_id=int(args.gpu), seed=0, deterministic=False)
    paths = make_exp_dirs(args.exp or "EXP")
    attach_stdout_logger(paths.log_dir)
    ckpt_dir = os.path.join(paths.exp_dir, "ckpt"); os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(os.path.join(paths.exp_dir, "vis"), exist_ok=True)
    writer = SummaryWriter(log_dir=paths.log_dir)

    use_amp = torch.cuda.is_available()
    model, optimizer = build_model(device, args)
    scaler = torch.amp.GradScaler("cuda")
    train_loader, val_loader = build_loaders(args)

    epoch_start, best_dsc = 0, -1.0
    if args.resume:
        ckpt = load_checkpoint_if_exists(args.resume, model=model, optimizer=optimizer, map_location=device)
        if ckpt is not None:
            epoch_start = int(ckpt.get("epoch", -1)) + 1
            best_dsc = float(ckpt.get("best_dsc", best_dsc))
            scaler.load_state_dict(ckpt["scaler"])
            print(f">>> Resumed from {args.resume} @ epoch {epoch_start}, best={best_dsc:.4f}")


    print(f">>> Experiment: {args.exp} | dataset = {dataset} | paths = {getattr(args,'paths',1)}")
    print(f"    train_dir = {args.train_dir}")
    print(f"    val_dir = {args.val_dir}")
    if dataset.upper() == "IXI" and hasattr(args, "atlas_path"): print(f"    atlas    ={args.atlas_path}")


    for epoch in range(epoch_start, int(args.max_epoch)):
        model.train()
        t0 = perf_epoch_start()
        if lr_step: writer.add_scalar("LR", float(lr_step(optimizer, epoch, args)), epoch)

        meters = {}
        iter_time_sum = 0.0
        print(f"Training Starts (epoch {epoch:03d})")

        for it, batch in enumerate(train_loader, start=1):
            t_it = time.perf_counter()
            optimizer.zero_grad(set_to_none=True)
            with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
                loss, logs = train_step(model, batch, device, args, epoch)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
            
            if logs:
                for k, v in logs.items():
                    if k not in meters: meters[k] = AverageMeter()
                    meters[k].update(float(v), 1)

            iter_time_sum += (time.perf_counter() - t_it)
            if it % 10 == 0 and meters:
                keys = list(meters.keys())
                main = keys[0]
                msg = f"Iter {it:4d} / {len(train_loader):4d} | {main}(avg)={meters[main].avg:.4f}"
                if "ncc" in meters and "reg" in meters: msg += f" | last NCC={meters['ncc'].val:.4f} REG={meters['reg'].val:.4f}"
                print(msg)

        for k, m in meters.items(): writer.add_scalar(f"Loss/train_{k}", m.avg, epoch)

        perf = perf_epoch_end(t0, iters=len(train_loader), iter_time_sum=iter_time_sum)
        writer.add_scalar("perf/epoch_time_sec", perf.epoch_time_sec, epoch)
        writer.add_scalar("perf/iter_time_ms", perf.mean_iter_time_ms, epoch)
        if perf.peak_gpu_mem_gib is not None: writer.add_scalar("perf/peak_gpu_mem_GB", perf.peak_gpu_mem_gib, epoch)

        model.eval()
        with torch.no_grad():
            val_res = validate(
                model=model, val_loader=val_loader, device=device,
                forward_flow_fn=lambda x, y: forward_flow_fn(model, x, y, device, args),
                dice_fn=dice_val_VOI, register_model_cls=register_model, mk_grid_img_fn=mk_grid_img,
                grid_step=int(args.grid_step), line_thickness=int(args.line_thickness)
            )

        dsc, foldp = float(val_res.dsc), float(val_res.fold_percent)
        writer.add_scalar("DSC/validate", dsc, epoch)
        writer.add_scalar("Metric/validate_fold_percent", foldp, epoch)

        if args.tb_images_every and (epoch % int(args.tb_images_every) == 0): write_tb_images(writer, val_res.last_vis or {}, epoch)

        is_best = dsc > best_dsc
        if is_best: best_dsc = dsc

        state = {"epoch": epoch, "state_dict": model.state_dict(), "optimizer": optimizer.state_dict(), "best_dsc": best_dsc}
        if scaler is not None: state["scaler"] = scaler.state_dict()
        if extra_state: state.update(extra_state(args=args, epoch=epoch) or {})

        torch.save(state, os.path.join(ckpt_dir, "last.pth"))
        if is_best: torch.save(state, os.path.join(ckpt_dir, "best.pth"))
        save_checkpoint(state, save_dir=os.path.join(ckpt_dir, "epochs"), filename=f"epoch_{epoch:04d}.pth", max_model_num=8)

        print(f"[epoch {epoch:03d}] val_dice={dsc:.4f} best={best_dsc:.4f} | fold%={foldp:.2f}")

    writer.close()
################################################################################
# FILE: models/CTCF/__init__.py
# SIZE: 25 bytes
################################################################################

# models/CTCF/__init__.py
################################################################################
# FILE: models/CTCF/cascade_nets.py
# SIZE: 4671 bytes
################################################################################

# models/CTCF/cascade_nets.py

from __future__ import annotations

import torch
import torch.nn as nn
import torch.nn.functional as F


class ConvBlock(nn.Module):
    def __init__(self, in_ch: int, out_ch: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv3d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.InstanceNorm3d(out_ch, affine=True),
            nn.LeakyReLU(0.1, inplace=True),
            nn.Conv3d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.InstanceNorm3d(out_ch, affine=True),
            nn.LeakyReLU(0.1, inplace=True),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class CoarseFlowNetQuarter(nn.Module):
    """
    Level-1: quarter-res coarse flow predictor (conv-only).
    Input: mov_q, fix_q -> concat (B,2,D,H,W)
    Output: flow_q (B,3,D,H,W)
    """
    def __init__(self, base_ch: int = 16):
        super().__init__()
        c = int(base_ch)

        self.enc1 = ConvBlock(2, c)
        self.pool1 = nn.AvgPool3d(2)

        self.enc2 = ConvBlock(c, c * 2)
        self.pool2 = nn.AvgPool3d(2)

        self.bot = ConvBlock(c * 2, c * 4)
        self.up2 = nn.Upsample(scale_factor=2, mode="trilinear", align_corners=False)
        self.dec2 = ConvBlock(c * 4 + c * 2, c * 2)

        self.up1 = nn.Upsample(scale_factor=2, mode="trilinear", align_corners=False)
        self.dec1 = ConvBlock(c * 2 + c, c)
        self.out = nn.Conv3d(c, 3, kernel_size=3, padding=1, bias=True)

        nn.init.zeros_(self.out.weight)
        nn.init.zeros_(self.out.bias)

    def forward(self, mov: torch.Tensor, fix: torch.Tensor) -> torch.Tensor:
        x = torch.cat([mov, fix], dim=1)

        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b = self.bot(self.pool2(e2))

        d2 = self.up2(b)
        d2 = self.dec2(torch.cat([d2, e2], dim=1))

        d1 = self.up1(d2)
        d1 = self.dec1(torch.cat([d1, e1], dim=1))

        return self.out(d1)


class FlowRefiner3D(nn.Module):
    """
    Level-3: half-res refinement using error-map.
    Inputs:
      - mov_warp_half: (B,1,D,H,W)
      - fix_half:      (B,1,D,H,W)
      - flow_half:     (B,3,D,H,W)  current flow (context)
    Output:
      - delta_flow_half: (B,3,D,H,W)
    """
    def __init__(self, base_ch: int = 16, error_mode: str = "gradmag"):
        super().__init__()
        self.error_mode = str(error_mode)
        c = int(base_ch)

        # channels: mov(1) + fix(1) + err(1) + flow(3) = 6
        self.enc1 = ConvBlock(6, c)
        self.pool1 = nn.AvgPool3d(2)

        self.enc2 = ConvBlock(c, c * 2)
        self.pool2 = nn.AvgPool3d(2)

        self.bot = ConvBlock(c * 2, c * 4)

        self.up2 = nn.Upsample(scale_factor=2, mode="trilinear", align_corners=False)
        self.dec2 = ConvBlock(c * 4 + c * 2, c * 2)

        self.up1 = nn.Upsample(scale_factor=2, mode="trilinear", align_corners=False)
        self.dec1 = ConvBlock(c * 2 + c, c)
        self.out = nn.Conv3d(c, 3, kernel_size=3, padding=1, bias=True)

        nn.init.zeros_(self.out.weight)
        nn.init.zeros_(self.out.bias)

    @staticmethod
    def _grad_mag(x: torch.Tensor) -> torch.Tensor:
        # x: (B,1,D,H,W)
        dz = x[:, :, 1:, :, :] - x[:, :, :-1, :, :]
        dy = x[:, :, :, 1:, :] - x[:, :, :, :-1, :]
        dx = x[:, :, :, :, 1:] - x[:, :, :, :, :-1]

        dz = torch.nn.functional.pad(dz, (0, 0, 0, 0, 0, 1))
        dy = torch.nn.functional.pad(dy, (0, 0, 0, 1, 0, 0))
        dx = torch.nn.functional.pad(dx, (0, 1, 0, 0, 0, 0))

        return torch.sqrt(dx * dx + dy * dy + dz * dz + 1e-6)

    def _error_map(self, mov_w: torch.Tensor, fix: torch.Tensor) -> torch.Tensor:
        if self.error_mode == "absdiff":
            return (mov_w - fix).abs()
        if self.error_mode == "gradmag":
            gm_m = self._grad_mag(mov_w)
            gm_f = self._grad_mag(fix)
            return (gm_m - gm_f).abs()
        raise ValueError(f"Unsupported error_mode: {self.error_mode}")

    def forward(self, mov_warp: torch.Tensor, fix: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:
        err = self._error_map(mov_warp, fix)
        x = torch.cat([mov_warp, fix, err, flow], dim=1)  # (B,6,D,H,W)

        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b = self.bot(self.pool2(e2))

        d2 = self.up2(b)
        d2 = self.dec2(torch.cat([d2, e2], dim=1))

        d1 = self.up1(d2)
        d1 = self.dec1(torch.cat([d1, e1], dim=1))

        return self.out(d1)
################################################################################
# FILE: models/CTCF/configs.py
# SIZE: 902 bytes
################################################################################

import ml_collections


def get_CTCF_config():
    c = ml_collections.ConfigDict()
    c.if_transskip = True
    c.if_convskip = True
    c.in_chans = 1
    c.img_size = (160, 192, 224)
    c.patch_size = 4

    c.embed_dim = 96
    c.depths = (4, 4, 5)
    c.num_heads = (8, 8, 8)
    c.window_size = (5, 6, 7)
    c.dwin_size = (7, 5, 3)

    c.mlp_ratio = 4
    c.pat_merg_rf = 4

    c.qkv_bias = False
    c.drop_rate = 0.0
    c.drop_path_rate = 0.3
    c.ape = False
    c.spe = False
    c.rpe = True
    c.patch_norm = True
    c.use_checkpoint = False
    c.out_indices = (0, 1, 2)

    c.reg_head_chan = 16
    c.time_steps = 12

    # Cascade switches
    c.use_level1 = True
    c.level1_base_ch = 16
    c.use_level2 = True
    c.use_level3 = True
    c.level3_base_ch = 16

    return c


CONFIGS = {
    "CTCF-CascadeA": get_CTCF_config(),
}
################################################################################
# FILE: models/CTCF/model.py
# SIZE: 8712 bytes
################################################################################

from typing import Optional, Tuple, List

import torch
import torch.nn as nn

from models.TransMorph_DCA.model import (
    SwinTransformer,
    Conv3dReLU,
    RegistrationHead,
    SpatialTransformer,
)

from models.CTCF.ut_blocks import SRUpBlock3D, CAB, upsample_flow
from models.CTCF.cascade_nets import CoarseFlowNetQuarter, FlowRefiner3D
from models.CTCF.configs import CONFIGS


class CTCF_DCA_CoreHalf(nn.Module):
    """
    Level-2: TM-DCA Swin encoder + SR-style decoder blocks + time integration.
    Operates on HALF-res grid derived from config.img_size.
    Inputs: [B, 1, D, H, W] on HALF grid
    Outputs: def_x_half [B,1,D,H,W], flow_half [B,3,D,H,W]
    """
    def __init__(self, config, time_steps: int):
        super().__init__()

        self.if_convskip = bool(config.if_convskip)
        self.if_transskip = bool(config.if_transskip)
        self.time_steps = int(time_steps)

        self.img_size_full = tuple(config.img_size)                  # FULL
        self.img_size = tuple(s // 2 for s in self.img_size_full)    # HALF

        self.transformer = SwinTransformer(
            patch_size=config.patch_size,
            in_chans=config.in_chans,
            embed_dim=config.embed_dim,
            depths=config.depths,
            num_heads=config.num_heads,
            window_size=config.window_size,
            mlp_ratio=config.mlp_ratio,
            qkv_bias=config.qkv_bias,
            drop_rate=config.drop_rate,
            drop_path_rate=config.drop_path_rate,
            ape=config.ape,
            spe=config.spe,
            rpe=config.rpe,
            patch_norm=config.patch_norm,
            use_checkpoint=config.use_checkpoint,
            out_indices=config.out_indices,
            pat_merg_rf=config.pat_merg_rf,
            img_size=self.img_size,
            dwin_size=config.dwin_size,
        )

        feats = list(self.transformer.num_features)
        c0, c1, c2 = int(feats[0]), int(feats[1]), int(feats[2])

        self.c_mid = max(1, c0 // 2)

        self.cab0 = CAB(c2, compress_ratio=3, squeeze_factor=30)
        self.cab1 = CAB(c1, compress_ratio=3, squeeze_factor=30)
        self.cab2 = CAB(c0, compress_ratio=3, squeeze_factor=30)

        self.up0 = SRUpBlock3D(in_channels=c2, out_channels=c1, skip_channels=(c1 if self.if_transskip else 0))
        self.up1 = SRUpBlock3D(in_channels=c1, out_channels=c0, skip_channels=(c0 if self.if_transskip else 0))
        
        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)
        self.c1 = Conv3dReLU(2, self.c_mid, kernel_size=3, stride=1, use_batchnorm=False)
        self.up2 = SRUpBlock3D(in_channels=c0,out_channels=self.c_mid,skip_channels=(self.c_mid if self.if_convskip else 0))

        reg_ch = int(config.reg_head_chan)

        self.cs = nn.ModuleList()
        self.up3s = nn.ModuleList()
        self.reg_heads = nn.ModuleList()

        for _ in range(self.time_steps):
            self.cs.append(Conv3dReLU(2, self.c_mid, kernel_size=3, stride=1, use_batchnorm=False))
            self.up3s.append(SRUpBlock3D(in_channels=self.c_mid,out_channels=reg_ch,skip_channels=(self.c_mid if self.if_convskip else 0)))
            self.reg_heads.append(RegistrationHead(in_channels=reg_ch, out_channels=3, kernel_size=3))

        self.spatial_trans = SpatialTransformer(self.img_size)  # HALF grid

    def forward(self,
        mov_half: torch.Tensor,
        fix_half: torch.Tensor,
        *,
        init_flow_half: Optional[torch.Tensor] = None,
        return_all_flows: bool = False,
    ):
        if init_flow_half is None:
            flow_prev = torch.zeros(
                (mov_half.shape[0], 3, *self.img_size),
                device=mov_half.device,
                dtype=mov_half.dtype,
            )
            def_x = mov_half
        else:
            flow_prev = init_flow_half
            def_x = self.spatial_trans(mov_half, flow_prev)

        x_cat = torch.cat((mov_half, fix_half), dim=1)
        x_s1 = self.avg_pool(x_cat)
        f3 = self.c1(x_s1).to(mov_half.dtype) if self.if_convskip else None

        out_feats = self.transformer((mov_half, fix_half))

        if self.if_transskip:
            mov_f1, fix_f1 = out_feats[-2]
            f1 = self.cab1(mov_f1 + fix_f1)

            mov_f2, fix_f2 = out_feats[-3]
            f2 = self.cab2(mov_f2 + fix_f2)
        else:
            f1 = None
            f2 = None

        mov_f0, fix_f0 = out_feats[-1]
        f0 = self.cab0(mov_f0 + fix_f0)

        x = self.up0(f0, f1)
        x = self.up1(x, f2)
        xx = self.up2(x, f3)

        flows = [] if return_all_flows else None

        for t in range(self.time_steps):
            f_out = self.cs[t](torch.cat((def_x, fix_half), dim=1))
            x_t = self.up3s[t](xx, f_out if self.if_convskip else None)
            flow_step = self.reg_heads[t](x_t)

            if flows is not None:
                flows.append(flow_step)

            flow_new = flow_prev + self.spatial_trans(flow_step, flow_prev)
            def_x = self.spatial_trans(mov_half, flow_new)
            flow_prev = flow_new

        if return_all_flows:
            return def_x, flow_prev, flows
        return def_x, flow_prev
    

class CTCF_CascadeA(nn.Module):
    """
    Variant A:
      L1: CoarseFlowNetQuarter (1/4)
      L2: CTCF_DCA_CoreHalf    (1/2) with init_flow from L1
      L3: FlowRefiner3D        (1/2) with error-map
    Output is produced on FULL-res by upsampling final half-res flow by x2 and warping full mov.
    """
    def __init__(self, config):
        super().__init__()

        self.img_size_full: Tuple[int, int, int] = tuple(config.img_size)
        self.use_level1 = bool(config.use_level1)
        self.use_level2 = bool(config.use_level2)
        self.use_level3 = bool(config.use_level3)
        self.level1 = CoarseFlowNetQuarter(base_ch=config.level1_base_ch) if self.use_level1 else None
        self.level2 = CTCF_DCA_CoreHalf(config, time_steps=config.time_steps) if self.use_level2 else None
        self.level3 = FlowRefiner3D(base_ch=config.level3_base_ch) if self.use_level3 else None

        self.st_full = SpatialTransformer(self.img_size_full)

    def forward(
        self,
        mov_full: torch.Tensor,
        fix_full: torch.Tensor,
        *,
        return_all: bool = False,
        alpha_l1: float = 1.0,
        alpha_l3: float = 1.0,
    ):
        mov_half = nn.functional.interpolate(mov_full, scale_factor=0.5, mode="trilinear", align_corners=False)
        fix_half = nn.functional.interpolate(fix_full, scale_factor=0.5, mode="trilinear", align_corners=False)

        aux = {} if return_all else None
        flow_half_init = None

        if self.level1 is not None and alpha_l1 > 0.0:
            mov_quarter = nn.functional.interpolate(mov_full, scale_factor=0.25, mode="trilinear", align_corners=False)
            fix_quarter = nn.functional.interpolate(fix_full, scale_factor=0.25, mode="trilinear", align_corners=False)
            flow_quarter = self.level1(mov_quarter, fix_quarter)
            flow_half_init = upsample_flow(flow_quarter, scale_factor=2) * float(alpha_l1)

            if aux is not None:
                aux["flow_quarter"] = flow_quarter
                aux["flow_half_init"] = flow_half_init

        if self.level2 is None:
            raise RuntimeError("CTCF_CascadeA requires level2 enabled (use_level2=True).")

        out_l2 = self.level2(
            mov_half,
            fix_half,
            init_flow_half=flow_half_init,
            return_all_flows=return_all,
        )

        if return_all:
            def_half_l2, flow_half_l2, flows_l2 = out_l2
            if aux is not None:
                aux["flows_l2"] = flows_l2
                aux["flow_half_l2"] = flow_half_l2
        else:
            def_half_l2, flow_half_l2 = out_l2

        if self.level3 is not None and alpha_l3 > 0.0:
            flow_half_ref = self.level3(def_half_l2, fix_half, flow_half_l2) * float(alpha_l3)
            flow_half = flow_half_l2 + flow_half_ref
            if aux is not None:
                aux["flow_half_ref"] = flow_half_ref
        else:
            flow_half = flow_half_l2

        flow_full = upsample_flow(flow_half, scale_factor=2)
        def_full = self.st_full(mov_full, flow_full)

        if aux is not None:
            aux["flow_half_final"] = flow_half
            aux["flow_full"] = flow_full

        if return_all:
            return def_full, flow_full, aux
        return def_full, flow_full
################################################################################
# FILE: models/CTCF/ut_blocks.py
# SIZE: 4513 bytes
################################################################################

# models/CTCF/ut_blocks.py

from __future__ import annotations

from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F


class CA(nn.Module):
    """Channel Attention (RCAN-style) for 3D tensors [B,C,D,H,W]."""
    def __init__(self, num_feat: int, squeeze_factor: int = 16):
        super().__init__()
        hidden = max(1, num_feat // squeeze_factor)
        self.net = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Conv3d(num_feat, hidden, kernel_size=1, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv3d(hidden, num_feat, kernel_size=1, bias=True),
            nn.Sigmoid(),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x * self.net(x)


class CAB(nn.Module):
    """
    Convolutional Attention Block (UTSRMorph idea):
    Conv3d -> GELU -> Conv3d -> CA
    """
    def __init__(self, num_feat: int, compress_ratio: int = 3, squeeze_factor: int = 30):
        super().__init__()
        hidden = max(1, num_feat // compress_ratio)
        self.body = nn.Sequential(
            nn.Conv3d(num_feat, hidden, kernel_size=3, padding=1, bias=True),
            nn.GELU(),
            nn.Conv3d(hidden, num_feat, kernel_size=3, padding=1, bias=True),
            CA(num_feat, squeeze_factor=squeeze_factor),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.body(x)


class Conv3dAct(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, k: int = 3, act: str = "gelu"):
        super().__init__()
        self.conv = nn.Conv3d(in_ch, out_ch, kernel_size=k, padding=k // 2, bias=True)
        if act == "gelu":
            self.act = nn.GELU()
        elif act == "lrelu":
            self.act = nn.LeakyReLU(0.1, inplace=True)
        else:
            raise ValueError(f"Unknown act: {act}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.act(self.conv(x))


def _match_size_3d(x: torch.Tensor, ref: torch.Tensor) -> torch.Tensor:
    """
    Make x match ref spatially using symmetric pad/crop.
    x, ref: [B, C, D, H, W]
    """
    if x.dim() != 5 or ref.dim() != 5:
        raise ValueError(f"_match_size_3d expects 5D tensors, got x={x.shape}, ref={ref.shape}")

    xd, xh, xw = x.shape[-3:]
    rd, rh, rw = ref.shape[-3:]

    # Pad if x is smaller
    pd = max(0, rd - xd)
    ph = max(0, rh - xh)
    pw = max(0, rw - xw)
    if pd or ph or pw:
        # F.pad order: (W_left, W_right, H_left, H_right, D_left, D_right)
        pad = (pw // 2, pw - pw // 2,
               ph // 2, ph - ph // 2,
               pd // 2, pd - pd // 2)
        x = F.pad(x, pad)

    # Crop if x is larger
    xd, xh, xw = x.shape[-3:]
    sd = (xd - rd) // 2 if xd > rd else 0
    sh = (xh - rh) // 2 if xh > rh else 0
    sw = (xw - rw) // 2 if xw > rw else 0

    x = x[..., sd:sd + rd, sh:sh + rh, sw:sw + rw]
    return x


class SRUpBlock3D(nn.Module):
    """
    A safe SR-style x2 upsampling block with optional skip.
    - Upsample (trilinear) x2
    - Match size to skip (pad/crop) to avoid off-by-one errors
    - Concat skip if provided
    - Conv -> Conv
    """
    def __init__(self, in_channels: int, out_channels: int, skip_channels: int = 0):
        super().__init__()
        self.skip_channels = int(skip_channels)
        self.up = nn.Upsample(scale_factor=2, mode="trilinear", align_corners=False)

        self.conv1 = Conv3dAct(in_channels + self.skip_channels, out_channels, k=3, act="gelu")
        self.conv2 = Conv3dAct(out_channels, out_channels, k=3, act="gelu")

    def forward(self, x: torch.Tensor, skip: Optional[torch.Tensor] = None) -> torch.Tensor:
        x = self.up(x)
        if self.skip_channels > 0:
            if skip is None:
                raise ValueError("SRUpBlock3D expects skip tensor but got None.")
            x = _match_size_3d(x, skip)
            x = torch.cat([x, skip], dim=1)
        return self.conv2(self.conv1(x))


def upsample_flow(flow: torch.Tensor, scale_factor: float = 2.0) -> torch.Tensor:
    """
    Upsample displacement field with proper magnitude scaling.
    flow: [B,3,D,H,W]
    """
    if scale_factor == 1:
        return flow

    flow_up = F.interpolate(
        flow,
        scale_factor=scale_factor,
        mode="trilinear",
        align_corners=False,
    )
    return flow_up * float(scale_factor)
################################################################################
# FILE: models/TransMorph_DCA/__init__.py
# SIZE: 0 bytes
################################################################################


################################################################################
# FILE: models/TransMorph_DCA/configs.py
# SIZE: 2803 bytes
################################################################################

import ml_collections

'''
********************************************************
             Deformable Swin Transformer
********************************************************
if_transskip (bool): Enable skip connections from Transformer Blocks
if_convskip (bool): Enable skip connections from Convolutional Blocks
patch_size (int | tuple(int)): Patch size. Default: 4
in_chans (int): Number of input image channels. Default: 2 (for moving and fixed images)
embed_dim (int): Patch embedding dimension. Default: 96
dwin_kernel_size (tuple(int)): Deformable window size of each Swin Transformer layer. Default: (7, 5, 3)
depths (tuple(int)): Depth of each Swin Transformer layer.
num_heads (tuple(int)): Number of attention heads in different layers.
window_size (tuple(int)): Image size should be divisible by window size, 
                     e.g., if image has a size of (160, 192, 224), then the window size can be (5, 6, 7)
mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
pat_merg_rf (int): Embed_dim reduction factor in patch merging, e.g., N*C->N/4*C if set to four. Default: 4. 
qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
drop_rate (float): Dropout rate. Default: 0
drop_path_rate (float): Stochastic depth rate. Default: 0.1
ape (bool): Enable learnable position embedding. Default: False
spe (bool): Enable sinusoidal position embedding. Default: False
patch_norm (bool): If True, add normalization after patch embedding. Default: True
use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False 
                       (Carried over from Swin Transformer, it is not needed)
out_indices (tuple(int)): Indices of Transformer blocks to output features. Default: (0, 1, 2, 3)
reg_head_chan (int): Number of channels in the registration head (i.e., the final convolutional layer) 
img_size (int | tuple(int)): Input image size, e.g., (160, 192, 224)
'''

def get_3DTransMorphDWin3Lvl_config():
    '''
    Trainable params: 15,201,579
    '''
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 96
    config.dwin_kernel_size = (7, 5, 3)
    config.depths = (4, 4, 5)
    config.num_heads = (8, 8, 8)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2)
    config.reg_head_chan = 16
    config.img_size = (160, 192, 224)
    return config
################################################################################
# FILE: models/TransMorph_DCA/model.py
# SIZE: 69552 bytes
################################################################################

'''
TransMorph-DCA model
Chen, J., Liu, Y., He, Y., & Du, Y. (2023).
Deformable Cross-Attention Transformer for Medical Image Registration.
TransMorph: Transformer for unsupervised medical image registration.
In Machine Learning in Medical Imaging: 14th International Workshop, MLMI 2023,
Held in Conjunction with MICCAI 2023.

Created by:
Junyu Chen
jchen245@jhmi.edu
Johns Hopkins University
'''

import torch
import torch.nn as nn
import torch.utils.checkpoint as checkpoint
from timm.layers import DropPath, trunc_normal_, to_3tuple
from torch.distributions.normal import Normal
import torch.nn.functional as nnf
import numpy as np
import models.TransMorph_DCA.configs as configs
import einops

class LayerNormProxy(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        x = einops.rearrange(x, 'b c h w d -> b h w d c')
        x = self.norm(x)
        return einops.rearrange(x, 'b h w d c -> b c h w d')

class Offset_block0(nn.Module):
    def __init__(self, in_channels, num_heads, kernel_size=3):
        super().__init__()
        self.conv3d = nn.Conv3d(in_channels, num_heads, kernel_size=kernel_size, padding=kernel_size // 2, groups=num_heads, bias=False)
        self.LN = LayerNormProxy(num_heads)
        self.act = nn.GELU()
        self.offsetx = nn.Conv3d(num_heads, num_heads, kernel_size=1, bias=False)
        self.offsety = nn.Conv3d(num_heads, num_heads, kernel_size=1, bias=False)
        self.offsetz = nn.Conv3d(num_heads, num_heads, kernel_size=1, bias=False)
    def forward(self, x):
        x = self.conv3d(x)
        x = self.LN(x)
        x = self.act(x)
        dx = self.offsetx(x).unsqueeze(2)
        dy = self.offsety(x).unsqueeze(2)
        dz = self.offsetz(x).unsqueeze(2)
        x = torch.cat((dx, dy, dz), dim=2)
        return x

class Offset_block(nn.Module):
    def __init__(self, in_channels, num_heads, kernel_size=3):
        super().__init__()
        self.conv3d_1 = nn.Conv3d(in_channels, in_channels//2, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)
        self.conv3d_1.weight = nn.Parameter(Normal(0, 1e-5).sample(self.conv3d_1.weight.shape))

        self.conv3d_2 = nn.Conv3d(in_channels//2, in_channels//4, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)
        self.conv3d_2.weight = nn.Parameter(Normal(0, 1e-5).sample(self.conv3d_2.weight.shape))

        self.conv3d_3 = nn.Conv3d(in_channels // 4, 3*num_heads, kernel_size=1, bias=False)
        self.conv3d_3.weight = nn.Parameter(Normal(0, 1e-5).sample(self.conv3d_3.weight.shape))

        self.bn_1 = nn.BatchNorm3d(in_channels // 2)
        self.bn_2 = nn.BatchNorm3d(in_channels // 4)
        self.relu_1 = nn.ReLU(inplace=True)
        self.relu_2 = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv3d_1(x)
        x = self.bn_1(x)
        x = self.relu_1(x)
        x = self.conv3d_2(x)
        x = self.bn_2(x)
        x = self.relu_2(x)
        x = self.conv3d_3(x)
        return x

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class XMlp(nn.Module):
    def __init__(self, in_size, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, reduction=4, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

        kernel_size = 3
        self.avg_pool_1 = nn.AdaptiveAvgPool1d(1)
        self.avg_pool_2 = nn.AdaptiveAvgPool1d(1)
        self.se_conv_1 = nn.Sequential(
            nn.Conv3d(in_features*2, in_features//4, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_features//4, in_features//16, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_features//16, 1, kernel_size=1, padding=0, bias=False),
            nn.Sigmoid()
        )
        self.se_fc_2 = nn.Sequential(
            nn.Linear(in_features*2, in_features // reduction, bias=False),
            act_layer(),
            nn.Linear(in_features // reduction, in_features, bias=False),
            nn.Sigmoid()
        )
        self.H, self.W, self.T = in_size

    def forward(self, x, y):
        N, D, C = y.shape
        y_conv = torch.reshape(y.permute(0, 2, 1), (N, C, self.H, self.W, self.T))
        x_conv = torch.reshape(x.permute(0, 2, 1), (N, C, self.H, self.W, self.T))
        y_se = self.se_conv_1(torch.cat((x_conv, y_conv), dim=1)).view(N, 1, D).permute(0, 2, 1)
        x = x * y_se
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        x_se = self.avg_pool_2(torch.cat((x, y), dim=2).permute(0, 2, 1)).view(N, 2*C)
        x_se = self.se_fc_2(x_se).view(N, 1, C)
        x = x * x_se
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, L, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, window_size, C)
    """
    B, H, W, L, C = x.shape
    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], L // window_size[2], window_size[2], C)

    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0], window_size[1], window_size[2], C)
    return windows

def deform_window_partition(x, window_size):
    """
    Args:
        x: (Head_size, B, H, W, L, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, window_size, C)
    """
    B, Head, H, W, L, C = x.shape
    x = x.view(B, Head, H // window_size[0], window_size[0], W // window_size[1], window_size[1], L // window_size[2], window_size[2], C)

    windows = x.permute(0, 1, 2, 4, 6, 3, 5, 7, 8).contiguous().view(B, Head, -1, window_size[0], window_size[1], window_size[2], C)
    return windows

def window_reverse(windows, window_size, H, W, L):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
        L (int): Length of image
    Returns:
        x: (B, H, W, L, C)
    """
    B = int(windows.shape[0] / (H * W * L / window_size[0] / window_size[1] / window_size[2]))
    x = windows.view(B, H // window_size[0], W // window_size[1], L // window_size[2], window_size[0], window_size[1], window_size[2], -1)
    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, H, W, L, -1)#
    return x

class WindowAttentionReverse(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, rpe=True, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1 * 2*Wt-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords_t = torch.arange(self.window_size[2])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing="ij"))  # 3, Wh, Ww, Wt
        coords_flatten = torch.flatten(coords, 1)  # 3, Wh*Ww*Wt
        self.rpe = rpe
        if self.rpe:
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wh*Ww*Wt, Wh*Ww*Wt
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww*Wt, Wh*Ww*Wt, 3
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 2] += self.window_size[2] - 1
            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww*Wt, Wh*Ww*Wt
            self.register_buffer("relative_position_index", relative_position_index)

        self.f_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.f_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.m_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.m_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.m_proj = nn.Linear(dim, dim)
        self.f_proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """ Forward function.
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww, Wt*Ww) or None
        """
        mov, fix, dmov, dfix = x
        B_, N, C = mov.shape
        dmov_kv = self.m_kv(dmov).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dfix_kv = self.f_kv(dfix).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        mov_q = self.m_q(mov).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        fix_q = self.f_q(fix).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        mov_Q, dmov_K, dmov_V = mov_q[0], dmov_kv[0], dmov_kv[1]  # make torchscript happy (cannot use tensor as tuple)
        fix_Q, dfix_K, dfix_V = fix_q[0], dfix_kv[0], dfix_kv[1]  # make torchscript happy (cannot use tensor as tuple)

        mov_Q = mov_Q * self.scale
        fix_Q = fix_Q * self.scale
        mov_attn = (fix_Q @ dmov_K.transpose(-2, -1))
        fix_attn = (mov_Q @ dfix_K.transpose(-2, -1))

        if self.rpe:
            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1] * self.window_size[2],
                self.window_size[0] * self.window_size[1] * self.window_size[2], -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
            mov_attn = mov_attn + relative_position_bias.unsqueeze(0)
            fix_attn = fix_attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            mov_attn = mov_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            mov_attn = mov_attn.view(-1, self.num_heads, N, N)
            mov_attn = self.softmax(mov_attn)
            fix_attn = fix_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            fix_attn = fix_attn.view(-1, self.num_heads, N, N)
            fix_attn = self.softmax(fix_attn)
        else:
            mov_attn = self.softmax(mov_attn)
            fix_attn = self.softmax(fix_attn)

        mov_attn = self.attn_drop(mov_attn)
        fix_attn = self.attn_drop(fix_attn)

        mov = (mov_attn @ dmov_V).transpose(1, 2).reshape(B_, N, C)
        mov = self.m_proj(mov)
        mov = self.proj_drop(mov)

        fix = (fix_attn @ dfix_V).transpose(1, 2).reshape(B_, N, C)
        fix = self.f_proj(fix)
        fix = self.proj_drop(fix)

        return mov, fix

class WindowAttention(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, rpe=True, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1 * 2*Wt-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords_t = torch.arange(self.window_size[2])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing="ij"))  # 3, Wh, Ww, Wt
        coords_flatten = torch.flatten(coords, 1)  # 3, Wh*Ww*Wt
        self.rpe = rpe
        if self.rpe:
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wh*Ww*Wt, Wh*Ww*Wt
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww*Wt, Wh*Ww*Wt, 3
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 2] += self.window_size[2] - 1
            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww*Wt, Wh*Ww*Wt
            self.register_buffer("relative_position_index", relative_position_index)

        self.m_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.m_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.f_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.f_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.m_proj = nn.Linear(dim, dim)
        self.f_proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, mov, fix, dmov, dfix, mask=None):
        """ Forward function.
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww, Wt*Ww) or None
        """
        B_, N, C = mov.shape
        mov_kv = self.m_kv(mov).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        fix_kv = self.f_kv(fix).reshape(B_, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dmov_q = self.m_q(dmov).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dfix_q = self.f_q(dfix).reshape(B_, N, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        dmov_Q, mov_K, mov_V = dmov_q[0], mov_kv[0], mov_kv[1]  # make torchscript happy (cannot use tensor as tuple)
        dfix_Q, fix_K, fix_V = dfix_q[0], fix_kv[0], fix_kv[1]  # make torchscript happy (cannot use tensor as tuple)

        dmov_Q = dmov_Q * self.scale
        dfix_Q = dfix_Q * self.scale
        mov_attn = (dfix_Q @ mov_K.transpose(-2, -1))
        fix_attn = (dmov_Q @ fix_K.transpose(-2, -1))

        if self.rpe:
            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1] * self.window_size[2],
                self.window_size[0] * self.window_size[1] * self.window_size[2], -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
            mov_attn = mov_attn + relative_position_bias.unsqueeze(0)
            fix_attn = fix_attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            mov_attn = mov_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            mov_attn = mov_attn.view(-1, self.num_heads, N, N)
            mov_attn = self.softmax(mov_attn)
            fix_attn = fix_attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            fix_attn = fix_attn.view(-1, self.num_heads, N, N)
            fix_attn = self.softmax(fix_attn)
        else:
            mov_attn = self.softmax(mov_attn)
            fix_attn = self.softmax(fix_attn)

        mov_attn = self.attn_drop(mov_attn)
        fix_attn = self.attn_drop(fix_attn)

        mov = (mov_attn @ mov_V).transpose(1, 2).reshape(B_, N, C)
        mov = self.m_proj(mov)
        mov = self.proj_drop(mov)

        fix = (fix_attn @ fix_V).transpose(1, 2).reshape(B_, N, C)
        fix = self.f_proj(fix)
        fix = self.proj_drop(fix)

        return mov, fix

class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, num_heads, window_size=(7, 7, 7), shift_size=(0, 0, 0),
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, rpe=True, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, img_size=(160, 160, 160), dwin_size=3):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        assert 0 <= min(self.shift_size) < min(self.window_size), "shift_size must in 0-window_size, shift_sz: {}, win_size: {}".format(self.shift_size, self.window_size)

        self.m_norm1 = norm_layer(dim)
        self.f_norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=self.window_size, num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, rpe=rpe, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.m_norm2 = norm_layer(dim)
        self.f_norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.m_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop) #XMlp(in_size=img_size,
        self.f_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)#XMlp(in_size=img_size,

        vectors = [torch.arange(0, s) for s in img_size]

        grids = torch.meshgrid(vectors, indexing="ij")
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)
        #for i in range(len(img_size)):
        #    grid[:, i, ...] = 2 * (grid[:, i, ...] / (img_size[i] - 1) - 0.5)
        sample_grid1 = grid.cuda()
        sample_grid1.requires_grad = False
        sample_grid1 = sample_grid1.permute(0, 2, 3, 4, 1)
        sample_grid1 = sample_grid1[..., [2, 1, 0]]
        self.grid = window_partition(sample_grid1, self.window_size)
        self.grid.requires_grad = False
        nW, wW, wH, wD, gC = self.grid.shape
        self.nW = nW
        self.wW = wW
        self.wH = wH
        self.wD = wD
        self.grid = self.grid.view(1, nW, -1, 3)
        self.grid = torch.unsqueeze(self.grid, 0)

        self.offset_block_1 = Offset_block(self.dim*2, num_heads, dwin_size)
        self.offset_block_2 = Offset_block(self.dim*2, num_heads, dwin_size)

        self.H = None
        self.W = None
        self.T = None


    def forward(self, mov, fix, mask_matrix):
        H, W, T = self.H, self.W, self.T
        B, L, C = mov.shape
        assert L == H * W * T, "input feature has wrong size"

        mov_shortcut = mov
        fix_shortcut = fix

        mov = self.m_norm1(mov)
        mov = mov.view(B, H, W, T, C)

        fix = self.f_norm1(fix)
        fix = fix.view(B, H, W, T, C)

        # pad feature maps to multiples of window size
        pad_l = pad_t = pad_f = 0
        pad_r = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]
        pad_b = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]
        pad_h = (self.window_size[2] - T % self.window_size[2]) % self.window_size[2]
        mov = nnf.pad(mov, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = mov.shape
        fix = nnf.pad(fix, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = fix.shape

        # cyclic shift
        if min(self.shift_size) > 0:
            shifted_mov = torch.roll(mov, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))
            shifted_fix = torch.roll(fix, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))
            attn_mask = mask_matrix
        else:
            shifted_mov = mov
            shifted_fix = fix
            attn_mask = None

        offset_range = torch.tensor([Hp, Wp, Tp], device='cuda').reshape(1, 1, 3, 1, 1, 1) * 0.5
        off_mov = shifted_mov.permute(0, 4, 1, 2, 3)
        off_fix = shifted_fix.permute(0, 4, 1, 2, 3)
        offset_mov_org = self.offset_block_1(torch.cat((off_mov, off_fix), dim=1))
        offset_mov_org = offset_mov_org.tanh().reshape(B, self.num_heads, 3, Hp, Wp, Tp).mul(offset_range)
        offset_mov = deform_window_partition(offset_mov_org.permute(0, 1, 3, 4, 5, 2), self.window_size)
        offset_mov = torch.reshape(offset_mov, (B, self.num_heads, self.nW, self.wW * self.wH * self.wD, 3))
        offset_mov = offset_mov[..., [2, 1, 0]]
        offset_mov = torch.unsqueeze(offset_mov, 2)

        offset_fix_org = self.offset_block_2(torch.cat((off_fix, off_mov), dim=1))
        offset_fix_org = offset_fix_org.tanh().reshape(B, self.num_heads, 3, Hp, Wp, Tp).mul(offset_range)
        offset_fix = deform_window_partition(offset_fix_org.permute(0, 1, 3, 4, 5, 2), self.window_size)
        offset_fix = torch.reshape(offset_fix, (B, self.num_heads, self.nW, self.wW * self.wH * self.wD, 3))
        offset_fix = offset_fix[..., [2, 1, 0]]
        offset_fix = torch.unsqueeze(offset_fix, 2)

        dmov = torch.clone(shifted_mov)
        dfix = torch.clone(shifted_fix)

        # deformable window for moving image features
        offset_mov_ = offset_mov.repeat(1, 1, C // self.num_heads, 1, 1, 1).view(B, C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_mov_ = offset_mov_.view(B * C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_mov_ = torch.unsqueeze(offset_mov_, 1)
        offset_fix_ = offset_fix.repeat(1, 1, C // self.num_heads, 1, 1, 1).view(B, C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_fix_ = offset_fix_.view(B * C, self.nW, self.wW * self.wH * self.wD, 3)
        offset_fix_ = torch.unsqueeze(offset_fix_, 1)
        mov_locs = self.grid.repeat(B*C, 1, 1, 1, 1) + offset_mov_
        fix_locs = self.grid.repeat(B*C, 1, 1, 1, 1) + offset_fix_
        # need to normalize grid values to [-1, 1] for resampler
        shape = [Tp, Hp, Wp]
        for i in range(len(shape)):
            mov_locs[..., i] = 2 * (mov_locs[..., i] / (shape[i] - 1) - 0.5)
            fix_locs[..., i] = 2 * (fix_locs[..., i] / (shape[i] - 1) - 0.5)

        dmov = dmov.permute(0, 4, 1, 2, 3)
        dmov = torch.reshape(dmov, (B * C, 1, Hp, Wp, Tp))
        dmov_windows = nnf.grid_sample(dmov, mov_locs, align_corners=False)
        dmov_windows = torch.reshape(dmov_windows, (B, C, self.nW, self.window_size[0], self.window_size[1], self.window_size[2])).permute(0, 2, 3, 4, 5, 1)
        dmov_windows = torch.reshape(dmov_windows, (-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C))

        # deformable window for moving image features
        dfix = dfix.permute(0, 4, 1, 2, 3)
        dfix = torch.reshape(dfix, (B * C, 1, Hp, Wp, Tp))
        dfix_windows = nnf.grid_sample(dfix, fix_locs, align_corners=False)
        dfix_windows = torch.reshape(dfix_windows, (B, C, self.nW, self.window_size[0], self.window_size[1], self.window_size[2])).permute(0, 2, 3, 4, 5, 1)
        dfix_windows = torch.reshape(dfix_windows, (-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C))

        # partition windows
        mov_windows = window_partition(shifted_mov, self.window_size)  # nW*B, window_size, window_size, C
        mov_windows = mov_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C)  # nW*B, window_size**3, C

        fix_windows = window_partition(shifted_fix, self.window_size)  # nW*B, window_size, window_size, C
        fix_windows = fix_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C)  # nW*B, window_size**3, C

        # Deformable Cross W-MSA/SW-MSA
        mov_attn_windows, fix_attn_windows = self.attn(mov_windows, fix_windows, dmov_windows, dfix_windows, mask=attn_mask)  # nW*B, window_size**3, C

        # merge windows
        mov_attn_windows = mov_attn_windows.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], C)
        shifted_mov = window_reverse(mov_attn_windows, self.window_size, Hp, Wp, Tp)  # B H' W' L' C

        fix_attn_windows = fix_attn_windows.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], C)
        shifted_fix = window_reverse(fix_attn_windows, self.window_size, Hp, Wp, Tp)  # B H' W' L' C

        # reverse cyclic shift
        if min(self.shift_size) > 0:
            mov = torch.roll(shifted_mov, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))
            fix = torch.roll(shifted_fix, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))
        else:
            mov, fix = shifted_mov, shifted_fix

        if pad_r > 0 or pad_b > 0:
            mov = mov[:, :H, :W, :T, :].contiguous()
            fix = fix[:, :H, :W, :T, :].contiguous()

        mov = mov.view(B, H * W * T, C)
        fix = fix.view(B, H * W * T, C)

        # FFN
        mov = mov_shortcut + self.drop_path(mov)
        fix = fix_shortcut + self.drop_path(fix)
        mov_norm = self.m_norm2(mov)
        fix_norm = self.f_norm2(fix)
        mov = mov + self.drop_path(self.m_mlp(mov_norm))#, fix_norm))
        fix = fix + self.drop_path(self.f_mlp(fix_norm))#, mov_norm))
        return mov, fix

class PatchMerging(nn.Module):
    r""" Patch Merging Layer.
    Args:
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, norm_layer=nn.LayerNorm, reduce_factor=2):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(8 * dim, (8//reduce_factor) * dim, bias=False)
        self.norm = norm_layer(8 * dim)


    def forward(self, x, H, W, T):
        """
        x: B, H*W*T, C
        """
        B, L, C = x.shape
        assert L == H * W * T, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0 and T % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, T, C)

        # padding
        pad_input = (H % 2 == 1) or (W % 2 == 1) or (T % 2 == 1)
        if pad_input:
            x = nnf.pad(x, (0, 0, 0, T % 2, 0, W % 2, 0, H % 2))

        x0 = x[:, 0::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x1 = x[:, 1::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x2 = x[:, 0::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x3 = x[:, 0::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x4 = x[:, 1::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x5 = x[:, 0::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x6 = x[:, 1::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x7 = x[:, 1::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)  # B H/2 W/2 T/2 8*C
        x = x.view(B, -1, 8 * C)  # B H/2*W/2*T/2 8*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.
    Args:
        dim (int): Number of feature channels
        depth (int): Depths of this stage.
        num_heads (int): Number of attention head.
        window_size (int): Local window size. Default: 7.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self,
                 dim,
                 depth,
                 num_heads,
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 rpe=True,
                 drop=0.,
                 attn_drop=0.,
                 drop_path=0.,
                 norm_layer=nn.LayerNorm,
                 downsample=None,
                 use_checkpoint=False,
                 pat_merg_rf=2,
                 img_size=(160, 160, 160),
                 dwin_size=3):
        super().__init__()
        self.window_size = window_size
        self.shift_size = (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2)
        self.depth = depth
        self.use_checkpoint = use_checkpoint
        self.pat_merg_rf = pat_merg_rf
        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=(0, 0, 0) if (i % 2 == 0) else (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2),
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                rpe=rpe,
                drop=drop,
                attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                norm_layer=norm_layer,
                img_size=img_size,
                dwin_size=dwin_size)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(dim=dim, norm_layer=norm_layer, reduce_factor=self.pat_merg_rf)
        else:
            self.downsample = None

    def forward(self, x, H, W, T):
        """ Forward function.
        Args:
            x: Input feature, tensor size (B, H*W*T, C).
            H, W, T: Spatial resolution of the input feature.
        """
        mov, fix = x
        # calculate attention mask for SW-MSA
        Hp = int(np.ceil(H / self.window_size[0])) * self.window_size[0]
        Wp = int(np.ceil(W / self.window_size[1])) * self.window_size[1]
        Tp = int(np.ceil(T / self.window_size[2])) * self.window_size[2]
        img_mask = torch.zeros((1, Hp, Wp, Tp, 1), device=mov.device)  # 1 Hp Wp 1
        h_slices = (slice(0, -self.window_size[0]),
                    slice(-self.window_size[0], -self.shift_size[0]),
                    slice(-self.shift_size[0], None))
        w_slices = (slice(0, -self.window_size[1]),
                    slice(-self.window_size[1], -self.shift_size[1]),
                    slice(-self.shift_size[1], None))
        t_slices = (slice(0, -self.window_size[2]),
                    slice(-self.window_size[2], -self.shift_size[2]),
                    slice(-self.shift_size[2], None))
        cnt = 0
        for h in h_slices:
            for w in w_slices:
                for t in t_slices:
                    img_mask[:, h, w, t, :] = cnt
                    cnt += 1

        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
        mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2])
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

        x_m, x_f = mov, fix
        for blk in self.blocks:
            blk.H, blk.W, blk.T = H, W, T
            if self.use_checkpoint:
                x_m, x_f = checkpoint.checkpoint(blk, x_m, x_f, attn_mask)
            else:
                x_m, x_f = blk(x_m, x_f, attn_mask)
        mov, fix = x_m, x_f
        if self.downsample is not None:
            mov_down = self.downsample(mov, H, W, T)
            fix_down = self.downsample(fix, H, W, T)
            Wh, Ww, Wt = (H + 1) // 2, (W + 1) // 2, (T + 1) // 2
            return (mov, fix), H, W, T, (mov_down, fix_down), Wh, Ww, Wt
        else:
            return (mov, fix), H, W, T, (mov, fix), H, W, T

class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    Args:
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        patch_size = to_3tuple(patch_size)
        self.patch_size = patch_size

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        """Forward function."""
        # padding
        _, _, H, W, T = x.size()
        if T % self.patch_size[2] != 0:
            x = nnf.pad(x, (0, self.patch_size[2] - T % self.patch_size[2]))
        if W % self.patch_size[1] != 0:
            x = nnf.pad(x, (0, 0, 0, self.patch_size[1] - W % self.patch_size[1]))
        if H % self.patch_size[0] != 0:
            x = nnf.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))

        x = self.proj(x)  # B C Wh Ww Wt
        if self.norm is not None:
            Wh, Ww, Wt = x.size(2), x.size(3), x.size(4)
            x = x.flatten(2).transpose(1, 2)
            x = self.norm(x)
            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww, Wt)

        return x

class SinusoidalPositionEmbedding(nn.Module):
    '''
    Rotary Position Embedding
    '''
    def __init__(self,):
        super(SinusoidalPositionEmbedding, self).__init__()

    def forward(self, x):
        batch_sz, n_patches, hidden = x.shape
        position_ids = torch.arange(0, n_patches).float().cuda()
        indices = torch.arange(0, hidden//2).float().cuda()
        indices = torch.pow(10000.0, -2 * indices / hidden)
        embeddings = torch.einsum('b,d->bd', position_ids, indices)
        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)
        embeddings = torch.reshape(embeddings, (1, n_patches, hidden))
        return embeddings

class SinPositionalEncoding3D(nn.Module):
    def __init__(self, channels):
        """
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        """
        super(SinPositionalEncoding3D, self).__init__()
        channels = int(np.ceil(channels/6)*2)
        if channels % 2:
            channels += 1
        self.channels = channels
        self.inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))
        #self.register_buffer('inv_freq', inv_freq)

    def forward(self, tensor):
        """
        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)
        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)
        """
        tensor = tensor.permute(0, 2, 3, 4, 1)
        if len(tensor.shape) != 5:
            raise RuntimeError("The input tensor has to be 5d!")
        batch_size, x, y, z, orig_ch = tensor.shape
        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())
        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())
        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())
        sin_inp_x = torch.einsum("i,j->ij", pos_x, self.inv_freq)
        sin_inp_y = torch.einsum("i,j->ij", pos_y, self.inv_freq)
        sin_inp_z = torch.einsum("i,j->ij", pos_z, self.inv_freq)
        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1).unsqueeze(1)
        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1).unsqueeze(1)
        emb_z = torch.cat((sin_inp_z.sin(), sin_inp_z.cos()), dim=-1)
        emb = torch.zeros((x,y,z,self.channels*3),device=tensor.device).type(tensor.type())
        emb[:,:,:,:self.channels] = emb_x
        emb[:,:,:,self.channels:2*self.channels] = emb_y
        emb[:,:,:,2*self.channels:] = emb_z
        emb = emb[None,:,:,:,:orig_ch].repeat(batch_size, 1, 1, 1, 1)
        return emb.permute(0, 4, 1, 2, 3)

class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030
    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (tuple): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(self, pretrain_img_size=224,
                 patch_size=4,
                 in_chans=3,
                 embed_dim=96,
                 depths=[2, 2, 6, 2],
                 num_heads=[3, 6, 12, 24],
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.2,
                 norm_layer=nn.LayerNorm,
                 ape=False,
                 spe=False,
                 rpe=True,
                 patch_norm=True,
                 out_indices=(0, 1, 2, 3),
                 frozen_stages=-1,
                 use_checkpoint=False,
                 pat_merg_rf=2,
                 img_size=(160, 160, 160),
                 dwin_size=(3, 3, 3)):
        super().__init__()
        self.pretrain_img_size = pretrain_img_size
        self.num_layers = len(depths)
        print('Depths: {}'.format(depths))
        print('DWin kernel size: {}'.format(dwin_size))
        self.embed_dim = embed_dim
        self.ape = ape
        self.spe = spe
        self.rpe = rpe
        self.patch_norm = patch_norm
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            patch_size=patch_size, in_chans=1, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)

        # absolute position embedding
        if self.ape:
            pretrain_img_size = to_3tuple(self.pretrain_img_size)
            patch_size = to_3tuple(patch_size)
            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1], pretrain_img_size[2] // patch_size[2]]

            self.absolute_pos_embed = nn.Parameter(
                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1], patches_resolution[2]))
            trunc_normal_(self.absolute_pos_embed, std=.02)
        elif self.spe:
            self.pos_embd = SinPositionalEncoding3D(embed_dim).cuda()
            #self.pos_embd = SinusoidalPositionEmbedding().cuda()
        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                                depth=depths[i_layer],
                                num_heads=num_heads[i_layer],
                                window_size=window_size,
                                mlp_ratio=mlp_ratio,
                                qkv_bias=qkv_bias,
                                rpe=rpe,
                                qk_scale=qk_scale,
                                drop=drop_rate,
                                attn_drop=attn_drop_rate,
                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                                norm_layer=norm_layer,
                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                                use_checkpoint=use_checkpoint,
                               pat_merg_rf=pat_merg_rf,
                               img_size=(img_size[0]//4//2**i_layer, img_size[1]//4//2**i_layer, img_size[2]//4//2**i_layer),
                               dwin_size=dwin_size[i_layer])
            self.layers.append(layer)

        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
        self.num_features = num_features

        # add a norm layer for each output
        for i_layer in out_indices:
            layer = norm_layer(num_features[i_layer])
            layer_name = f'm_norm{i_layer}'
            self.add_module(layer_name, layer)
            layer = norm_layer(num_features[i_layer])
            layer_name = f'f_norm{i_layer}'
            self.add_module(layer_name, layer)

        self._freeze_stages()

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False

        if self.frozen_stages >= 1 and self.ape:
            self.absolute_pos_embed.requires_grad = False

        if self.frozen_stages >= 2:
            self.pos_drop.eval()
            for i in range(0, self.frozen_stages - 1):
                m = self.layers[i]
                m.eval()
                for param in m.parameters():
                    param.requires_grad = False

    def init_weights(self, pretrained=None):
        """Initialize the weights in backbone.
        Args:
            pretrained (str, optional): Path to pre-trained weights.
                Defaults to None.
        """

        def _init_weights(m):
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

        if isinstance(pretrained, str):
            self.apply(_init_weights)
        elif pretrained is None:
            self.apply(_init_weights)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, x):
        mov, fix = x
        """Forward function."""
        mov = self.patch_embed(mov)
        fix = self.patch_embed(fix)

        Wh, Ww, Wt = mov.size(2), mov.size(3), mov.size(4)

        if self.ape:
            # interpolate the position embedding to the corresponding size
            absolute_pos_embed = nnf.interpolate(self.absolute_pos_embed, size=(Wh, Ww, Wt), mode='trilinear')
            mov = (mov + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww*Wt C
            fix = (fix + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww*Wt C
        elif self.spe:
            mov = (mov + self.pos_embd(mov)).flatten(2).transpose(1, 2)
            fix = (fix + self.pos_embd(mov)).flatten(2).transpose(1, 2)
        else:
            mov = mov.flatten(2).transpose(1, 2)
            fix = fix.flatten(2).transpose(1, 2)
        mov = self.pos_drop(mov)
        fix = self.pos_drop(fix)
        x = (mov, fix)
        outs = []
        for i in range(self.num_layers):
            layer = self.layers[i]
            x_out, H, W, T, x, Wh, Ww, Wt = layer(x, Wh, Ww, Wt)
            if i in self.out_indices:
                m_norm_layer = getattr(self, f'm_norm{i}')
                f_norm_layer = getattr(self, f'f_norm{i}')
                mov_out, fix_out = x_out
                mov_out = m_norm_layer(mov_out)
                fix_out = f_norm_layer(fix_out)
                mov_out = mov_out.view(-1, H, W, T, self.num_features[i]).permute(0, 4, 1, 2, 3).contiguous()
                fix_out = fix_out.view(-1, H, W, T, self.num_features[i]).permute(0, 4, 1, 2, 3).contiguous()
                out = (mov_out, fix_out)#torch.cat(x_out, dim=4)
                outs.append(out)
        return outs

    def train(self, mode=True):
        """Convert the model into training mode while keep layers freezed."""
        super(SwinTransformer, self).train(mode)
        self._freeze_stages()

class Conv3dReLU(nn.Sequential):
    def __init__(
            self,
            in_channels,
            out_channels,
            kernel_size,
            padding=0,
            stride=1,
            use_batchnorm=True,
    ):
        conv = nn.Conv3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            bias=False,
        )
        relu = nn.LeakyReLU(inplace=True)
        if not use_batchnorm:
            nm = nn.InstanceNorm3d(out_channels)
        else:
            nm = nn.BatchNorm3d(out_channels)

        super(Conv3dReLU, self).__init__(conv, nm, relu)


class DecoderBlock(nn.Module):
    def __init__(
            self,
            in_channels,
            out_channels,
            skip_channels=0,
            use_batchnorm=True,
    ):
        super().__init__()
        self.conv1 = Conv3dReLU(
            in_channels + skip_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            use_batchnorm=use_batchnorm,
        )
        self.conv2 = Conv3dReLU(
            out_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            use_batchnorm=use_batchnorm,
        )
        self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)

    def forward(self, x, skip=None):
        x = self.up(x)
        if skip is not None:
            x = torch.cat([x, skip], dim=1)
        x = self.conv1(x)
        x = self.conv2(x)
        return x

class RegistrationHead(nn.Sequential):
    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):
        conv3d = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)
        conv3d.weight = nn.Parameter(Normal(0, 1e-5).sample(conv3d.weight.shape))
        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))
        super().__init__(conv3d)


class SpatialTransformer(nn.Module):
    """
    N-D Spatial Transformer
    Obtained from https://github.com/voxelmorph/voxelmorph
    """

    def __init__(self, size, mode='bilinear'):
        super().__init__()

        self.mode = mode

        # create sampling grid
        vectors = [torch.arange(0, s) for s in size]
        grids = torch.meshgrid(vectors, indexing="ij")
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)

        # registering the grid as a buffer cleanly moves it to the GPU, but it also
        # adds it to the state dict. this is annoying since everything in the state dict
        # is included when saving weights to disk, so the model files are way bigger
        # than they need to be. so far, there does not appear to be an elegant solution.
        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict
        self.register_buffer('grid', grid)

    def forward(self, src, flow):
        # new locations
        new_locs = self.grid + flow
        shape = flow.shape[2:]

        # need to normalize grid values to [-1, 1] for resampler
        for i in range(len(shape)):
            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)

        # move channels dim to last position
        # also not sure why, but the channels need to be reversed
        if len(shape) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(shape) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return nnf.grid_sample(src, new_locs, align_corners=False, mode=self.mode)

class ConstrainedConv3d(nn.Conv3d):
    def forward(self, input):
        weight = self.weight/torch.sum(self.weight, dim=(2, 3, 4), keepdim=True)
        return nnf.conv3d(input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)

class NCC_volume(torch.nn.Module):
    """
    Local (over window) correlation.
    """
    def __init__(self, in_dim, img_size, win=3, num_head=1):
        super(NCC_volume, self).__init__()
        self.win = win
        self.num_head = num_head
        self.conv3D = ConstrainedConv3d(in_dim, in_dim, kernel_size=win, padding=win // 2, groups=in_dim, bias=False)
        self.conv3D.weight.data.fill_(1/win**3)
        self.layer_norm_1 = nn.LayerNorm([in_dim//num_head, img_size[0], img_size[1], img_size[2]])
        self.layer_norm_2 = nn.LayerNorm([in_dim//num_head, img_size[0], img_size[1], img_size[2]])

    def forward(self, y_true, y_pred):
        N, C, H, W, L = y_true.shape
        I = torch.reshape(y_true, (N, self.num_head, C // self.num_head, H, W, L)).view(N * self.num_head, C//self.num_head, H, W, L)
        J = torch.reshape(y_pred, (N, self.num_head, C // self.num_head, H, W, L)).view(N * self.num_head, C // self.num_head, H, W, L)

        Ii = I#self.layer_norm_1(I)
        Ji = J#self.layer_norm_2(J)
        # compute CC squares
        mu1 = self.conv3D(Ii)
        mu2 = self.conv3D(Ji)

        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2

        sigma1_sq = self.conv3D(Ii * Ii) - mu1_sq
        sigma2_sq = self.conv3D(Ji * Ji) - mu2_sq
        sigma12 = self.conv3D(Ii * Ji) - mu1_mu2

        cc = (sigma12 * sigma12 + 1e-4) / (sigma1_sq * sigma2_sq + 1e-4)
        cc = 1-torch.sigmoid(cc)
        return cc
        
class TransMorphCascadeAd1(nn.Module):
    def __init__(self, config, time_steps=4):
        '''
        Multi-resolution TransMorph
        '''
        super(TransMorphCascadeAd, self).__init__()
        if_convskip = config.if_convskip
        self.if_convskip = if_convskip
        if_transskip = config.if_transskip
        self.if_transskip = if_transskip
        embed_dim = config.embed_dim
        self.time_steps = time_steps
        self.transformer = SwinTransformer(patch_size=config.patch_size,
                                           in_chans=config.in_chans,
                                           embed_dim=config.embed_dim,
                                           depths=config.depths,
                                           num_heads=config.num_heads,
                                           window_size=config.window_size,
                                           mlp_ratio=config.mlp_ratio,
                                           qkv_bias=config.qkv_bias,
                                           drop_rate=config.drop_rate,
                                           drop_path_rate=config.drop_path_rate,
                                           ape=config.ape,
                                           spe=config.spe,
                                           rpe=config.rpe,
                                           patch_norm=config.patch_norm,
                                           use_checkpoint=config.use_checkpoint,
                                           out_indices=config.out_indices,
                                           pat_merg_rf=config.pat_merg_rf,
                                           img_size=config.img_size,
                                           dwin_size=config.dwin_kernel_size)
        self.up0 = DecoderBlock(embed_dim * 4, embed_dim * 2, skip_channels=embed_dim * 2 if if_transskip else 0,
                                use_batchnorm=False)
        self.up1 = DecoderBlock(embed_dim * 2, embed_dim, skip_channels=embed_dim if if_transskip else 0,
                                use_batchnorm=False)  # 384, 20, 20, 64
        self.up2 = DecoderBlock(embed_dim, embed_dim//2, skip_channels=embed_dim//2 if if_transskip else 0,
                                use_batchnorm=False)  # 384, 20, 20, 64
        self.c1 = Conv3dReLU(2, embed_dim//2, 3, 1, use_batchnorm=False)
        self.spatial_trans = SpatialTransformer(config.img_size)
        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)
        self.reg_heads = nn.ModuleList()
        self.up3s = nn.ModuleList()
        self.cs = nn.ModuleList()
        for t in range(self.time_steps):
            self.cs.append(Conv3dReLU(2, embed_dim // 2, 3, 1, use_batchnorm=False))
            self.reg_heads.append(RegistrationHead(in_channels=config.reg_head_chan, out_channels=3, kernel_size=3, ))
            self.up3s.append(DecoderBlock(embed_dim//2, config.reg_head_chan, skip_channels=embed_dim // 2 if if_convskip else 0, use_batchnorm=False))
        self.tri_up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)

    def forward(self, inputs):
        mov, fix = inputs
        source_d = self.avg_pool(mov)
        x_cat = torch.cat((mov, fix), dim=1)
        out_feats = self.transformer((mov, fix))  # (B, n_patch, hidden)
        if self.if_convskip:
            x_s0 = x_cat.clone()
            f3 = self.c1(x_s0)
        else:
            f3 = None

        if self.if_transskip:
            mov_f1, fix_f1 = out_feats[-2]
            f1 = (mov_f1 + fix_f1)
            mov_f2, fix_f2 = out_feats[-3]
            f2 = (mov_f2 + fix_f2)
        else:
            f1 = None
            f2 = None
        mov_f0, fix_f0 = out_feats[-1]
        f0 = (mov_f0 + fix_f0)
        x = self.up0(f0, f1)
        x = self.up1(x, f2)
        xx = self.up2(x, f3) # 40
        def_x = x_s0[:, 0:1,...]
        flow_previous = torch.zeros((source_d.shape[0], 3, source_d.shape[2], source_d.shape[3], source_d.shape[4])).to(source_d.device)
        flows = []
        # flow integration
        for t in range(self.time_steps):
            f_out = self.cs[t](torch.cat((def_x, x_s0[:, 1:2,...]), dim=1))
            x = self.up3s[t](xx, f_out)
            flow = self.reg_heads[t](x)
            flows.append(flow)
            flow_new = flow_previous + self.spatial_trans(flow, flow)
            def_x = self.spatial_trans(source_d, flow_new)
            flow_previous = flow_new
        flow = flow_new
        return flow

class TransMorphCascadeAd(nn.Module):
    def __init__(self, config, time_steps=7):
        '''
        Multi-resolution TransMorph
        '''
        super(TransMorphCascadeAd, self).__init__()
        if_convskip = config.if_convskip
        self.if_convskip = if_convskip
        if_transskip = config.if_transskip
        self.if_transskip = if_transskip
        embed_dim = config.embed_dim
        self.time_steps = time_steps
        self.img_size = config.img_size
        self.transformer = SwinTransformer(patch_size=config.patch_size,
                                           in_chans=config.in_chans,
                                           embed_dim=config.embed_dim,
                                           depths=config.depths,
                                           num_heads=config.num_heads,
                                           window_size=config.window_size,
                                           mlp_ratio=config.mlp_ratio,
                                           qkv_bias=config.qkv_bias,
                                           drop_rate=config.drop_rate,
                                           drop_path_rate=config.drop_path_rate,
                                           ape=config.ape,
                                           spe=config.spe,
                                           rpe=config.rpe,
                                           patch_norm=config.patch_norm,
                                           use_checkpoint=config.use_checkpoint,
                                           out_indices=config.out_indices,
                                           pat_merg_rf=config.pat_merg_rf,
                                           img_size=config.img_size,
                                           dwin_size=config.dwin_kernel_size)
        self.up0 = DecoderBlock(embed_dim * 4, embed_dim * 2, skip_channels=embed_dim * 2 if if_transskip else 0,
                                use_batchnorm=False)
        self.up1 = DecoderBlock(embed_dim * 2, embed_dim, skip_channels=embed_dim if if_transskip else 0,
                                use_batchnorm=False)  # 384, 20, 20, 64
        self.up2 = DecoderBlock(embed_dim, embed_dim//2, skip_channels=embed_dim//2 if if_transskip else 0,
                                use_batchnorm=False)  # 384, 20, 20, 64
        self.c1 = Conv3dReLU(2, embed_dim//2, 3, 1, use_batchnorm=False)
        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)
        self.reg_heads = nn.ModuleList()
        self.up3s = nn.ModuleList()
        self.cs = nn.ModuleList()
        for t in range(self.time_steps):
            self.cs.append(Conv3dReLU(2, embed_dim // 2, 3, 1, use_batchnorm=False))
            self.reg_heads.append(RegistrationHead(in_channels=config.reg_head_chan, out_channels=3, kernel_size=3, ))
            self.up3s.append(DecoderBlock(embed_dim//2, config.reg_head_chan, skip_channels=embed_dim // 2 if if_convskip else 0,
                                   use_batchnorm=False))
        self.spatial_trans = SpatialTransformer(config.img_size)

    def forward(self, inputs):
        mov, fix = inputs
        x_cat = torch.cat((mov, fix), dim=1)
        x_s1 = self.avg_pool(x_cat)
        out_feats = self.transformer((mov, fix))  # (B, n_patch, hidden)
        if self.if_convskip:
            f3 = self.c1(x_s1)
        else:
            f3 = None
        if self.if_transskip:
            mov_f1, fix_f1 = out_feats[-2]
            f1 = (mov_f1 + fix_f1)
            mov_f2, fix_f2 = out_feats[-3]
            f2 = (mov_f2 + fix_f2)
        else:
            f1 = None
            f2 = None
        mov_f0, fix_f0 = out_feats[-1]
        f0 = (mov_f0 + fix_f0)
        x = self.up0(f0, f1)
        x = self.up1(x, f2)
        xx = self.up2(x, f3)
        def_x = mov.clone()
        flow_previous = torch.zeros((mov.shape[0], 3, mov.shape[2], mov.shape[3], mov.shape[4])).to(mov.device)
        flows = []
        # flow integration
        for t in range(self.time_steps):
            f_out = self.cs[t](torch.cat((def_x, fix), dim=1))
            x = self.up3s[t](xx, f_out)
            flow = self.reg_heads[t](x)
            flows.append(flow)
            flow_new = flow_previous + self.spatial_trans(flow, flow_previous)
            def_x = self.spatial_trans(mov, flow_new)
            flow_previous = flow_new
        flow = flow_new

        return flow

class TransMorphCascadeAdFullRes(nn.Module):
    def __init__(self, config, time_steps=4):
        '''
        Multi-resolution TransMorph
        '''
        super(TransMorphCascadeAdFullRes, self).__init__()
        if_convskip = config.if_convskip
        self.if_convskip = if_convskip
        if_transskip = config.if_transskip
        self.if_transskip = if_transskip
        embed_dim = config.embed_dim
        self.time_steps = time_steps
        self.transformer = SwinTransformer(patch_size=config.patch_size,
                                           in_chans=config.in_chans,
                                           embed_dim=config.embed_dim,
                                           depths=config.depths,
                                           num_heads=config.num_heads,
                                           window_size=config.window_size,
                                           mlp_ratio=config.mlp_ratio,
                                           qkv_bias=config.qkv_bias,
                                           drop_rate=config.drop_rate,
                                           drop_path_rate=config.drop_path_rate,
                                           ape=config.ape,
                                           spe=config.spe,
                                           rpe=config.rpe,
                                           patch_norm=config.patch_norm,
                                           use_checkpoint=config.use_checkpoint,
                                           out_indices=config.out_indices,
                                           pat_merg_rf=config.pat_merg_rf,
                                           img_size=config.img_size, )
        self.up0 = DecoderBlock(embed_dim * 4, embed_dim * 2, skip_channels=embed_dim * 2 if if_transskip else 0,
                                use_batchnorm=False)
        self.up1 = DecoderBlock(embed_dim * 2, embed_dim, skip_channels=embed_dim if if_transskip else 0,
                                use_batchnorm=False)  # 384, 20, 20, 64
        self.spatial_trans = SpatialTransformer(config.img_size)
        self.spatial_trans_down = SpatialTransformer((config.img_size[0]//2, config.img_size[1]//2, config.img_size[2]//2))
        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)
        self.reg_heads = nn.ModuleList()
        self.up2s = nn.ModuleList()
        self.cs = nn.ModuleList()
        for t in range(self.time_steps):
            self.cs.append(Conv3dReLU(2, embed_dim // 2, 3, 1, use_batchnorm=False))
            self.reg_heads.append(RegistrationHead(in_channels=config.reg_head_chan, out_channels=3, kernel_size=3, ))
            self.up2s.append(DecoderBlock(embed_dim, config.reg_head_chan, skip_channels=embed_dim // 2 if if_convskip else 0,
                                   use_batchnorm=False))

    def forward(self, inputs):
        mov, fix = inputs
        source_d = self.avg_pool(mov)
        x_cat = torch.cat((mov, fix), dim=1)
        x_s1 = self.avg_pool(x_cat)
        out_feats = self.transformer((mov, fix))  # (B, n_patch, hidden)
        if self.if_transskip:
            mov_f1, fix_f1 = out_feats[-2]
            f1 = (mov_f1 + fix_f1)
            mov_f2, fix_f2 = out_feats[-3]
            f2 = (mov_f2 + fix_f2)
        else:
            f1 = None
            f2 = None
        mov_f0, fix_f0 = out_feats[-1]
        f0 = mov_f0 + fix_f0
        x = self.up0(f0, f1)
        xx = self.up1(x, f2)
        def_x = x_s1[:, 0:1,...]
        flow_previous = torch.zeros((source_d.shape[0], 3, source_d.shape[2], source_d.shape[3], source_d.shape[4])).to(source_d.device)
        flows = []
        # flow integration
        for t in range(self.time_steps):
            f_out = self.cs[t](torch.cat((def_x, x_s1[:, 1:2,...]), dim=1))
            x = self.up2s[t](xx, f_out)
            flow = self.reg_heads[t](x)
            flows.append(flow)
            flow_new = flow_previous + self.spatial_trans_down(flow, flow_previous)
            def_x = self.spatial_trans_down(source_d, flow_new)
            flow_previous = flow_new
        flow = flow_new
        return  flow

CONFIGS = {
    'TransMorph-3-LVL': configs.get_3DTransMorphDWin3Lvl_config(),
}
################################################################################
# FILE: models/UTSRMorph/__init__.py
# SIZE: 0 bytes
################################################################################


################################################################################
# FILE: models/UTSRMorph/configs.py
# SIZE: 4072 bytes
################################################################################

import ml_collections

'''
********************************************************
                   Swin Transformer
********************************************************
if_transskip (bool): Enable skip connections from Transformer Blocks
if_convskip (bool): Enable skip connections from Convolutional Blocks
patch_size (int | tuple(int)): Patch size. Default: 4
in_chans (int): Number of input image channels. Default: 2 (for moving and fixed images)
embed_dim (int): Patch embedding dimension. Default: 96
depths (tuple(int)): Depth of each Swin Transformer layer.
num_heads (tuple(int)): Number of attention heads in different layers.
window_size (tuple(int)): Image size should be divisible by window size, 
                     e.g., if image has a size of (160, 192, 224), then the window size can be (5, 6, 7)
mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
pat_merg_rf (int): Embed_dim reduction factor in patch merging, e.g., N*C->N/4*C if set to four. Default: 4. 
qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
drop_rate (float): Dropout rate. Default: 0
drop_path_rate (float): Stochastic depth rate. Default: 0.1
ape (bool): Enable learnable position embedding. Default: False
spe (bool): Enable sinusoidal position embedding. Default: False
patch_norm (bool): If True, add normalization after patch embedding. Default: True
use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False 
                       (Carried over from Swin Transformer, it is not needed)
out_indices (tuple(int)): Indices of Transformer blocks to output features. Default: (0, 1, 2, 3)
reg_head_chan (int): Number of channels in the registration head (i.e., the final convolutional layer) 
img_size (int | tuple(int)): Input image size, e.g., (160, 192, 224)
'''

def get_UTSRMorph_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 96
    config.depths = (2, 2, 4, 2)
    config.num_heads = (4, 4, 8, 8)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2, 3)
    config.reg_head_chan = 16
    config.img_size = (160, 192, 224)
    return config


def get_UTSRMorph_debug_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 40
    config.depths = (2, 2, 18, 2)
    config.num_heads = (4, 4, 8, 16)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2, 3)
    config.reg_head_chan = 4
    config.img_size = (160, 192, 224)
    return config


def get_UTSRMorphLarge_config():
    config = ml_collections.ConfigDict()
    config.if_transskip = True
    config.if_convskip = True
    config.patch_size = 4
    config.in_chans = 2
    config.embed_dim = 160
    config.depths = (2, 2, 18, 2)
    config.num_heads = (4, 4, 8, 16)
    config.window_size = (5, 6, 7)
    config.mlp_ratio = 4
    config.pat_merg_rf = 4
    config.qkv_bias = False
    config.drop_rate = 0
    config.drop_path_rate = 0.3
    config.ape = False
    config.spe = False
    config.rpe = True
    config.patch_norm = True
    config.use_checkpoint = False
    config.out_indices = (0, 1, 2, 3)
    config.reg_head_chan = 16
    config.img_size = (160, 192, 224)
    return config
################################################################################
# FILE: models/UTSRMorph/model.py
# SIZE: 54257 bytes
################################################################################

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from timm.layers import DropPath, trunc_normal_, to_3tuple
from torch.distributions.normal import Normal
import torch.nn.functional as nnf
from typing import Tuple, Union
from einops import rearrange
import numpy as np
import math

import models.UTSRMorph.configs as configs


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
    

class CA(nn.Module):
    """Channel attention used in RCAN.
    Args:
        num_feat (int): Channel number of intermediate features.
        squeeze_factor (int): Channel squeeze factor. Default: 16.
    """

    def __init__(self, num_feat, squeeze_factor=16):
        super(CA, self).__init__()
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Conv3d(num_feat, num_feat // squeeze_factor, 1, padding=0),
            nn.ReLU(inplace=True),
            nn.Conv3d(num_feat // squeeze_factor, num_feat, 1, padding=0),
            nn.Sigmoid())

    def forward(self, x):
        y = self.attention(x)
        return x * y
    

class CAB(nn.Module):

    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):
        super(CAB, self).__init__()

        self.cab = nn.Sequential(
            nn.Conv3d(num_feat, num_feat // compress_ratio, 3, 1, 1),
            nn.GELU(),
            nn.Conv3d(num_feat // compress_ratio, num_feat, 3, 1, 1),
            CA(num_feat, squeeze_factor)
            )

    def forward(self, x):
        return self.cab(x)
    

def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, L, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, window_size, C)
    """
    B, H, W, L, C = x.shape
    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], L // window_size[2], window_size[2], C)

    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0], window_size[1], window_size[2], C)
    return windows


def window_reverse(windows, window_size, H, W, L):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
        L (int): Length of image
    Returns:
        x: (B, H, W, L, C)
    """
    B = int(windows.shape[0] / (H * W * L / window_size[0] / window_size[1] / window_size[2]))
    x = windows.view(B, H // window_size[0], W // window_size[1], L // window_size[2], window_size[0], window_size[1], window_size[2], -1)
    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, H, W, L, -1)
    return x


def filter_dilated_rows(
    tensor: torch.Tensor,
    dilation: Tuple[int, int, int],
    dilated_kernel_size: Tuple[int, int, int],
    kernel_size: Tuple[int, int, int],
):
    """
    A helper function that removes extra rows created during the process of
    implementing dilation.

    Args:
        tensor: A tensor containing the output slices resulting from unfolding
                the input tensor to `unfold3d()`.
                Shape is ``(B, C, D_out, H_out, W_out, dilated_kernel_size[0],
                dilated_kernel_size[1], dilated_kernel_size[2])``.
        dilation: The dilation given to `unfold3d()`.
        dilated_kernel_size: The size of the dilated kernel.
        kernel_size: The size of the kernel given to `unfold3d()`.

    Returns:
        A tensor of shape (B, C, D_out, H_out, W_out, kernel_size[0], kernel_size[1], kernel_size[2])
        For D_out, H_out, W_out definitions see :class:`torch.nn.Unfold`.

    Example:
        >>> tensor = torch.zeros([1, 1, 3, 3, 3, 5, 5, 5])
        >>> dilation = (2, 2, 2)
        >>> dilated_kernel_size = (5, 5, 5)
        >>> kernel_size = (3, 3, 3)
        >>> filter_dilated_rows(tensor, dilation, dilated_kernel_size, kernel_size).shape
        torch.Size([1, 1, 3, 3, 3, 3, 3, 3])
    """

    kernel_rank = len(kernel_size)
    indices_to_keep = [list(range(0, dilated_kernel_size[i], dilation[i])) for i in range(kernel_rank)]
    tensor_np = tensor.numpy()
    axis_offset = len(tensor.shape) - kernel_rank

    for dim in range(kernel_rank):
        tensor_np = np.take(tensor_np, indices_to_keep[dim], axis=axis_offset + dim)

    return torch.Tensor(tensor_np)


def unfold3d(
    tensor: torch.Tensor,
    *,
    kernel_size: Union[int, Tuple[int, int, int]],
    padding: Union[int, Tuple[int, int, int]] = 0,
    stride: Union[int, Tuple[int, int, int]] = 1,
    dilation: Union[int, Tuple[int, int, int]] = 1,
):
    r"""
    Extracts sliding local blocks from an batched input tensor.

    :class:`torch.nn.Unfold` only supports 4D inputs (batched image-like tensors).
    This method implements the same action for 5D inputs

    Args:
        tensor: An input tensor of shape ``(B, C, D, H, W)``.
        kernel_size: the size of the sliding blocks
        padding: implicit zero padding to be added on both sides of input
        stride: the stride of the sliding blocks in the input spatial dimensions
        dilation: the spacing between the kernel points.

    Returns:
        A tensor of shape ``(B, C * np.product(kernel_size), L)``, where L - output spatial dimensions.
        See :class:`torch.nn.Unfold` for more details

    Example:
        >>> B, C, D, H, W = 3, 4, 5, 6, 7
        >>> tensor = torch.arange(1, B*C*D*H*W + 1.).view(B, C, D, H, W)
        >>> unfold3d(tensor, kernel_size=2, padding=0, stride=1).shape
        torch.Size([3, 32, 120])
    """

    if len(tensor.shape) != 5:
        raise ValueError(
            f"Input tensor must be of the shape [B, C, D, H, W]. Got{tensor.shape}"
        )

    if isinstance(kernel_size, int):
        kernel_size = (kernel_size, kernel_size, kernel_size)

    if isinstance(padding, int):
        padding = (padding, padding, padding)

    if isinstance(stride, int):
        stride = (stride, stride, stride)

    if isinstance(dilation, int):
        dilation = (dilation, dilation, dilation)

    if padding == "same":
        total_pad_D = dilation[0] * (kernel_size[0] - 1)
        total_pad_H = dilation[1] * (kernel_size[1] - 1)
        total_pad_W = dilation[2] * (kernel_size[2] - 1)
        pad_D_left = math.floor(total_pad_D / 2)
        pad_D_right = total_pad_D - pad_D_left
        pad_H_left = math.floor(total_pad_H / 2)
        pad_H_right = total_pad_H - pad_H_left
        pad_W_left = math.floor(total_pad_W / 2)
        pad_W_right = total_pad_W - pad_W_left

    elif padding == "valid":
        pad_D_left, pad_D_right, pad_W_left, pad_W_right, pad_H_left, pad_H_right = (
            0,
            0,
            0,
            0,
            0,
            0,
        )
    else:
        pad_D_left, pad_D_right, pad_H_left, pad_H_right, pad_W_left, pad_W_right = (
            padding[0],
            padding[0],
            padding[1],
            padding[1],
            padding[2],
            padding[2],
        )

    batch_size, channels, _, _, _ = tensor.shape

    # Input shape: (B, C, D, H, W)
    tensor = F.pad(
        tensor,
        (pad_W_left, pad_W_right, pad_H_left, pad_H_right, pad_D_left, pad_D_right),
    )
    # Output shape: (B, C, D+pad_W_left+pad_W_right, H+pad_H_left+pad_H_right, W+pad_D_left+pad_D_right)

    dilated_kernel_size = (
        kernel_size[0] + (kernel_size[0] - 1) * (dilation[0] - 1),
        kernel_size[1] + (kernel_size[1] - 1) * (dilation[1] - 1),
        kernel_size[2] + (kernel_size[2] - 1) * (dilation[2] - 1),
    )

    tensor = tensor.unfold(dimension=2, size=dilated_kernel_size[0], step=stride[0])
    tensor = tensor.unfold(dimension=3, size=dilated_kernel_size[1], step=stride[1])
    tensor = tensor.unfold(dimension=4, size=dilated_kernel_size[2], step=stride[2])

    if dilation != (1, 1, 1):
        tensor = filter_dilated_rows(tensor, dilation, dilated_kernel_size, kernel_size)

    # Output shape: (B, C, D_out, H_out, W_out, kernel_size[0], kernel_size[1], kernel_size[2])
    # For D_out, H_out, W_out definitions see :class:`torch.nn.Unfold`

    tensor = tensor.permute(0, 2, 3, 4, 1, 5, 6, 7)
    # Output shape: (B, D_out, H_out, W_out, C, kernel_size[0], kernel_size[1], kernel_size[2])

    tensor = tensor.reshape(batch_size, -1, channels * np.prod(kernel_size)).transpose(1, 2)
    # Output shape: (B, D_out * H_out * W_out, C * kernel_size[0] * kernel_size[1] * kernel_size[2]

    return tensor


class OAB(nn.Module):
    # overlapping cross-attention block

    def __init__(self, dim,
                #input_resolution,
                window_size,
                overlap_ratio,
                num_heads,
                qkv_bias=True,
                qk_scale=None,
                mlp_ratio=2,
                norm_layer=nn.LayerNorm,
                rpe=True
                ):

        super().__init__()
        self.dim = dim
        #self.input_resolution = input_resolution
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5
        self.overlap_win_size = int(window_size * overlap_ratio) + window_size

        self.norm1 = norm_layer(dim)
        self.qkv = nn.Linear(dim, dim * 3,  bias=qkv_bias)
        #self.unfold = nn.Unfold(kernel_size=(self.overlap_win_size, self.overlap_win_size, self.overlap_win_size), stride=window_size, padding=(self.overlap_win_size-window_size)//2)

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((window_size + self.overlap_win_size - 1) * (window_size + self.overlap_win_size - 1) * (window_size + self.overlap_win_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH
        # get pair-wise relative position index for each token inside the window
        window_size_ori = self.window_size
        window_size_ext = self.overlap_win_size

        coords_h = torch.arange(window_size_ori)
        coords_w = torch.arange(window_size_ori)
        coords_t = torch.arange(window_size_ori)
        coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  #3, ws, ws
        coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws

        coords_h = torch.arange(window_size_ext)
        coords_w = torch.arange(window_size_ext)
        coords_t = torch.arange(window_size_ext)
        coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  # 3, wse, wse
        coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse

        relative_coords = coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]   # 3, ws*ws, wse*wse

        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # ws*ws, wse*wse, 2
        relative_coords[:, :, 0] += window_size_ori - window_size_ext + 1  # shift to start from 0
        relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1
        relative_coords[:, :, 2] += window_size_ori - window_size_ext + 1

        relative_coords[:, :, 0] *= (window_size_ori + window_size_ext - 1) * (window_size_ori + window_size_ext - 1)
        relative_coords[:, :, 1] *= window_size_ori + window_size_ext - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer("relative_position_index_OAB", relative_position_index)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

        self.proj = nn.Linear(dim,dim)

        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU)
        self.H = None
        self.W = None
        self.T = None

    def forward(self, x, mask=None):
        H, W, T = self.H, self.W, self.T
        B, L, C = x.shape

        assert L == H * W * T, "input feature has wrong size"
        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, T, C)

        pad_l = pad_t = pad_f = 0
        pad_r = (self.window_size - H % self.window_size) % self.window_size
        pad_b = (self.window_size- W % self.window_size) % self.window_size
        pad_h = (self.window_size - T % self.window_size) % self.window_size
        x = nnf.pad(x, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = x.shape
        qkv = self.qkv(x).reshape(B, Hp, Wp, Tp, 3, C).permute(4, 0, 5, 1, 2, 3) # 3, b, c, h, w, t
        q = qkv[0].permute(0, 2, 3, 4, 1) # b, h, w, T, c
        kv = torch.cat((qkv[1], qkv[2]), dim=1) # b, 2*c, h, w, T

        # partition windows
        q_windows = window_partition(q, [self.window_size,self.window_size,self.window_size])  # nw*b, window_size, window_size, c
        q_windows = q_windows.view(-1, self.window_size * self.window_size * self.window_size, C)  # nw*b, window_size*window_size, c

        kv_windows = unfold3d(kv, kernel_size=self.overlap_win_size, stride=self.window_size, padding=int((self.window_size)/2)) # b, c*w*w*W, nw

        kv_windows = rearrange(kv_windows, 'b (nc ch owh oww owt) nw -> nc (b nw) (owh oww owt) ch', nc=2, ch=C, owh=self.overlap_win_size, oww=self.overlap_win_size, owt=self.overlap_win_size).contiguous() # 2, nw*b, ow*ow, c
        k_windows, v_windows = kv_windows[0], kv_windows[1] # nw*b, ow*ow*ow, c

        b_, nq, _ = q_windows.shape
        _, n, _ = k_windows.shape
        d = self.dim // self.num_heads

        q = q_windows.reshape(b_, nq, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, nq, d
        k = k_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d
        v = v_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index_OAB.view(-1)].view(
                self.window_size * self.window_size * self.window_size, self.overlap_win_size * self.overlap_win_size * self.overlap_win_size, -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
        attn = attn + relative_position_bias.unsqueeze(0)


        attn = self.softmax(attn)
        attn_windows = (attn @ v).transpose(1, 2).reshape(b_, nq, self.dim)

        # merge windows
        #attn_windows = attn_windows.view(-1, self.window_size, self.window_size, self.window_size, self.dim)
        #x = window_reverse(attn_windows, self.window_size, H, W, T)  # b h w t c
        #x = x.view(B, H * W * T, self.dim)

        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, [self.window_size,self.window_size,self.window_size], Hp, Wp, Tp)  # B H' W' L' C
        x = shifted_x

        if pad_r > 0 or pad_b > 0 or pad_h > 0:
            x = x[:, :H, :W, :T, :].contiguous()

        x = x.view(B, H * W * T, C)
        x = self.proj(x) + shortcut

        x = x + self.mlp(self.norm2(x))
        return x
    

class WindowAttention(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, rpe=True, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1 * 2*Wt-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords_t = torch.arange(self.window_size[2])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w, coords_t], indexing='ij'))  # 3, Wh, Ww, Wt
        coords_flatten = torch.flatten(coords, 1)  # 3, Wh*Ww*Wt
        self.rpe = rpe
        if self.rpe:
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wh*Ww*Wt, Wh*Ww*Wt
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww*Wt, Wh*Ww*Wt, 3
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 2] += self.window_size[2] - 1
            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww*Wt, Wh*Ww*Wt
            self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """ Forward function.
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww, Wt*Ww) or None
        """
        B_, N, C = x.shape #(num_windows*B, Wh*Ww*Wt, C)
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        if self.rpe:
            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1] * self.window_size[2],
                self.window_size[0] * self.window_size[1] * self.window_size[2], -1)  # Wh*Ww*Wt,Wh*Ww*Wt,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww*Wt, Wh*Ww*Wt
            attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, num_heads, window_size=(7, 7, 7), shift_size=(0, 0, 0),
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, rpe=True, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        assert 0 <= min(self.shift_size) < min(self.window_size), "shift_size must in 0-window_size, shift_sz: {}, win_size: {}".format(self.shift_size, self.window_size)

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=self.window_size, num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, rpe=rpe, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
        self.conv_block = CAB(num_feat=self.dim, compress_ratio=3, squeeze_factor=30)
        self.H = None
        self.W = None
        self.T = None

    def forward(self, x, mask_matrix):
        H, W, T = self.H, self.W, self.T
        B, L, C = x.shape
        assert L == H * W * T, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, T, C)
        # Conv_X
        conv_x = self.conv_block(x.permute(0, 4, 1, 2, 3))
        conv_x = conv_x.permute(0, 2, 3, 4, 1).contiguous().view(B, H * W * T, C)
        # pad feature maps to multiples of window size
        pad_l = pad_t = pad_f = 0
        pad_r = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]
        pad_b = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]
        pad_h = (self.window_size[2] - T % self.window_size[2]) % self.window_size[2]
        x = nnf.pad(x, (0, 0, pad_f, pad_h, pad_t, pad_b, pad_l, pad_r))
        _, Hp, Wp, Tp, _ = x.shape

        # cyclic shift
        if min(self.shift_size) > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))
            attn_mask = mask_matrix
        else:
            shifted_x = x
            attn_mask = None

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C)  # nW*B, window_size*window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], C)
        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp, Tp)  # B H' W' L' C

        # reverse cyclic shift
        if min(self.shift_size) > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))
        else:
            x = shifted_x

        if pad_r > 0 or pad_b > 0 or pad_h > 0:
            x = x[:, :H, :W, :T, :].contiguous()

        x = x.view(B, H * W * T, C)

        # FFN
        # FFN
        x = shortcut + self.drop_path(x) + conv_x * 0.01
        #x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.
    Args:
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, norm_layer=nn.LayerNorm, reduce_factor=2):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(8 * dim, (8//reduce_factor) * dim, bias=False)
        self.norm = norm_layer(8 * dim)


    def forward(self, x, H, W, T):
        """
        x: B, H*W*T, C
        """
        B, L, C = x.shape
        assert L == H * W * T, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0 and T % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, T, C)

        # padding
        pad_input = (H % 2 == 1) or (W % 2 == 1) or (T % 2 == 1)
        if pad_input:
            x = nnf.pad(x, (0, 0, 0, T % 2, 0, W % 2, 0, H % 2))

        x0 = x[:, 0::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x1 = x[:, 1::2, 0::2, 0::2, :]  # B H/2 W/2 T/2 C
        x2 = x[:, 0::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x3 = x[:, 0::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x4 = x[:, 1::2, 1::2, 0::2, :]  # B H/2 W/2 T/2 C
        x5 = x[:, 0::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x6 = x[:, 1::2, 0::2, 1::2, :]  # B H/2 W/2 T/2 C
        x7 = x[:, 1::2, 1::2, 1::2, :]  # B H/2 W/2 T/2 C
        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)  # B H/2 W/2 T/2 8*C
        x = x.view(B, -1, 8 * C)  # B H/2*W/2*T/2 8*C

        x = self.norm(x)
        x = self.reduction(x)

        return x


class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.
    Args:
        dim (int): Number of feature channels
        depth (int): Depths of this stage.
        num_heads (int): Number of attention head.
        window_size (int): Local window size. Default: 7.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """
    def __init__(self,
                 dim,
                 depth,
                 num_heads,
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 rpe=True,
                 drop=0.,
                 attn_drop=0.,
                 drop_path=0.,
                 norm_layer=nn.LayerNorm,
                 downsample=None,
                 use_checkpoint=False,
                 pat_merg_rf=2,):
        super().__init__()
        self.window_size = window_size
        self.shift_size = (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2)
        self.depth = depth
        self.use_checkpoint = use_checkpoint
        self.pat_merg_rf = pat_merg_rf

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=(0, 0, 0) if (i % 2 == 0) else (window_size[0] // 2, window_size[1] // 2, window_size[2] // 2),
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                rpe=rpe,
                drop=drop,
                attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                norm_layer=norm_layer,)
            for i in range(depth)])

        self.overlap_attn = OAB(
            dim=dim,
            # input_resolution=input_resolution,
            window_size=4,
            overlap_ratio=0.5,
            num_heads=4,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            mlp_ratio=mlp_ratio,
            norm_layer=norm_layer
        )


        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(dim=dim, norm_layer=norm_layer, reduce_factor=self.pat_merg_rf)
        else:
            self.downsample = None

    def forward(self, x, H, W, T):
        """ Forward function.
        Args:
            x: Input feature, tensor size (B, H*W*T, C).
            H, W: Spatial resolution of the input feature.
        """
        # calculate attention mask for SW-MSA
        Hp = int(np.ceil(H / self.window_size[0])) * self.window_size[0]
        Wp = int(np.ceil(W / self.window_size[1])) * self.window_size[1]
        Tp = int(np.ceil(T / self.window_size[2])) * self.window_size[2]
        img_mask = torch.zeros((1, Hp, Wp, Tp, 1), device=x.device)  # 1 Hp Wp 1
        h_slices = (slice(0, -self.window_size[0]),
                    slice(-self.window_size[0], -self.shift_size[0]),
                    slice(-self.shift_size[0], None))
        w_slices = (slice(0, -self.window_size[1]),
                    slice(-self.window_size[1], -self.shift_size[1]),
                    slice(-self.shift_size[1], None))
        t_slices = (slice(0, -self.window_size[2]),
                    slice(-self.window_size[2], -self.shift_size[2]),
                    slice(-self.shift_size[2], None))
        cnt = 0
        for h in h_slices:
            for w in w_slices:
                for t in t_slices:
                    img_mask[:, h, w, t, :] = cnt
                    cnt += 1

        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
        mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2])
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

        for blk in self.blocks:
            blk.H, blk.W, blk.T = H, W, T
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x, attn_mask)
            else:
                x = blk(x, attn_mask)
        self.overlap_attn.H, self.overlap_attn.W, self.overlap_attn.T = H, W, T
        if self.use_checkpoint:
            x = checkpoint.checkpoint(self.overlap_attn, x, None)
        else:
            x = self.overlap_attn(x, None)

        if self.downsample is not None:
            x_down = self.downsample(x, H, W, T)
            Wh, Ww, Wt = (H + 1) // 2, (W + 1) // 2, (T + 1) // 2
            return x, H, W, T, x_down, Wh, Ww, Wt
        else:
            return x, H, W, T, x, H, W, T


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    Args:
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        patch_size = to_3tuple(patch_size)
        self.patch_size = patch_size

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        """Forward function."""
        # padding
        _, _, H, W, T = x.size()
        if T % self.patch_size[2] != 0:
            x = nnf.pad(x, (0, self.patch_size[2] - T % self.patch_size[2]))
        if W % self.patch_size[1] != 0:
            x = nnf.pad(x, (0, 0, 0, self.patch_size[1] - W % self.patch_size[1]))
        if H % self.patch_size[0] != 0:
            x = nnf.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))

        x = self.proj(x)  # B C Wh Ww Wt
        if self.norm is not None:
            Wh, Ww, Wt = x.size(2), x.size(3), x.size(4)
            x = x.flatten(2).transpose(1, 2)
            x = self.norm(x)
            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww, Wt)
        return x


class SinusoidalPositionEmbedding(nn.Module):
    '''
    Rotary Position Embedding
    '''
    def __init__(self,):
        super(SinusoidalPositionEmbedding, self).__init__()

    def forward(self, x):
        batch_sz, n_patches, hidden = x.shape
        position_ids = torch.arange(0, n_patches).float().cuda()
        indices = torch.arange(0, hidden//2).float().cuda()
        indices = torch.pow(10000.0, -2 * indices / hidden)
        embeddings = torch.einsum('b,d->bd', position_ids, indices)
        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)
        embeddings = torch.reshape(embeddings, (1, n_patches, hidden))
        return embeddings


class SinPositionalEncoding3D(nn.Module):
    def __init__(self, channels):
        """
        :param channels: The last dimension of the tensor you want to apply pos emb to.
        """
        super(SinPositionalEncoding3D, self).__init__()
        channels = int(np.ceil(channels/6)*2)
        if channels % 2:
            channels += 1
        self.channels = channels
        self.inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))
        #self.register_buffer('inv_freq', inv_freq)

    def forward(self, tensor):
        """
        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)
        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)
        """
        tensor = tensor.permute(0, 2, 3, 4, 1)
        if len(tensor.shape) != 5:
            raise RuntimeError("The input tensor has to be 5d!")
        batch_size, x, y, z, orig_ch = tensor.shape
        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())
        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())
        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())
        sin_inp_x = torch.einsum("i,j->ij", pos_x, self.inv_freq)
        sin_inp_y = torch.einsum("i,j->ij", pos_y, self.inv_freq)
        sin_inp_z = torch.einsum("i,j->ij", pos_z, self.inv_freq)
        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1).unsqueeze(1)
        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1).unsqueeze(1)
        emb_z = torch.cat((sin_inp_z.sin(), sin_inp_z.cos()), dim=-1)
        emb = torch.zeros((x,y,z,self.channels*3),device=tensor.device).type(tensor.type())
        emb[:,:,:,:self.channels] = emb_x
        emb[:,:,:,self.channels:2*self.channels] = emb_y
        emb[:,:,:,2*self.channels:] = emb_z
        emb = emb[None,:,:,:,:orig_ch].repeat(batch_size, 1, 1, 1, 1)
        return emb.permute(0, 4, 1, 2, 3)


class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030
    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (tuple): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(self, pretrain_img_size=224,
                 patch_size=4,
                 in_chans=3,
                 embed_dim=96,
                 depths=[2, 2, 6, 2],
                 num_heads=[3, 6, 12, 24],
                 window_size=(7, 7, 7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.2,
                 norm_layer=nn.LayerNorm,
                 ape=False,
                 spe=False,
                 rpe=True,
                 patch_norm=True,
                 out_indices=(0, 1, 2, 3),
                 frozen_stages=-1,
                 use_checkpoint=False,
                 pat_merg_rf=2,):
        super().__init__()
        self.pretrain_img_size = pretrain_img_size
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.spe = spe
        self.rpe = rpe
        self.patch_norm = patch_norm
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)

        # absolute position embedding
        if self.ape:
            pretrain_img_size = to_3tuple(self.pretrain_img_size)
            patch_size = to_3tuple(patch_size)
            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1], pretrain_img_size[2] // patch_size[2]]

            self.absolute_pos_embed = nn.Parameter(
                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1], patches_resolution[2]))
            trunc_normal_(self.absolute_pos_embed, std=.02)
        elif self.spe:
            self.pos_embd = SinPositionalEncoding3D(embed_dim).cuda()
            #self.pos_embd = SinusoidalPositionEmbedding().cuda()
        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                                depth=depths[i_layer],
                                num_heads=num_heads[i_layer],
                                window_size=window_size,
                                mlp_ratio=mlp_ratio,
                                qkv_bias=qkv_bias,
                                rpe = rpe,
                                qk_scale=qk_scale,
                                drop=drop_rate,
                                attn_drop=attn_drop_rate,
                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                                norm_layer=norm_layer,
                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                                use_checkpoint=use_checkpoint,
                               pat_merg_rf=pat_merg_rf,)
            self.layers.append(layer)

        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
        self.num_features = num_features

        # add a norm layer for each output
        for i_layer in out_indices:
            layer = norm_layer(num_features[i_layer])
            layer_name = f'norm{i_layer}'
            self.add_module(layer_name, layer)

        self._freeze_stages()

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False

        if self.frozen_stages >= 1 and self.ape:
            self.absolute_pos_embed.requires_grad = False

        if self.frozen_stages >= 2:
            self.pos_drop.eval()
            for i in range(0, self.frozen_stages - 1):
                m = self.layers[i]
                m.eval()
                for param in m.parameters():
                    param.requires_grad = False

    def init_weights(self, pretrained=None):
        """Initialize the weights in backbone.
        Args:
            pretrained (str, optional): Path to pre-trained weights.
                Defaults to None.
        """

        def _init_weights(m):
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

        if isinstance(pretrained, str):
            self.apply(_init_weights)
        elif pretrained is None:
            self.apply(_init_weights)
        else:
            raise TypeError('pretrained must be a str or None')

    def forward(self, x):
        """Forward function."""
        x = self.patch_embed(x)

        Wh, Ww, Wt = x.size(2), x.size(3), x.size(4)

        if self.ape:
            # interpolate the position embedding to the corresponding size
            absolute_pos_embed = nnf.interpolate(self.absolute_pos_embed, size=(Wh, Ww, Wt), mode='trilinear')
            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww*Wt C
        elif self.spe:
            x = (x + self.pos_embd(x)).flatten(2).transpose(1, 2)
        else:
            x = x.flatten(2).transpose(1, 2)
        x = self.pos_drop(x)

        outs = []
        for i in range(self.num_layers):
            layer = self.layers[i]
            x_out, H, W, T, x, Wh, Ww, Wt = layer(x, Wh, Ww, Wt)
            if i in self.out_indices:
                norm_layer = getattr(self, f'norm{i}')
                x_out = norm_layer(x_out)

                out = x_out.view(-1, H, W, T, self.num_features[i]).permute(0, 4, 1, 2, 3).contiguous()
                outs.append(out)
        return outs

    def train(self, mode=True):
        """Convert the model into training mode while keep layers freezed."""
        super(SwinTransformer, self).train(mode)
        self._freeze_stages()


class Conv3dReLU(nn.Sequential):
    def __init__(
            self,
            in_channels,
            out_channels,
            kernel_size,
            padding=0,
            stride=1,
            use_batchnorm=True,
    ):
        conv = nn.Conv3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            bias=False,
        )
        relu = nn.LeakyReLU(inplace=True)
        if not use_batchnorm:
            nm = nn.InstanceNorm3d(out_channels)
        else:
            nm = nn.BatchNorm3d(out_channels)

        super(Conv3dReLU, self).__init__(conv, nm, relu)


class PixelShuffle3d(nn.Module):
    '''
    This class is a 3d version of pixelshuffle.
    '''
    def __init__(self, scale):
        '''
        :param scale: upsample scale
        '''
        super().__init__()
        self.scale = scale

    def forward(self, input):
        batch_size, channels, in_depth, in_height, in_width = input.size()
        nOut = channels // self.scale ** 3

        out_depth = in_depth * self.scale
        out_height = in_height * self.scale
        out_width = in_width * self.scale

        input_view = input.contiguous().view(batch_size, nOut, self.scale, self.scale, self.scale, in_depth, in_height, in_width)

        output = input_view.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()

        return output.view(batch_size, nOut, out_depth, out_height, out_width)


class ConvergeHead(nn.Module):
    def __init__(self, in_dim, up_ratio, kernel_size, padding):
        super().__init__()
        self.in_dim = in_dim
        self.up_ratio = up_ratio


        self.conv = nn.Conv3d(in_dim, (up_ratio**3)*in_dim, kernel_size, 1, padding, 1, in_dim)
        self.apply(self._init_weights)

    def forward(self, x):
        hp = self.conv(x)
        #hp = F.pixel_shuffle(hp, self.up_ratio)
        poxel = PixelShuffle3d(self.up_ratio)
        hp = poxel(hp)

        return hp
    def _init_weights(self, m):
        if isinstance(m, (nn.Conv3d, nn.Linear)):
            nn.init.normal_(m.weight, std=0.001)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm3d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)


class SR(nn.Module):
    def __init__(
            self,
            in_channels,
            out_channels,
            skip_channels=0,
            use_batchnorm=True,
    ):
        super().__init__()
        self.conv1 = Conv3dReLU(
            in_channels + skip_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            use_batchnorm=use_batchnorm,
        )
        self.conv2 = Conv3dReLU(
            out_channels,
            out_channels,
            kernel_size=3,
            padding=1,
            use_batchnorm=use_batchnorm,
        )
        #self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)
        self.up = ConvergeHead(in_channels, 2, 3, 1)

    def forward(self, x, skip=None):
        x = self.up(x)
        if skip is not None:
            if skip.shape[2:] != x.shape[2:]:
                skip = nnf.interpolate(skip, size=x.shape[2:],  mode='trilinear', align_corners=False)
            x = torch.cat([x, skip], dim=1)
        x = self.conv1(x)
        x = self.conv2(x)
        return x


class RegistrationHead(nn.Sequential):
    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):
        conv3d = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)
        conv3d.weight = nn.Parameter(Normal(0, 1e-5).sample(conv3d.weight.shape))
        conv3d.bias = nn.Parameter(torch.zeros(conv3d.bias.shape))
        super().__init__(conv3d)


class SpatialTransformer(nn.Module):
    """
    N-D Spatial Transformer
    Obtained from https://github.com/voxelmorph/voxelmorph
    """
    def __init__(self, size, mode='bilinear'):
        super().__init__()

        self.mode = mode

        # create sampling grid
        vectors = [torch.arange(0, s) for s in size]
        grids = torch.meshgrid(vectors, indexing='ij')
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)

        # registering the grid as a buffer cleanly moves it to the GPU, but it also
        # adds it to the state dict. this is annoying since everything in the state dict
        # is included when saving weights to disk, so the model files are way bigger
        # than they need to be. so far, there does not appear to be an elegant solution.
        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict
        self.register_buffer('grid', grid)

    def forward(self, src, flow):
        # new locations
        new_locs = self.grid + flow
        shape = flow.shape[2:]

        # need to normalize grid values to [-1, 1] for resampler
        for i in range(len(shape)):
            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)

        # move channels dim to last position
        # also not sure why, but the channels need to be reversed
        if len(shape) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(shape) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return nnf.grid_sample(src, new_locs, align_corners=False, mode=self.mode)


class UTSRMorph(nn.Module):
    def __init__(self, config):
        '''
        UTSRMorph Model
        '''
        super(UTSRMorph, self).__init__()
        if_convskip = config.if_convskip
        self.if_convskip = if_convskip
        if_transskip = config.if_transskip
        self.if_transskip = if_transskip
        embed_dim = config.embed_dim
        self.transformer = SwinTransformer(patch_size=config.patch_size,
                                           in_chans=config.in_chans,
                                           embed_dim=config.embed_dim,
                                           depths=config.depths,
                                           num_heads=config.num_heads,
                                           window_size=config.window_size,
                                           mlp_ratio=config.mlp_ratio,
                                           qkv_bias=config.qkv_bias,
                                           drop_rate=config.drop_rate,
                                           drop_path_rate=config.drop_path_rate,
                                           ape=config.ape,
                                           spe=config.spe,
                                           rpe=config.rpe,
                                           patch_norm=config.patch_norm,
                                           use_checkpoint=config.use_checkpoint,
                                           out_indices=config.out_indices,
                                           pat_merg_rf=config.pat_merg_rf,
                                           )

        self.up0 = SR(embed_dim*8, embed_dim*4, skip_channels=embed_dim*4 if if_transskip else 0, use_batchnorm=False)
        self.up1 = SR(embed_dim*4, embed_dim*2, skip_channels=embed_dim*2 if if_transskip else 0, use_batchnorm=False)  # 384, 20, 20, 64
        self.up2 = SR(embed_dim*2, embed_dim, skip_channels=embed_dim if if_transskip else 0, use_batchnorm=False)  # 384, 40, 40, 64
        self.up3 = SR(embed_dim, config.reg_head_chan, skip_channels=embed_dim//2 if if_convskip else 0, use_batchnorm=False)  # 384, 80, 80, 128
        self.c1 = Conv3dReLU(2, embed_dim//2, 3, 1, use_batchnorm=False)
        self.reg_head = RegistrationHead(
            in_channels=config.reg_head_chan,
            out_channels=3,
            kernel_size=3,
        )
        self.spatial_trans = SpatialTransformer(config.img_size)
        self.avg_pool = nn.AvgPool3d(3, stride=2, padding=1)
        self.up = ConvergeHead(3, 2, 3, 1)

    def forward(self, x):
        source = x[:, 0:1, :, :]
        if self.if_convskip:
            x_s1 = self.avg_pool(x)
            f4 = self.c1(x_s1)
        else:
            f4 = None

        out_feats = self.transformer(x)

        if self.if_transskip:
            f1 = out_feats[-2]
            f2 = out_feats[-3]
            f3 = out_feats[-4]
        else:
            f1 = None
            f2 = None
            f3 = None
        x = self.up0(out_feats[-1], f1)
        x = self.up1(x, f2)
        x = self.up2(x, f3)
        x = self.up3(x, f4)
        flow = self.reg_head(x)
        flow = self.up(flow)
        out = self.spatial_trans(source, flow)
        return out, flow


CONFIGS = {
    'UTSRMorph': configs.get_UTSRMorph_config(),
    'UTSRMorph-Large': configs.get_UTSRMorphLarge_config(),
    'UTSRMorph-Debug': configs.get_UTSRMorph_debug_config(),
}
################################################################################
# FILE: models/__init__.py
# SIZE: 0 bytes
################################################################################


################################################################################
# FILE: utils/__init__.py
# SIZE: 380 bytes
################################################################################

# utils/__init__.py
# Public API of utils.


from utils.core import *
from utils.ctcf_losses import *
from utils.data import *
from utils.dice import *
from utils.field import *
from utils.losses import *
from utils.misc import *
from utils.rand import *
from utils.spatial import *
from utils.train import *
from utils.trans import *
from utils.validation import *
################################################################################
# FILE: utils/core.py
# SIZE: 1336 bytes
################################################################################

import pickle
import numpy as np
import torch
import torch.nn.functional as F


def pkload(fname: str):
    with open(fname, "rb") as f:
        return pickle.load(f)


class AverageMeter:
    """Computes and stores the average and current value."""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0.0
        self.avg = 0.0
        self.sum = 0.0
        self.count = 0
        self.vals = []
        self.std = 0.0

    def update(self, val, n: int = 1):
        val = float(val)
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / max(1, self.count)
        self.vals.append(val)
        self.std = float(np.std(self.vals)) if len(self.vals) > 1 else 0.0


def pad_image(img: torch.Tensor, target_size):
    """
    Pad 3D image tensor [B,C,D,H,W] to at least target_size (D,H,W) with zeros.
    """
    rows_to_pad = max(target_size[0] - img.shape[2], 0)
    cols_to_pad = max(target_size[1] - img.shape[3], 0)
    slcs_to_pad = max(target_size[2] - img.shape[4], 0)
    return F.pad(img, (0, slcs_to_pad, 0, cols_to_pad, 0, rows_to_pad), "constant", 0)


def write2csv(line: str, name: str):
    with open(name + ".csv", "a", encoding="utf-8") as f:
        f.write(line)
        f.write("\n")
################################################################################
# FILE: utils/ctcf_losses.py
# SIZE: 1515 bytes
################################################################################

import torch
from utils import field


def icon_loss(flow_ab: torch.Tensor, flow_ba: torch.Tensor) -> torch.Tensor:
    """
    ICON / inverse-consistency.
    Penalize composition phi_ab ∘ phi_ba deviating from identity.
    """
    phi_ab_ba = field.compose_flows(flow_ab, flow_ba, mode='bilinear')
    phi_ba_ab = field.compose_flows(flow_ba, flow_ab, mode='bilinear')
    return phi_ab_ba.abs().mean() + phi_ba_ab.abs().mean()


def cycle_image_loss(model,
                     x: torch.Tensor, y: torch.Tensor,
                     x_warp: torch.Tensor, y_warp: torch.Tensor,
                     flow_xy: torch.Tensor, flow_yx: torch.Tensor) -> torch.Tensor:
    """
    Cycle-consistency in image space:
    x -> y -> x and y -> x -> y.
    """
    # Prefer spatial_trans_full if exists, else spatial_trans
    if hasattr(model, 'spatial_trans_down'):
        warp_fn = model.spatial_trans_down
    elif hasattr(model, 'spatial_trans_full'):
        warp_fn = model.spatial_trans_full
    else:
        warp_fn = model.spatial_trans

    x_cycle = warp_fn(x_warp, flow_yx)
    y_cycle = warp_fn(y_warp, flow_xy)
    return (x_cycle - x).abs().mean() + (y_cycle - y).abs().mean()


def percent_nonpositive_jacobian(flow: torch.Tensor) -> torch.Tensor:
    """
    Metric: percentage of voxels with detJ <= 0 (foldings).
    Returns a scalar tensor in [0,100].
    """
    detJ = field.jacobian_det(flow)  # [B,1,D,H,W]
    return (detJ <= 0.0).float().mean() * 100.0
################################################################################
# FILE: utils/data.py
# SIZE: 2182 bytes
################################################################################

import random
import numpy as np
import torch
import re

M = 2 ** 32 - 1
_shape = (240, 240, 155)
_zero = torch.tensor([0])


def init_fn(worker: int):
    seed = torch.LongTensor(1).random_().item()
    seed = (seed + worker) % M
    np.random.seed(seed)
    random.seed(seed)


def add_mask(x: torch.Tensor, mask: torch.Tensor, dim: int = 1) -> torch.Tensor:
    mask = mask.unsqueeze(dim)
    shape = list(x.shape)
    shape[dim] += 21
    new_x = x.new_zeros(*shape)
    new_x = new_x.scatter_(dim, mask, 1.0)
    s = [slice(None)] * len(shape)
    s[dim] = slice(21, None)
    new_x[s] = x
    return new_x


def sample(x: np.ndarray, size: int) -> torch.Tensor:
    i = random.sample(range(x.shape[0]), size)
    return torch.tensor(x[i], dtype=torch.int16)


def get_all_coords(stride: int) -> torch.Tensor:
    return torch.tensor(
        np.stack(
            [v.reshape(-1) for v in np.meshgrid(
                *[stride // 2 + np.arange(0, s, stride) for s in _shape],
                indexing="ij"
            )],
            -1
        ),
        dtype=torch.int16
    )


def gen_feats() -> np.ndarray:
    x, y, z = _shape
    feats = np.stack(np.meshgrid(np.arange(x), np.arange(y), np.arange(z), indexing="ij"), -1).astype("float32")
    shape = np.array([x, y, z])
    feats -= shape / 2.0
    feats /= shape
    return feats


def process_label(label_info_path: str = "label_info.txt"):
    seg_table = [
        0, 2, 3, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 24, 26,
        28, 30, 31, 41, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 58,
        60, 62, 63, 72, 77, 80, 85, 251, 252, 253, 254, 255
    ]
    with open(label_info_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    label_dict = {}
    seg_i = 0
    for seg_label in seg_table:
        for line in lines:
            parts = re.sub(" +", " ", line).split(" ")
            try:
                int(parts[0])
            except Exception:
                continue
            if int(parts[0]) == seg_label:
                label_dict[seg_i] = parts[1]
        seg_i += 1
    return label_dict
################################################################################
# FILE: utils/dice.py
# SIZE: 2468 bytes
################################################################################

import numpy as np
import torch
from torch import nn
from scipy.ndimage import gaussian_filter


def dice_val(y_pred: torch.Tensor, y_true: torch.Tensor, num_clus: int) -> torch.Tensor:
    y_pred = nn.functional.one_hot(y_pred, num_classes=num_clus)
    y_pred = torch.squeeze(y_pred, 1).permute(0, 4, 1, 2, 3).contiguous()
    y_true = nn.functional.one_hot(y_true, num_classes=num_clus)
    y_true = torch.squeeze(y_true, 1).permute(0, 4, 1, 2, 3).contiguous()

    intersection = (y_pred * y_true).sum(dim=[2, 3, 4])
    union = y_pred.sum(dim=[2, 3, 4]) + y_true.sum(dim=[2, 3, 4])
    dsc = (2.0 * intersection) / (union + 1e-5)
    return torch.mean(torch.mean(dsc, dim=1))


def dice_val_VOI(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
    VOI_lbls = list(range(1, 36))  # 1..35
    pred = y_pred.detach().cpu().numpy()[0, 0, ...]
    true = y_true.detach().cpu().numpy()[0, 0, ...]
    dscs = []
    for i in VOI_lbls:
        pred_i = (pred == i)
        true_i = (true == i)
        inter = np.sum(pred_i & true_i)
        union = np.sum(pred_i) + np.sum(true_i)
        dscs.append((2.0 * inter) / (union + 1e-5))
    return torch.tensor(float(np.mean(dscs)), device=y_pred.device)


def dice_val_substruct(y_pred: torch.Tensor, y_true: torch.Tensor, std_idx: int) -> str:
    with torch.no_grad():
        y_pred_oh = nn.functional.one_hot(y_pred, num_classes=46)
        y_pred_oh = torch.squeeze(y_pred_oh, 1).permute(0, 4, 1, 2, 3).contiguous()
        y_true_oh = nn.functional.one_hot(y_true, num_classes=46)
        y_true_oh = torch.squeeze(y_true_oh, 1).permute(0, 4, 1, 2, 3).contiguous()

    yp = y_pred_oh.detach().cpu().numpy()
    yt = y_true_oh.detach().cpu().numpy()

    line = f"p_{std_idx}"
    for i in range(46):
        pred_clus = yp[0, i, ...]
        true_clus = yt[0, i, ...]
        inter = (pred_clus * true_clus).sum()
        union = pred_clus.sum() + true_clus.sum()
        dsc = (2.0 * inter) / (union + 1e-5)
        line += "," + str(dsc)
    return line


def dice(y_pred: np.ndarray, y_true: np.ndarray) -> float:
    inter = float(np.sum(y_pred * y_true))
    union = float(np.sum(y_pred) + np.sum(y_true))
    return (2.0 * inter) / (union + 1e-5)


def smooth_seg(binary_img: np.ndarray, sigma: float = 1.5, thresh: float = 0.4) -> np.ndarray:
    sm = gaussian_filter(binary_img.astype(np.float32), sigma=sigma)
    return (sm > thresh)
################################################################################
# FILE: utils/field.py
# SIZE: 2730 bytes
################################################################################

import torch
import torch.nn.functional as F


def _warp(tensor: torch.Tensor, flow: torch.Tensor, mode: str = "bilinear") -> torch.Tensor:
    """
    Warp tensor [B,C,D,H,W] by flow [B,3,D,H,W] (Voxelmorph-style).
    """
    b, c, d, h, w = tensor.shape
    device = tensor.device

    # base grid in ij indexing: z,y,x
    zz = torch.arange(d, device=device)
    yy = torch.arange(h, device=device)
    xx = torch.arange(w, device=device)
    grid = torch.stack(torch.meshgrid(zz, yy, xx, indexing="ij"), dim=0).float()  # [3,D,H,W]
    grid = grid.unsqueeze(0)  # [1,3,D,H,W]

    new_locs = grid + flow
    # normalize to [-1,1]
    new_locs[:, 0] = 2.0 * (new_locs[:, 0] / (d - 1) - 0.5)
    new_locs[:, 1] = 2.0 * (new_locs[:, 1] / (h - 1) - 0.5)
    new_locs[:, 2] = 2.0 * (new_locs[:, 2] / (w - 1) - 0.5)

    # grid_sample expects (..., x,y,z) i.e. reverse
    grid_sample_grid = new_locs.permute(0, 2, 3, 4, 1)[..., [2, 1, 0]]
    return F.grid_sample(tensor, grid_sample_grid, mode=mode, align_corners=False)


def compose_flows(flow_ab: torch.Tensor, flow_bc: torch.Tensor, mode: str = "bilinear") -> torch.Tensor:
    """
    Compose displacement fields:
      phi_ac = phi_ab ∘ phi_bc  (displacement convention)
    For displacements: flow_ac = flow_ab + warp(flow_bc, flow_ab)
    """
    return flow_ab + _warp(flow_bc, flow_ab, mode=mode)


def jacobian_det(flow: torch.Tensor) -> torch.Tensor:
    """
    det(J) for transformation (Id + flow) using finite differences in torch.
    flow: [B,3,D,H,W]
    returns: [B,1,D,H,W]
    """
    # gradients w.r.t. z,y,x (D,H,W)
    dz = flow[:, :, 2:, :, :] - flow[:, :, :-2, :, :]
    dy = flow[:, :, :, 2:, :] - flow[:, :, :, :-2, :]
    dx = flow[:, :, :, :, 2:] - flow[:, :, :, :, :-2]

    # pad back to original size (central diff approx)
    dz = F.pad(dz, (0, 0, 0, 0, 1, 1)) * 0.5
    dy = F.pad(dy, (0, 0, 1, 1, 0, 0)) * 0.5
    dx = F.pad(dx, (1, 1, 0, 0, 0, 0)) * 0.5

    # J = I + grad(flow)
    fz, fy, fx = flow[:, 0], flow[:, 1], flow[:, 2]

    # partials:
    fz_z, fz_y, fz_x = dz[:, 0], dy[:, 0], dx[:, 0]
    fy_z, fy_y, fy_x = dz[:, 1], dy[:, 1], dx[:, 1]
    fx_z, fx_y, fx_x = dz[:, 2], dy[:, 2], dx[:, 2]

    J00 = 1.0 + fz_z
    J01 = fz_y
    J02 = fz_x

    J10 = fy_z
    J11 = 1.0 + fy_y
    J12 = fy_x

    J20 = fx_z
    J21 = fx_y
    J22 = 1.0 + fx_x

    det = (
        J00 * (J11 * J22 - J12 * J21)
        - J01 * (J10 * J22 - J12 * J20)
        + J02 * (J10 * J21 - J11 * J20)
    )
    return det.unsqueeze(1)


def neg_jacobian_penalty(flow: torch.Tensor) -> torch.Tensor:
    detJ = jacobian_det(flow)
    return torch.relu(-detJ).mean()
################################################################################
# FILE: utils/losses.py
# SIZE: 27643 bytes
################################################################################

import torch
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
from math import exp
import math
import torch.nn as nn


class NCC_gauss(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss via Gaussian kernel
    """
    def __init__(self, win=9):
        super(NCC_gauss, self).__init__()
        self.win = [win]*3
        self.filt = self.create_window_3D(win, 1).to("cuda")

    def gaussian(self, window_size, sigma):
        gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])
        return gauss / gauss.sum()

    def create_window_3D(self, window_size, channel):
        _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
        _2D_window = _1D_window.mm(_1D_window.t())
        _3D_window = _1D_window.mm(_2D_window.reshape(1, -1)).reshape(window_size, window_size,
                                                                      window_size).float().unsqueeze(0).unsqueeze(0)
        window = Variable(_3D_window.expand(channel, 1, window_size, window_size, window_size).contiguous())
        return window

    def forward(self, y_true, y_pred):

        Ii = y_true
        Ji = y_pred

        ndims = len(list(Ii.size())) - 2
        assert ndims in [1, 2, 3], "volumes should be 1 to 3 dimensions. found: %d" % ndims
        pad_no = math.floor(self.win[0] / 2)
        conv_fn = getattr(F, 'conv%dd' % ndims)

        mu1 = conv_fn(Ii, self.filt, padding=pad_no)
        mu2 = conv_fn(Ji, self.filt, padding=pad_no)

        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2

        sigma1_sq = conv_fn(Ii * Ii, self.filt, padding=pad_no) - mu1_sq
        sigma2_sq = conv_fn(Ji * Ji, self.filt, padding=pad_no) - mu2_sq
        sigma12 = conv_fn(Ii * Ji, self.filt, padding=pad_no) - mu1_mu2

        cc = (sigma12 * sigma12 + 1e-5)/(sigma1_sq * sigma2_sq + 1e-5)
        return 1-torch.mean(cc)


def gaussian(window_size, sigma):
    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])
    return gauss / gauss.sum()


def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
    return window


def create_window_3D(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t())
    _3D_window = _1D_window.mm(_2D_window.reshape(1, -1)).reshape(window_size, window_size,
                                                                  window_size).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_3D_window.expand(channel, 1, window_size, window_size, window_size).contiguous())
    return window


def _ssim(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq
    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2

    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)


def _ssim_3D(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv3d(img1, window, padding=window_size // 2, groups=channel)
    mu2 = F.conv3d(img2, window, padding=window_size // 2, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)

    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv3d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq
    sigma2_sq = F.conv3d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq
    sigma12 = F.conv3d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2

    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)


class SSIM(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True):
        super(SSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window(window_size, self.channel)

    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()

        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window(self.window_size, channel)

            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)

            self.window = window
            self.channel = channel

        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)


class SSIM3D(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True):
        super(SSIM3D, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window_3D(window_size, self.channel)

    def forward(self, img1, img2):
        (_, channel, _, _, _) = img1.size()

        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window_3D(self.window_size, channel)

            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)

            self.window = window
            self.channel = channel

        return 1-_ssim_3D(img1, img2, window, self.window_size, channel, self.size_average)


def ssim(img1, img2, window_size=11, size_average=True):
    (_, channel, _, _) = img1.size()
    window = create_window(window_size, channel)

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)

    return _ssim(img1, img2, window, window_size, channel, size_average)


def ssim3D(img1, img2, window_size=11, size_average=True):
    (_, channel, _, _, _) = img1.size()
    window = create_window_3D(window_size, channel)

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)

    return _ssim_3D(img1, img2, window, window_size, channel, size_average)


class Grad(torch.nn.Module):
    """
    N-D gradient loss.
    """
    def __init__(self, penalty='l1', loss_mult=None):
        super(Grad, self).__init__()
        self.penalty = penalty
        self.loss_mult = loss_mult

    def forward(self, y_pred, y_true):
        dy = torch.abs(y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :])
        dx = torch.abs(y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1])

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx

        d = torch.mean(dx) + torch.mean(dy)
        grad = d / 2.0

        if self.loss_mult is not None:
            grad *= self.loss_mult
        return grad


class Grad3d(torch.nn.Module):
    """
    N-D gradient loss.
    """
    def __init__(self, penalty='l1', loss_mult=None):
        super(Grad3d, self).__init__()
        self.penalty = penalty
        self.loss_mult = loss_mult

    def forward(self, y_pred, y_true=None):
        dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])
        dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])
        dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx
            dz = dz * dz

        d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)
        grad = d / 3.0

        if self.loss_mult is not None:
            grad *= self.loss_mult
        return grad


class Grad3DiTV(torch.nn.Module):
    """
    N-D gradient loss.
    """
    def __init__(self):
        super(Grad3DiTV, self).__init__()
        a = 1

    def forward(self, y_pred, y_true):
        dy = torch.abs(y_pred[:, :, 1:, 1:, 1:] - y_pred[:, :, :-1, 1:, 1:])
        dx = torch.abs(y_pred[:, :, 1:, 1:, 1:] - y_pred[:, :, 1:, :-1, 1:])
        dz = torch.abs(y_pred[:, :, 1:, 1:, 1:] - y_pred[:, :, 1:, 1:, :-1])
        dy = dy * dy
        dx = dx * dx
        dz = dz * dz
        d = torch.mean(torch.sqrt(dx+dy+dz+1e-6))
        grad = d / 3.0
        return grad
    

class WeightedGrad3d(nn.Module):
    """
    Weighted smoothness: mean( w(x) * ||∇flow||^2 )
    flow:   (B,3,D,H,W)
    weight: (B,1,D,H,W) or (B,D,H,W)
    """
    def __init__(self, penalty='l2'):
        super().__init__()
        assert penalty in ['l1', 'l2']
        self.penalty = penalty

    def forward(self, flow, weight):
        if weight.dim() == 4:
            weight = weight.unsqueeze(1)  # (B,1,D,H,W)

        dy = flow[:, :, 1:, :, :] - flow[:, :, :-1, :, :]
        dx = flow[:, :, :, 1:, :] - flow[:, :, :, :-1, :]
        dz = flow[:, :, :, :, 1:] - flow[:, :, :, :, :-1]

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx
            dz = dz * dz
        else:
            dy = torch.abs(dy)
            dx = torch.abs(dx)
            dz = torch.abs(dz)

        wy = weight[:, :, 1:, :, :]
        wx = weight[:, :, :, 1:, :]
        wz = weight[:, :, :, :, 1:]

        return ((wy * dy).mean() + (wx * dx).mean() + (wz * dz).mean()) / 3.0    


class Grad1ch3d(nn.Module):
    """
    Smoothness for 1-channel map (for lambda map).
    x: (B,1,D,H,W) or (B,D,H,W)
    """
    def __init__(self, penalty='l2'):
        super().__init__()
        assert penalty in ['l1', 'l2']
        self.penalty = penalty

    def forward(self, x):
        if x.dim() == 4:
            x = x.unsqueeze(1)

        dy = x[:, :, 1:, :, :] - x[:, :, :-1, :, :]
        dx = x[:, :, :, 1:, :] - x[:, :, :, :-1, :]
        dz = x[:, :, :, :, 1:] - x[:, :, :, :, :-1]

        if self.penalty == 'l2':
            dy = dy * dy
            dx = dx * dx
            dz = dz * dz
        else:
            dy = torch.abs(dy)
            dx = torch.abs(dx)
            dz = torch.abs(dz)

        return (dy.mean() + dx.mean() + dz.mean()) / 3.0


class DisplacementRegularizer(torch.nn.Module):
    def __init__(self, energy_type):
        super().__init__()
        self.energy_type = energy_type

    def gradient_dx(self, fv): return (fv[:, 2:, 1:-1, 1:-1] - fv[:, :-2, 1:-1, 1:-1]) / 2

    def gradient_dy(self, fv): return (fv[:, 1:-1, 2:, 1:-1] - fv[:, 1:-1, :-2, 1:-1]) / 2

    def gradient_dz(self, fv): return (fv[:, 1:-1, 1:-1, 2:] - fv[:, 1:-1, 1:-1, :-2]) / 2

    def gradient_txyz(self, Txyz, fn):
        return torch.stack([fn(Txyz[:,i,...]) for i in [0, 1, 2]], dim=1)

    def compute_gradient_norm(self, displacement, flag_l1=False):
        dTdx = self.gradient_txyz(displacement, self.gradient_dx)
        dTdy = self.gradient_txyz(displacement, self.gradient_dy)
        dTdz = self.gradient_txyz(displacement, self.gradient_dz)
        if flag_l1:
            norms = torch.abs(dTdx) + torch.abs(dTdy) + torch.abs(dTdz)
        else:
            norms = dTdx**2 + dTdy**2 + dTdz**2
        return torch.mean(norms)/3.0

    def compute_bending_energy(self, displacement):
        dTdx = self.gradient_txyz(displacement, self.gradient_dx)
        dTdy = self.gradient_txyz(displacement, self.gradient_dy)
        dTdz = self.gradient_txyz(displacement, self.gradient_dz)
        dTdxx = self.gradient_txyz(dTdx, self.gradient_dx)
        dTdyy = self.gradient_txyz(dTdy, self.gradient_dy)
        dTdzz = self.gradient_txyz(dTdz, self.gradient_dz)
        dTdxy = self.gradient_txyz(dTdx, self.gradient_dy)
        dTdyz = self.gradient_txyz(dTdy, self.gradient_dz)
        dTdxz = self.gradient_txyz(dTdx, self.gradient_dz)
        return torch.mean(dTdxx**2 + dTdyy**2 + dTdzz**2 + 2*dTdxy**2 + 2*dTdxz**2 + 2*dTdyz**2)

    def forward(self, disp, _):
        if self.energy_type == 'bending':
            energy = self.compute_bending_energy(disp)
        elif self.energy_type == 'gradient-l2':
            energy = self.compute_gradient_norm(disp)
        elif self.energy_type == 'gradient-l1':
            energy = self.compute_gradient_norm(disp, flag_l1=True)
        else:
            raise Exception('Not recognised local regulariser!')
        return energy


class DiceLoss(nn.Module):
    """Dice and Xentropy loss"""
    def __init__(self, num_class=36, if_onehot=False):
        super().__init__()
        self.num_class = num_class
        self.if_onehot = if_onehot

    def forward(self, y_pred, y_true):
        if self.if_onehot:
            y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)
            y_true = torch.squeeze(y_true, 1)
            y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()
        intersection = y_pred * y_true
        intersection = intersection.sum(dim=[2, 3, 4])
        union = torch.pow(y_pred, 1).sum(dim=[2, 3, 4]) + torch.pow(y_true, 1).sum(dim=[2, 3, 4])
        dsc = (2.*intersection) / (union + 1e-5)
        dsc = (1-torch.mean(dsc))
        return dsc


class NCC_vxm(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss.
    """
    def __init__(self, win=None, eps: float = 1e-5):
        super().__init__()
        self.win = win
        self.eps = float(eps)

    def forward(self, y_true, y_pred):
        Ii = y_true
        Ji = y_pred

        ndims = len(Ii.shape) - 2
        assert ndims in (1, 2, 3)

        win = [9] * ndims if self.win is None else list(self.win)
        pad_no = win[0] // 2

        if ndims == 1:
            stride, padding = (1,), (pad_no,)
        elif ndims == 2:
            stride, padding = (1, 1), (pad_no, pad_no)
        else:
            stride, padding = (1, 1, 1), (pad_no, pad_no, pad_no)

        conv_fn = getattr(F, f"conv{ndims}d")

        # FILTER MUST MATCH DEVICE+DTYPE
        sum_filt = torch.ones((1, 1, *win), device=Ii.device, dtype=Ii.dtype)

        I2 = Ii * Ii
        J2 = Ji * Ji
        IJ = Ii * Ji

        I_sum  = conv_fn(Ii, sum_filt, stride=stride, padding=padding)
        J_sum  = conv_fn(Ji, sum_filt, stride=stride, padding=padding)
        I2_sum = conv_fn(I2, sum_filt, stride=stride, padding=padding)
        J2_sum = conv_fn(J2, sum_filt, stride=stride, padding=padding)
        IJ_sum = conv_fn(IJ, sum_filt, stride=stride, padding=padding)

        win_size = float(np.prod(win))
        u_I = I_sum / win_size
        u_J = J_sum / win_size

        cross = IJ_sum - u_J * I_sum - u_I * J_sum + u_I * u_J * win_size
        I_var = I2_sum - 2 * u_I * I_sum + u_I * u_I * win_size
        J_var = J2_sum - 2 * u_J * J_sum + u_J * u_J * win_size

        # CRITICAL: avoid negative/zero variances
        I_var = torch.clamp(I_var, min=self.eps)
        J_var = torch.clamp(J_var, min=self.eps)

        cc = (cross * cross) / (I_var * J_var)
        return -torch.mean(cc)

class NCC(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss.
    """
    def __init__(self, win=None):
        super(NCC, self).__init__()
        self.win = win

    def forward(self, y_true, y_pred):
        Ii = y_true#/100
        Ji = y_pred#/100

        ndims = len(list(Ii.size())) - 2
        assert ndims in [1, 2, 3], "volumes should be 1 to 3 dimensions. found: %d" % ndims
        win = [9] * ndims if self.win is None else [self.win] * ndims
        sum_filt = torch.ones([1, 1, *win]).to("cuda")/float(np.prod(win))
        pad_no = win[0] // 2

        if ndims == 1:
            stride = (1)
            padding = (pad_no)
        elif ndims == 2:
            stride = (1, 1)
            padding = (pad_no, pad_no)
        else:
            stride = (1, 1, 1)
            padding = (pad_no, pad_no, pad_no)

        conv_fn = getattr(F, 'conv%dd' % ndims)

        mu1 = conv_fn(Ii, sum_filt, padding=padding, stride=stride)
        mu2 = conv_fn(Ji, sum_filt, padding=padding, stride=stride)

        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2

        sigma1_sq = conv_fn(Ii * Ii, sum_filt, padding=padding, stride=stride) - mu1_sq
        sigma2_sq = conv_fn(Ji * Ji, sum_filt, padding=padding, stride=stride) - mu2_sq
        sigma12 = conv_fn(Ii * Ji, sum_filt, padding=padding, stride=stride) - mu1_mu2

        cc = (sigma12 * sigma12) / (sigma1_sq * sigma2_sq + 1e-5)
        return - torch.mean(cc)


class MIND_loss(torch.nn.Module):
    """
    Local (over window) normalized cross correlation loss.
    """
    def __init__(self, win=None):
        super(MIND_loss, self).__init__()
        self.win = win

    def pdist_squared(self, x):
        xx = (x ** 2).sum(dim=1).unsqueeze(2)
        yy = xx.permute(0, 2, 1)
        dist = xx + yy - 2.0 * torch.bmm(x.permute(0, 2, 1), x)
        dist[dist != dist] = 0
        dist = torch.clamp(dist, 0.0, np.inf)
        return dist

    def MINDSSC(self, img, radius=2, dilation=2):
        # see http://mpheinrich.de/pub/miccai2013_943_mheinrich.pdf for details on the MIND-SSC descriptor

        # kernel size
        kernel_size = radius * 2 + 1

        # define start and end locations for self-similarity pattern
        six_neighbourhood = torch.Tensor([[0, 1, 1],
                                          [1, 1, 0],
                                          [1, 0, 1],
                                          [1, 1, 2],
                                          [2, 1, 1],
                                          [1, 2, 1]]).long()

        # squared distances
        dist = self.pdist_squared(six_neighbourhood.t().unsqueeze(0)).squeeze(0)

        # define comparison mask
        x, y = torch.meshgrid(torch.arange(6), torch.arange(6), indexing='ij')
        mask = ((x > y).view(-1) & (dist == 2).view(-1))

        # build kernel
        idx_shift1 = six_neighbourhood.unsqueeze(1).repeat(1, 6, 1).view(-1, 3)[mask, :]
        idx_shift2 = six_neighbourhood.unsqueeze(0).repeat(6, 1, 1).view(-1, 3)[mask, :]
        mshift1 = torch.zeros(12, 1, 3, 3, 3).cuda()
        mshift1.view(-1)[torch.arange(12) * 27 + idx_shift1[:, 0] * 9 + idx_shift1[:, 1] * 3 + idx_shift1[:, 2]] = 1
        mshift2 = torch.zeros(12, 1, 3, 3, 3).cuda()
        mshift2.view(-1)[torch.arange(12) * 27 + idx_shift2[:, 0] * 9 + idx_shift2[:, 1] * 3 + idx_shift2[:, 2]] = 1
        rpad1 = nn.ReplicationPad3d(dilation)
        rpad2 = nn.ReplicationPad3d(radius)

        # compute patch-ssd
        ssd = F.avg_pool3d(rpad2(
            (F.conv3d(rpad1(img), mshift1, dilation=dilation) - F.conv3d(rpad1(img), mshift2, dilation=dilation)) ** 2),
                           kernel_size, stride=1)

        # MIND equation
        mind = ssd - torch.min(ssd, 1, keepdim=True)[0]
        mind_var = torch.mean(mind, 1, keepdim=True)
        mind_var = torch.clamp(mind_var, (mind_var.mean() * 0.001).item(), (mind_var.mean() * 1000).item())
        mind /= mind_var
        mind = torch.exp(-mind)

        # permute to have same ordering as C++ code
        mind = mind[:, torch.Tensor([6, 8, 1, 11, 2, 10, 0, 7, 9, 4, 5, 3]).long(), :, :, :]

        return mind

    def forward(self, y_pred, y_true):
        return torch.mean((self.MINDSSC(y_pred) - self.MINDSSC(y_true)) ** 2)


class MutualInformation(torch.nn.Module):
    """
    Mutual Information
    """
    def __init__(self, sigma_ratio=1, minval=0., maxval=1., num_bin=32):
        super(MutualInformation, self).__init__()

        """Create bin centers"""
        bin_centers = np.linspace(minval, maxval, num=num_bin)
        vol_bin_centers = Variable(torch.linspace(minval, maxval, num_bin), requires_grad=False).cuda()
        num_bins = len(bin_centers)

        """Sigma for Gaussian approx."""
        sigma = np.mean(np.diff(bin_centers)) * sigma_ratio
        print(sigma)

        self.preterm = 1 / (2 * sigma ** 2)
        self.bin_centers = bin_centers
        self.max_clip = maxval
        self.num_bins = num_bins
        self.vol_bin_centers = vol_bin_centers

    def mi(self, y_true, y_pred):
        y_pred = torch.clamp(y_pred, 0., self.max_clip)
        y_true = torch.clamp(y_true, 0, self.max_clip)

        y_true = y_true.view(y_true.shape[0], -1)
        y_true = torch.unsqueeze(y_true, 2)
        y_pred = y_pred.view(y_pred.shape[0], -1)
        y_pred = torch.unsqueeze(y_pred, 2)

        nb_voxels = y_pred.shape[1]  # total num of voxels

        """Reshape bin centers"""
        o = [1, 1, np.prod(self.vol_bin_centers.shape)]
        vbc = torch.reshape(self.vol_bin_centers, o).cuda()

        """compute image terms by approx. Gaussian dist."""
        I_a = torch.exp(- self.preterm * torch.square(y_true - vbc))
        I_a = I_a / torch.sum(I_a, dim=-1, keepdim=True)

        I_b = torch.exp(- self.preterm * torch.square(y_pred - vbc))
        I_b = I_b / torch.sum(I_b, dim=-1, keepdim=True)

        # compute probabilities
        pab = torch.bmm(I_a.permute(0, 2, 1), I_b)
        pab = pab / nb_voxels
        pa = torch.mean(I_a, dim=1, keepdim=True)
        pb = torch.mean(I_b, dim=1, keepdim=True)

        papb = torch.bmm(pa.permute(0, 2, 1), pb) + 1e-6
        mi = torch.sum(torch.sum(pab * torch.log(pab / papb + 1e-6), dim=1), dim=1)
        return mi.mean()  # average across batch

    def forward(self, y_true, y_pred):
        return -self.mi(y_true, y_pred)


class localMutualInformation(torch.nn.Module):
    """
    Local Mutual Information for non-overlapping patches
    """
    def __init__(self, sigma_ratio=1, minval=0., maxval=1., num_bin=32, patch_size=5):
        super(localMutualInformation, self).__init__()

        """Create bin centers"""
        bin_centers = np.linspace(minval, maxval, num=num_bin)
        vol_bin_centers = Variable(torch.linspace(minval, maxval, num_bin), requires_grad=False).cuda()
        num_bins = len(bin_centers)

        """Sigma for Gaussian approx."""
        sigma = np.mean(np.diff(bin_centers)) * sigma_ratio

        self.preterm = 1 / (2 * sigma ** 2)
        self.bin_centers = bin_centers
        self.max_clip = maxval
        self.num_bins = num_bins
        self.vol_bin_centers = vol_bin_centers
        self.patch_size = patch_size

    def local_mi(self, y_true, y_pred):
        y_pred = torch.clamp(y_pred, 0., self.max_clip)
        y_true = torch.clamp(y_true, 0, self.max_clip)

        """Reshape bin centers"""
        o = [1, 1, np.prod(self.vol_bin_centers.shape)]
        vbc = torch.reshape(self.vol_bin_centers, o).cuda()

        """Making image paddings"""
        if len(list(y_pred.size())[2:]) == 3:
            ndim = 3
            x, y, z = list(y_pred.size())[2:]
            # compute padding sizes
            x_r = -x % self.patch_size
            y_r = -y % self.patch_size
            z_r = -z % self.patch_size
            padding = (z_r // 2, z_r - z_r // 2, y_r // 2, y_r - y_r // 2, x_r // 2, x_r - x_r // 2, 0, 0, 0, 0)
        elif len(list(y_pred.size())[2:]) == 2:
            ndim = 2
            x, y = list(y_pred.size())[2:]
            # compute padding sizes
            x_r = -x % self.patch_size
            y_r = -y % self.patch_size
            padding = (y_r // 2, y_r - y_r // 2, x_r // 2, x_r - x_r // 2, 0, 0, 0, 0)
        else:
            raise Exception('Supports 2D and 3D but not {}'.format(list(y_pred.size())))
        y_true = F.pad(y_true, padding, "constant", 0)
        y_pred = F.pad(y_pred, padding, "constant", 0)

        """Reshaping images into non-overlapping patches"""
        if ndim == 3:
            y_true_patch = torch.reshape(y_true, (y_true.shape[0], y_true.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size,
                                                  (z + z_r) // self.patch_size, self.patch_size))
            y_true_patch = y_true_patch.permute(0, 1, 2, 4, 6, 3, 5, 7)
            y_true_patch = torch.reshape(y_true_patch, (-1, self.patch_size ** 3, 1))

            y_pred_patch = torch.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size,
                                                  (z + z_r) // self.patch_size, self.patch_size))
            y_pred_patch = y_pred_patch.permute(0, 1, 2, 4, 6, 3, 5, 7)
            y_pred_patch = torch.reshape(y_pred_patch, (-1, self.patch_size ** 3, 1))
        else:
            y_true_patch = torch.reshape(y_true, (y_true.shape[0], y_true.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size))
            y_true_patch = y_true_patch.permute(0, 1, 2, 4, 3, 5)
            y_true_patch = torch.reshape(y_true_patch, (-1, self.patch_size ** 2, 1))

            y_pred_patch = torch.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1],
                                                  (x + x_r) // self.patch_size, self.patch_size,
                                                  (y + y_r) // self.patch_size, self.patch_size))
            y_pred_patch = y_pred_patch.permute(0, 1, 2, 4, 3, 5)
            y_pred_patch = torch.reshape(y_pred_patch, (-1, self.patch_size ** 2, 1))

        """Compute MI"""
        I_a_patch = torch.exp(- self.preterm * torch.square(y_true_patch - vbc))
        I_a_patch = I_a_patch / torch.sum(I_a_patch, dim=-1, keepdim=True)

        I_b_patch = torch.exp(- self.preterm * torch.square(y_pred_patch - vbc))
        I_b_patch = I_b_patch / torch.sum(I_b_patch, dim=-1, keepdim=True)

        pab = torch.bmm(I_a_patch.permute(0, 2, 1), I_b_patch)
        pab = pab / self.patch_size ** ndim
        pa = torch.mean(I_a_patch, dim=1, keepdim=True)
        pb = torch.mean(I_b_patch, dim=1, keepdim=True)

        papb = torch.bmm(pa.permute(0, 2, 1), pb) + 1e-6
        mi = torch.sum(torch.sum(pab * torch.log(pab / papb + 1e-6), dim=1), dim=1)
        return mi.mean()

    def forward(self, y_true, y_pred):
        return -self.local_mi(y_true, y_pred)
################################################################################
# FILE: utils/misc.py
# SIZE: 3154 bytes
################################################################################

import torch
from torch import nn


def get_mc_preds(net, inputs, mc_iter: int = 25):
    img_list, flow_list = [], []
    with torch.no_grad():
        for _ in range(mc_iter):
            img, flow = net(inputs)
            img_list.append(img)
            flow_list.append(flow)
    return img_list, flow_list


def calc_error(tar, img_list):
    sqr_diffs = [(img - tar) ** 2 for img in img_list]
    return torch.mean(torch.cat(sqr_diffs, dim=0), dim=0, keepdim=True)


def get_mc_preds_w_errors(net, inputs, target, mc_iter: int = 25):
    img_list, flow_list, err = [], [], []
    mse = nn.MSELoss()
    with torch.no_grad():
        for _ in range(mc_iter):
            img, flow = net(inputs)
            img_list.append(img)
            flow_list.append(flow)
            err.append(mse(img, target).item())
    return img_list, flow_list, err


def get_diff_mc_preds(net, inputs, mc_iter: int = 25):
    img_list, flow_list, disp_list = [], [], []
    with torch.no_grad():
        for _ in range(mc_iter):
            img, _, flow, disp = net(inputs)
            img_list.append(img)
            flow_list.append(flow)
            disp_list.append(disp)
    return img_list, flow_list, disp_list


def uncert_regression_gal(img_list, reduction="mean"):
    img_list = torch.cat(img_list, dim=0)
    ale = img_list[:, -1:].mean(dim=0, keepdim=True)
    epi = torch.var(img_list[:, :-1], dim=0, keepdim=True).mean(dim=1, keepdim=True)
    uncert = ale + epi
    if reduction == "mean":
        return ale.mean().item(), epi.mean().item(), uncert.mean().item()
    if reduction == "sum":
        return ale.sum().item(), epi.sum().item(), uncert.sum().item()
    return ale.detach(), epi.detach(), uncert.detach()


def uceloss(errors, uncert, n_bins=15, outlier=0.0, range=None):
    device = errors.device
    if range is None:
        bin_boundaries = torch.linspace(uncert.min().item(), uncert.max().item(), n_bins + 1, device=device)
    else:
        bin_boundaries = torch.linspace(range[0], range[1], n_bins + 1, device=device)

    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]

    uce = torch.zeros(1, device=device)
    errors_in_bin_list, avg_uncert_in_bin_list, prop_in_bin_list = [], [], []

    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = uncert.gt(bin_lower.item()) * uncert.le(bin_upper.item())
        prop_in_bin = in_bin.float().mean()
        prop_in_bin_list.append(prop_in_bin)
        if prop_in_bin.item() > outlier:
            errors_in_bin = errors[in_bin].float().mean()
            avg_uncert_in_bin = uncert[in_bin].mean()
            uce += torch.abs(avg_uncert_in_bin - errors_in_bin) * prop_in_bin
            errors_in_bin_list.append(errors_in_bin)
            avg_uncert_in_bin_list.append(avg_uncert_in_bin)

    err_in_bin = torch.tensor(errors_in_bin_list, device=device)
    avg_uncert_in_bin = torch.tensor(avg_uncert_in_bin_list, device=device)
    prop_in_bin = torch.tensor(prop_in_bin_list, device=device)
    return uce, err_in_bin, avg_uncert_in_bin, prop_in_bin
################################################################################
# FILE: utils/rand.py
# SIZE: 507 bytes
################################################################################

import random


class Uniform(object):
    def __init__(self, a, b):
        self.a = a
        self.b = b

    def sample(self):
        return random.uniform(self.a, self.b)


class Gaussian(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def sample(self):
        return random.gauss(self.mean, self.std)


class Constant(object):
    def __init__(self, val):
        self.val = val

    def sample(self):
        return self.val
################################################################################
# FILE: utils/spatial.py
# SIZE: 2644 bytes
################################################################################

import numpy as np
import torch
import torch.nn.functional as F
from torch import nn
import pystrum.pynd.ndutils as nd


class SpatialTransformer(nn.Module):
    """
    Double of ST from TM-DCA model.
    """
    def __init__(self, size, mode: str = "bilinear"):
        super().__init__()
        self.mode = mode

        vectors = [torch.arange(0, s) for s in size]
        grids = torch.meshgrid(vectors, indexing="ij")
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)
        self.register_buffer("grid", grid, persistent=False)

    def forward(self, src: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:
        new_locs = self.grid + flow
        shape = flow.shape[2:]

        # normalize to [-1, 1]
        for i in range(len(shape)):
            new_locs[:, i, ...] = 2.0 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)

        if len(shape) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(shape) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return F.grid_sample(src, new_locs, align_corners=False, mode=self.mode)


class register_model(nn.Module):
    """
    Wrapper used in old trainers: forward([img, flow]) -> warped_img.
    """
    def __init__(self, img_size=(64, 256, 256), mode="bilinear"):
        super().__init__()
        self.spatial_trans = SpatialTransformer(img_size, mode)

    def forward(self, x):
        img, flow = x[0], x[1]
        return self.spatial_trans(img, flow)


def jacobian_determinant_vxm(disp: np.ndarray) -> np.ndarray:
    """
    Jacobian determinant of a displacement field (numpy version, VXM-style).
    disp: [*vol_shape, nb_dims] OR with transpose needed.
    """
    disp = disp.transpose(1, 2, 3, 0)
    volshape = disp.shape[:-1]
    nb_dims = len(volshape)
    assert nb_dims in (2, 3), "flow has to be 2D or 3D"

    grid_lst = nd.volsize2ndgrid(volshape)
    grid = np.stack(grid_lst, len(volshape))
    J = np.gradient(disp + grid)

    if nb_dims == 3:
        dx, dy, dz = J[0], J[1], J[2]
        Jdet0 = dx[..., 0] * (dy[..., 1] * dz[..., 2] - dy[..., 2] * dz[..., 1])
        Jdet1 = dx[..., 1] * (dy[..., 0] * dz[..., 2] - dy[..., 2] * dz[..., 0])
        Jdet2 = dx[..., 2] * (dy[..., 0] * dz[..., 1] - dy[..., 1] * dz[..., 0])
        return Jdet0 - Jdet1 + Jdet2
    else:
        dfdx, dfdy = J[0], J[1]
        return dfdx[..., 0] * dfdy[..., 1] - dfdy[..., 0] * dfdx[..., 1]
################################################################################
# FILE: utils/surface_distance/__init__.py
# SIZE: 771 bytes
################################################################################

# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS-IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Surface distance module: https://github.com/deepmind/surface-distance ."""

from .metrics import *  # pylint: disable=wildcard-import
__version__ = "0.1"

################################################################################
# FILE: utils/surface_distance/lookup_tables.py
# SIZE: 23158 bytes
################################################################################

# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS-IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Lookup tables used by surface distance metrics."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import numpy as np

ENCODE_NEIGHBOURHOOD_3D_KERNEL = np.array([[[128, 64], [32, 16]], [[8, 4],
                                                                   [2, 1]]])

# _NEIGHBOUR_CODE_TO_NORMALS is a lookup table.
# For every binary neighbour code
# (2x2x2 neighbourhood = 8 neighbours = 8 bits = 256 codes)
# it contains the surface normals of the triangles (called "surfel" for
# "surface element" in the following). The length of the normal
# vector encodes the surfel area.
#
# created using the marching_cube algorithm
# see e.g. https://en.wikipedia.org/wiki/Marching_cubes
# pylint: disable=line-too-long
_NEIGHBOUR_CODE_TO_NORMALS = [
    [[0, 0, 0]],
    [[0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125]],
    [[-0.5, 0.0, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[0.5, 0.0, 0.0], [0.5, 0.0, 0.0]],
    [[0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.5, 0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, 0.25], [-0.125, -0.125, -0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [0.125, -0.125, -0.125]],
    [[0.125, 0.125, 0.125], [0.375, 0.375, 0.375], [0.0, -0.25, 0.25], [-0.25, 0.0, 0.25]],
    [[0.125, -0.125, -0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.375, 0.375, 0.375], [0.0, 0.25, -0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.125, 0.125, 0.125]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25]],
    [[0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.375, -0.375, 0.375], [-0.0, 0.25, 0.25], [0.125, 0.125, -0.125], [-0.25, -0.0, -0.25]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.25, 0.25, -0.25], [0.25, 0.25, -0.25], [0.125, 0.125, -0.125], [-0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125], [0.125, -0.125, 0.125]],
    [[0.0, 0.25, -0.25], [0.375, -0.375, -0.375], [-0.125, 0.125, 0.125], [0.25, 0.25, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[0.0, 0.5, 0.0], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.0, 0.5, 0.0], [0.125, -0.125, 0.125], [-0.25, 0.25, -0.25]],
    [[0.0, 0.5, 0.0], [0.0, -0.5, 0.0]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0], [0.125, -0.125, 0.125]],
    [[-0.375, -0.375, -0.375], [-0.25, 0.0, 0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[0.125, 0.125, 0.125], [0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[-0.125, 0.125, 0.125], [0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.375, 0.375, -0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.375, -0.375, 0.375], [0.0, -0.25, -0.25], [-0.125, 0.125, -0.125], [0.25, 0.25, 0.0]],
    [[-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.25, 0.25, -0.25], [-0.25, 0.25, -0.25], [-0.125, 0.125, -0.125], [-0.125, 0.125, -0.125]],
    [[-0.25, 0.0, -0.25], [0.375, -0.375, -0.375], [0.0, 0.25, -0.25], [-0.125, 0.125, 0.125]],
    [[0.5, 0.0, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[-0.0, 0.0, 0.5], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[-0.25, -0.0, -0.25], [-0.375, 0.375, 0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, 0.125]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25], [-0.125, 0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[0.125, -0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[0.25, 0.0, 0.25], [-0.375, -0.375, 0.375], [-0.25, 0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.5, 0.0, -0.0], [0.25, -0.25, -0.25], [0.125, -0.125, -0.125]],
    [[-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.375, -0.375, 0.375], [0.0, 0.25, 0.25], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[0.0, -0.5, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.375, -0.375, 0.375], [0.25, -0.25, 0.0], [0.0, 0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125], [0.125, 0.125, 0.125]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.125, 0.125, 0.125], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[-0.375, -0.375, 0.375], [0.25, -0.25, 0.0], [0.0, 0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.0, -0.5, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[0.375, -0.375, 0.375], [0.0, 0.25, 0.25], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.5, 0.0, -0.0], [0.25, -0.25, -0.25], [0.125, -0.125, -0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.0, 0.25, 0.25], [0.0, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25], [0.0, 0.25, 0.25]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[0.125, 0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.25, 0.0, 0.25], [-0.375, -0.375, 0.375], [-0.25, 0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25], [0.25, 0.0, 0.25]],
    [[-0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25], [-0.125, 0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[0.125, 0.125, 0.125], [0.125, 0.125, 0.125], [0.25, 0.25, 0.25], [0.0, 0.0, 0.5]],
    [[-0.0, 0.0, 0.5], [0.0, 0.0, 0.5]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [-0.375, 0.375, 0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[-0.0, 0.0, 0.5], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [0.25, 0.0, -0.25]],
    [[0.5, 0.0, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[-0.25, 0.0, -0.25], [0.375, -0.375, -0.375], [0.0, 0.25, -0.25], [-0.125, 0.125, 0.125]],
    [[-0.25, 0.25, -0.25], [-0.25, 0.25, -0.25], [-0.125, 0.125, -0.125], [-0.125, 0.125, -0.125]],
    [[-0.0, 0.5, 0.0], [-0.25, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[0.375, -0.375, 0.375], [0.0, -0.25, -0.25], [-0.125, 0.125, -0.125], [0.25, 0.25, 0.0]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.0, 0.0, 0.5], [0.25, -0.25, 0.25], [0.125, -0.125, 0.125]],
    [[0.0, -0.25, 0.25], [0.0, -0.25, 0.25]],
    [[-0.125, -0.125, 0.125], [-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[0.125, 0.125, 0.125], [-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125]],
    [[-0.375, 0.375, -0.375], [-0.25, -0.25, 0.0], [-0.125, 0.125, -0.125], [-0.25, 0.0, 0.25]],
    [[0.0, 0.5, 0.0], [0.25, 0.25, -0.25], [-0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[0.125, 0.125, 0.125], [0.0, -0.5, 0.0], [-0.25, -0.25, -0.25], [-0.125, -0.125, -0.125]],
    [[-0.375, -0.375, -0.375], [-0.25, 0.0, 0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0], [0.125, -0.125, 0.125]],
    [[0.0, 0.5, 0.0], [0.0, -0.5, 0.0]],
    [[0.0, 0.5, 0.0], [0.125, -0.125, 0.125], [-0.25, 0.25, -0.25]],
    [[0.0, 0.5, 0.0], [-0.25, 0.25, 0.25], [0.125, -0.125, -0.125]],
    [[0.25, -0.25, 0.0], [-0.25, 0.25, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.0, 0.25, -0.25], [0.375, -0.375, -0.375], [-0.125, 0.125, 0.125], [0.25, 0.25, 0.0]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.25, 0.25, -0.25], [0.25, 0.25, -0.25], [0.125, 0.125, -0.125], [-0.125, -0.125, 0.125]],
    [[-0.0, 0.0, 0.5], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[-0.375, -0.375, 0.375], [-0.0, 0.25, 0.25], [0.125, 0.125, -0.125], [-0.25, -0.0, -0.25]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.0, -0.5, 0.0], [0.125, 0.125, -0.125], [0.25, 0.25, -0.25]],
    [[0.0, -0.25, 0.25], [0.0, 0.25, -0.25]],
    [[0.125, 0.125, 0.125], [0.125, -0.125, 0.125]],
    [[0.125, -0.125, 0.125]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25]],
    [[-0.5, 0.0, 0.0], [-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.125, 0.125, 0.125]],
    [[0.375, 0.375, 0.375], [0.0, 0.25, -0.25], [-0.125, -0.125, -0.125], [-0.25, 0.25, 0.0]],
    [[0.125, -0.125, -0.125], [0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.125, 0.125, 0.125], [0.375, 0.375, 0.375], [0.0, -0.25, 0.25], [-0.25, 0.0, 0.25]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125], [0.125, -0.125, -0.125]],
    [[-0.125, -0.125, -0.125], [-0.25, -0.25, -0.25], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, 0.0, -0.5], [0.25, 0.25, 0.25], [-0.125, -0.125, -0.125]],
    [[0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.5, 0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[-0.125, -0.125, 0.125], [0.125, -0.125, -0.125]],
    [[0.0, -0.25, -0.25], [0.0, 0.25, 0.25]],
    [[0.125, -0.125, -0.125]],
    [[0.5, 0.0, 0.0], [0.5, 0.0, 0.0]],
    [[-0.5, 0.0, 0.0], [-0.25, 0.25, 0.25], [-0.125, 0.125, 0.125]],
    [[0.5, 0.0, 0.0], [0.25, -0.25, 0.25], [-0.125, 0.125, -0.125]],
    [[0.25, -0.25, 0.0], [0.25, -0.25, 0.0]],
    [[0.5, 0.0, 0.0], [-0.25, -0.25, 0.25], [-0.125, -0.125, 0.125]],
    [[-0.25, 0.0, 0.25], [-0.25, 0.0, 0.25]],
    [[0.125, 0.125, 0.125], [-0.125, 0.125, 0.125]],
    [[-0.125, 0.125, 0.125]],
    [[0.5, 0.0, -0.0], [0.25, 0.25, 0.25], [0.125, 0.125, 0.125]],
    [[0.125, -0.125, 0.125], [-0.125, -0.125, 0.125]],
    [[-0.25, -0.0, -0.25], [0.25, 0.0, 0.25]],
    [[0.125, -0.125, 0.125]],
    [[-0.25, -0.25, 0.0], [0.25, 0.25, -0.0]],
    [[-0.125, -0.125, 0.125]],
    [[0.125, 0.125, 0.125]],
    [[0, 0, 0]]]
# pylint: enable=line-too-long


def create_table_neighbour_code_to_surface_area(spacing_mm):
  """Returns an array mapping neighbourhood code to the surface elements area.

  Note that the normals encode the initial surface area. This function computes
  the area corresponding to the given `spacing_mm`.

  Args:
    spacing_mm: 3-element list-like structure. Voxel spacing in x0, x1 and x2
      direction.
  """
  # compute the area for all 256 possible surface elements
  # (given a 2x2x2 neighbourhood) according to the spacing_mm
  neighbour_code_to_surface_area = np.zeros([256])
  for code in range(256):
    normals = np.array(_NEIGHBOUR_CODE_TO_NORMALS[code])
    sum_area = 0
    for normal_idx in range(normals.shape[0]):
      # normal vector
      n = np.zeros([3])
      n[0] = normals[normal_idx, 0] * spacing_mm[1] * spacing_mm[2]
      n[1] = normals[normal_idx, 1] * spacing_mm[0] * spacing_mm[2]
      n[2] = normals[normal_idx, 2] * spacing_mm[0] * spacing_mm[1]
      area = np.linalg.norm(n)
      sum_area += area
    neighbour_code_to_surface_area[code] = sum_area

  return neighbour_code_to_surface_area


# In the neighbourhood, points are ordered: top left, top right, bottom left,
# bottom right.
ENCODE_NEIGHBOURHOOD_2D_KERNEL = np.array([[8, 4], [2, 1]])


def create_table_neighbour_code_to_contour_length(spacing_mm):
  """Returns an array mapping neighbourhood code to the contour length.

  For the list of possible cases and their figures, see page 38 from:
  https://nccastaff.bournemouth.ac.uk/jmacey/MastersProjects/MSc14/06/thesis.pdf

  In 2D, each point has 4 neighbors. Thus, are 16 configurations. A
  configuration is encoded with '1' meaning "inside the object" and '0' "outside
  the object". The points are ordered: top left, top right, bottom left, bottom
  right.

  The x0 axis is assumed vertical downward, and the x1 axis is horizontal to the
  right:
   (0, 0) --> (0, 1)
     |
   (1, 0)

  Args:
    spacing_mm: 2-element list-like structure. Voxel spacing in x0 and x1
      directions.
  """
  neighbour_code_to_contour_length = np.zeros([16])

  vertical = spacing_mm[0]
  horizontal = spacing_mm[1]
  diag = 0.5 * math.sqrt(spacing_mm[0]**2 + spacing_mm[1]**2)
  # pyformat: disable
  neighbour_code_to_contour_length[int("00"
                                       "01", 2)] = diag

  neighbour_code_to_contour_length[int("00"
                                       "10", 2)] = diag

  neighbour_code_to_contour_length[int("00"
                                       "11", 2)] = horizontal

  neighbour_code_to_contour_length[int("01"
                                       "00", 2)] = diag

  neighbour_code_to_contour_length[int("01"
                                       "01", 2)] = vertical

  neighbour_code_to_contour_length[int("01"
                                       "10", 2)] = 2*diag

  neighbour_code_to_contour_length[int("01"
                                       "11", 2)] = diag

  neighbour_code_to_contour_length[int("10"
                                       "00", 2)] = diag

  neighbour_code_to_contour_length[int("10"
                                       "01", 2)] = 2*diag

  neighbour_code_to_contour_length[int("10"
                                       "10", 2)] = vertical

  neighbour_code_to_contour_length[int("10"
                                       "11", 2)] = diag

  neighbour_code_to_contour_length[int("11"
                                       "00", 2)] = horizontal

  neighbour_code_to_contour_length[int("11"
                                       "01", 2)] = diag

  neighbour_code_to_contour_length[int("11"
                                       "10", 2)] = diag
  # pyformat: enable

  return neighbour_code_to_contour_length

################################################################################
# FILE: utils/surface_distance/metrics.py
# SIZE: 18813 bytes
################################################################################

# Copyright 2018 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS-IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Module exposing surface distance based measures."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from . import lookup_tables  # pylint: disable=relative-beyond-top-level
import numpy as np
from scipy import ndimage


def _assert_is_numpy_array(name, array):
  """Raises an exception if `array` is not a numpy array."""
  if not isinstance(array, np.ndarray):
    raise ValueError("The argument {!r} should be a numpy array, not a "
                     "{}".format(name, type(array)))


def _check_nd_numpy_array(name, array, num_dims):
  """Raises an exception if `array` is not a `num_dims`-D numpy array."""
  if len(array.shape) != num_dims:
    raise ValueError("The argument {!r} should be a {}D array, not of "
                     "shape {}".format(name, num_dims, array.shape))


def _check_2d_numpy_array(name, array):
  _check_nd_numpy_array(name, array, num_dims=2)


def _check_3d_numpy_array(name, array):
  _check_nd_numpy_array(name, array, num_dims=3)


def _assert_is_bool_numpy_array(name, array):
  _assert_is_numpy_array(name, array)
  if array.dtype != np.bool:
    raise ValueError("The argument {!r} should be a numpy array of type bool, "
                     "not {}".format(name, array.dtype))


def _compute_bounding_box(mask):
  """Computes the bounding box of the masks.

  This function generalizes to arbitrary number of dimensions great or equal
  to 1.

  Args:
    mask: The 2D or 3D numpy mask, where '0' means background and non-zero means
      foreground.

  Returns:
    A tuple:
     - The coordinates of the first point of the bounding box (smallest on all
       axes), or `None` if the mask contains only zeros.
     - The coordinates of the second point of the bounding box (greatest on all
       axes), or `None` if the mask contains only zeros.
  """
  num_dims = len(mask.shape)
  bbox_min = np.zeros(num_dims, np.int64)
  bbox_max = np.zeros(num_dims, np.int64)

  # max projection to the x0-axis
  proj_0 = np.amax(mask, axis=tuple(range(num_dims))[1:])
  idx_nonzero_0 = np.nonzero(proj_0)[0]
  if len(idx_nonzero_0) == 0:  # pylint: disable=g-explicit-length-test
    return None, None

  bbox_min[0] = np.min(idx_nonzero_0)
  bbox_max[0] = np.max(idx_nonzero_0)

  # max projection to the i-th-axis for i in {1, ..., num_dims - 1}
  for axis in range(1, num_dims):
    max_over_axes = list(range(num_dims))  # Python 3 compatible
    max_over_axes.pop(axis)  # Remove the i-th dimension from the max
    max_over_axes = tuple(max_over_axes)  # numpy expects a tuple of ints
    proj = np.amax(mask, axis=max_over_axes)
    idx_nonzero = np.nonzero(proj)[0]
    bbox_min[axis] = np.min(idx_nonzero)
    bbox_max[axis] = np.max(idx_nonzero)

  return bbox_min, bbox_max


def _crop_to_bounding_box(mask, bbox_min, bbox_max):
  """Crops a 2D or 3D mask to the bounding box specified by `bbox_{min,max}`."""
  # we need to zeropad the cropped region with 1 voxel at the lower,
  # the right (and the back on 3D) sides. This is required to obtain the
  # "full" convolution result with the 2x2 (or 2x2x2 in 3D) kernel.
  # TODO:  This is correct only if the object is interior to the
  # bounding box.
  cropmask = np.zeros((bbox_max - bbox_min) + 2, np.uint8)

  num_dims = len(mask.shape)
  # pyformat: disable
  if num_dims == 2:
    cropmask[0:-1, 0:-1] = mask[bbox_min[0]:bbox_max[0] + 1,
                                bbox_min[1]:bbox_max[1] + 1]
  elif num_dims == 3:
    cropmask[0:-1, 0:-1, 0:-1] = mask[bbox_min[0]:bbox_max[0] + 1,
                                      bbox_min[1]:bbox_max[1] + 1,
                                      bbox_min[2]:bbox_max[2] + 1]
  # pyformat: enable
  else:
    assert False

  return cropmask


def _sort_distances_surfels(distances, surfel_areas):
  """Sorts the two list with respect to the tuple of (distance, surfel_area).

  Args:
    distances: The distances from A to B (e.g. `distances_gt_to_pred`).
    surfel_areas: The surfel areas for A (e.g. `surfel_areas_gt`).

  Returns:
    A tuple of the sorted (distances, surfel_areas).
  """
  sorted_surfels = np.array(sorted(zip(distances, surfel_areas)))
  return sorted_surfels[:, 0], sorted_surfels[:, 1]


def compute_surface_distances(mask_gt,
                              mask_pred,
                              spacing_mm):
  """Computes closest distances from all surface points to the other surface.

  This function can be applied to 2D or 3D tensors. For 2D, both masks must be
  2D and `spacing_mm` must be a 2-element list. For 3D, both masks must be 3D
  and `spacing_mm` must be a 3-element list. The description is done for the 2D
  case, and the formulation for the 3D case is present is parenthesis,
  introduced by "resp.".

  Finds all contour elements (resp surface elements "surfels" in 3D) in the
  ground truth mask `mask_gt` and the predicted mask `mask_pred`, computes their
  length in mm (resp. area in mm^2) and the distance to the closest point on the
  other contour (resp. surface). It returns two sorted lists of distances
  together with the corresponding contour lengths (resp. surfel areas). If one
  of the masks is empty, the corresponding lists are empty and all distances in
  the other list are `inf`.

  Args:
    mask_gt: 2-dim (resp. 3-dim) bool Numpy array. The ground truth mask.
    mask_pred: 2-dim (resp. 3-dim) bool Numpy array. The predicted mask.
    spacing_mm: 2-element (resp. 3-element) list-like structure. Voxel spacing
      in x0 anx x1 (resp. x0, x1 and x2) directions.

  Returns:
    A dict with:
    "distances_gt_to_pred": 1-dim numpy array of type float. The distances in mm
        from all ground truth surface elements to the predicted surface,
        sorted from smallest to largest.
    "distances_pred_to_gt": 1-dim numpy array of type float. The distances in mm
        from all predicted surface elements to the ground truth surface,
        sorted from smallest to largest.
    "surfel_areas_gt": 1-dim numpy array of type float. The length of the
      of the ground truth contours in mm (resp. the surface elements area in
      mm^2) in the same order as distances_gt_to_pred.
    "surfel_areas_pred": 1-dim numpy array of type float. The length of the
      of the predicted contours in mm (resp. the surface elements area in
      mm^2) in the same order as distances_gt_to_pred.

  Raises:
    ValueError: If the masks and the `spacing_mm` arguments are of incompatible
      shape or type. Or if the masks are not 2D or 3D.
  """
  # The terms used in this function are for the 3D case. In particular, surface
  # in 2D stands for contours in 3D. The surface elements in 3D correspond to
  # the line elements in 2D.

  _assert_is_bool_numpy_array("mask_gt", mask_gt)
  _assert_is_bool_numpy_array("mask_pred", mask_pred)

  if not len(mask_gt.shape) == len(mask_pred.shape) == len(spacing_mm):
    raise ValueError("The arguments must be of compatible shape. Got mask_gt "
                     "with {} dimensions ({}) and mask_pred with {} dimensions "
                     "({}), while the spacing_mm was {} elements.".format(
                         len(mask_gt.shape),
                         mask_gt.shape, len(mask_pred.shape), mask_pred.shape,
                         len(spacing_mm)))

  num_dims = len(spacing_mm)
  if num_dims == 2:
    _check_2d_numpy_array("mask_gt", mask_gt)
    _check_2d_numpy_array("mask_pred", mask_pred)

    # compute the area for all 16 possible surface elements
    # (given a 2x2 neighbourhood) according to the spacing_mm
    neighbour_code_to_surface_area = (
        lookup_tables.create_table_neighbour_code_to_contour_length(spacing_mm))
    kernel = lookup_tables.ENCODE_NEIGHBOURHOOD_2D_KERNEL
    full_true_neighbours = 0b1111
  elif num_dims == 3:
    _check_3d_numpy_array("mask_gt", mask_gt)
    _check_3d_numpy_array("mask_pred", mask_pred)

    # compute the area for all 256 possible surface elements
    # (given a 2x2x2 neighbourhood) according to the spacing_mm
    neighbour_code_to_surface_area = (
        lookup_tables.create_table_neighbour_code_to_surface_area(spacing_mm))
    kernel = lookup_tables.ENCODE_NEIGHBOURHOOD_3D_KERNEL
    full_true_neighbours = 0b11111111
  else:
    raise ValueError("Only 2D and 3D masks are supported, not "
                     "{}D.".format(num_dims))

  # compute the bounding box of the masks to trim the volume to the smallest
  # possible processing subvolume
  bbox_min, bbox_max = _compute_bounding_box(mask_gt | mask_pred)
  # Both the min/max bbox are None at the same time, so we only check one.
  if bbox_min is None:
    return {
        "distances_gt_to_pred": np.array([]),
        "distances_pred_to_gt": np.array([]),
        "surfel_areas_gt": np.array([]),
        "surfel_areas_pred": np.array([]),
    }

  # crop the processing subvolume.
  cropmask_gt = _crop_to_bounding_box(mask_gt, bbox_min, bbox_max)
  cropmask_pred = _crop_to_bounding_box(mask_pred, bbox_min, bbox_max)

  # compute the neighbour code (local binary pattern) for each voxel
  # the resulting arrays are spacially shifted by minus half a voxel in each
  # axis.
  # i.e. the points are located at the corners of the original voxels
  neighbour_code_map_gt = ndimage.filters.correlate(
      cropmask_gt.astype(np.uint8), kernel, mode="constant", cval=0)
  neighbour_code_map_pred = ndimage.filters.correlate(
      cropmask_pred.astype(np.uint8), kernel, mode="constant", cval=0)

  # create masks with the surface voxels
  borders_gt = ((neighbour_code_map_gt != 0) &
                (neighbour_code_map_gt != full_true_neighbours))
  borders_pred = ((neighbour_code_map_pred != 0) &
                  (neighbour_code_map_pred != full_true_neighbours))

  # compute the distance transform (closest distance of each voxel to the
  # surface voxels)
  if borders_gt.any():
    distmap_gt = ndimage.morphology.distance_transform_edt(
        ~borders_gt, sampling=spacing_mm)
  else:
    distmap_gt = np.Inf * np.ones(borders_gt.shape)

  if borders_pred.any():
    distmap_pred = ndimage.morphology.distance_transform_edt(
        ~borders_pred, sampling=spacing_mm)
  else:
    distmap_pred = np.Inf * np.ones(borders_pred.shape)

  # compute the area of each surface element
  surface_area_map_gt = neighbour_code_to_surface_area[neighbour_code_map_gt]
  surface_area_map_pred = neighbour_code_to_surface_area[
      neighbour_code_map_pred]

  # create a list of all surface elements with distance and area
  distances_gt_to_pred = distmap_pred[borders_gt]
  distances_pred_to_gt = distmap_gt[borders_pred]
  surfel_areas_gt = surface_area_map_gt[borders_gt]
  surfel_areas_pred = surface_area_map_pred[borders_pred]

  # sort them by distance
  if distances_gt_to_pred.shape != (0,):
    distances_gt_to_pred, surfel_areas_gt = _sort_distances_surfels(
        distances_gt_to_pred, surfel_areas_gt)

  if distances_pred_to_gt.shape != (0,):
    distances_pred_to_gt, surfel_areas_pred = _sort_distances_surfels(
        distances_pred_to_gt, surfel_areas_pred)

  return {
      "distances_gt_to_pred": distances_gt_to_pred,
      "distances_pred_to_gt": distances_pred_to_gt,
      "surfel_areas_gt": surfel_areas_gt,
      "surfel_areas_pred": surfel_areas_pred,
  }


def compute_average_surface_distance(surface_distances):
  """Returns the average surface distance.

  Computes the average surface distances by correctly taking the area of each
  surface element into account. Call compute_surface_distances(...) before, to
  obtain the `surface_distances` dict.

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
    "surfel_areas_gt", "surfel_areas_pred" created by
    compute_surface_distances()

  Returns:
    A tuple with two float values:
      - the average distance (in mm) from the ground truth surface to the
        predicted surface
      - the average distance from the predicted surface to the ground truth
        surface.
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  average_distance_gt_to_pred = (
      np.sum(distances_gt_to_pred * surfel_areas_gt) / np.sum(surfel_areas_gt))
  average_distance_pred_to_gt = (
      np.sum(distances_pred_to_gt * surfel_areas_pred) /
      np.sum(surfel_areas_pred))
  return (average_distance_gt_to_pred, average_distance_pred_to_gt)


def compute_robust_hausdorff(surface_distances, percent):
  """Computes the robust Hausdorff distance.

  Computes the robust Hausdorff distance. "Robust", because it uses the
  `percent` percentile of the distances instead of the maximum distance. The
  percentage is computed by correctly taking the area of each surface element
  into account.

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
      "surfel_areas_gt", "surfel_areas_pred" created by
      compute_surface_distances()
    percent: a float value between 0 and 100.

  Returns:
    a float value. The robust Hausdorff distance in mm.
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  if len(distances_gt_to_pred) > 0:  # pylint: disable=g-explicit-length-test
    surfel_areas_cum_gt = np.cumsum(surfel_areas_gt) / np.sum(surfel_areas_gt)
    idx = np.searchsorted(surfel_areas_cum_gt, percent/100.0)
    perc_distance_gt_to_pred = distances_gt_to_pred[
        min(idx, len(distances_gt_to_pred)-1)]
  else:
    perc_distance_gt_to_pred = np.Inf

  if len(distances_pred_to_gt) > 0:  # pylint: disable=g-explicit-length-test
    surfel_areas_cum_pred = (np.cumsum(surfel_areas_pred) /
                             np.sum(surfel_areas_pred))
    idx = np.searchsorted(surfel_areas_cum_pred, percent/100.0)
    perc_distance_pred_to_gt = distances_pred_to_gt[
        min(idx, len(distances_pred_to_gt)-1)]
  else:
    perc_distance_pred_to_gt = np.Inf

  return max(perc_distance_gt_to_pred, perc_distance_pred_to_gt)


def compute_surface_overlap_at_tolerance(surface_distances, tolerance_mm):
  """Computes the overlap of the surfaces at a specified tolerance.

  Computes the overlap of the ground truth surface with the predicted surface
  and vice versa allowing a specified tolerance (maximum surface-to-surface
  distance that is regarded as overlapping). The overlapping fraction is
  computed by correctly taking the area of each surface element into account.

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
      "surfel_areas_gt", "surfel_areas_pred" created by
      compute_surface_distances()
    tolerance_mm: a float value. The tolerance in mm

  Returns:
    A tuple of two float values. The overlap fraction in [0.0, 1.0] of the
    ground truth surface with the predicted surface and vice versa.
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  rel_overlap_gt = (
      np.sum(surfel_areas_gt[distances_gt_to_pred <= tolerance_mm]) /
      np.sum(surfel_areas_gt))
  rel_overlap_pred = (
      np.sum(surfel_areas_pred[distances_pred_to_gt <= tolerance_mm]) /
      np.sum(surfel_areas_pred))
  return (rel_overlap_gt, rel_overlap_pred)


def compute_surface_dice_at_tolerance(surface_distances, tolerance_mm):
  """Computes the _surface_ DICE coefficient at a specified tolerance.

  Computes the _surface_ DICE coefficient at a specified tolerance. Not to be
  confused with the standard _volumetric_ DICE coefficient. The surface DICE
  measures the overlap of two surfaces instead of two volumes. A surface
  element is counted as overlapping (or touching), when the closest distance to
  the other surface is less or equal to the specified tolerance. The DICE
  coefficient is in the range between 0.0 (no overlap) to 1.0 (perfect overlap).

  Args:
    surface_distances: dict with "distances_gt_to_pred", "distances_pred_to_gt"
      "surfel_areas_gt", "surfel_areas_pred" created by
      compute_surface_distances()
    tolerance_mm: a float value. The tolerance in mm

  Returns:
    A float value. The surface DICE coefficient in [0.0, 1.0].
  """
  distances_gt_to_pred = surface_distances["distances_gt_to_pred"]
  distances_pred_to_gt = surface_distances["distances_pred_to_gt"]
  surfel_areas_gt = surface_distances["surfel_areas_gt"]
  surfel_areas_pred = surface_distances["surfel_areas_pred"]
  overlap_gt = np.sum(surfel_areas_gt[distances_gt_to_pred <= tolerance_mm])
  overlap_pred = np.sum(surfel_areas_pred[distances_pred_to_gt <= tolerance_mm])
  surface_dice = (overlap_gt + overlap_pred) / (
      np.sum(surfel_areas_gt) + np.sum(surfel_areas_pred))
  return surface_dice


def compute_dice_coefficient(mask_gt, mask_pred):
  """Computes soerensen-dice coefficient.

  compute the soerensen-dice coefficient between the ground truth mask `mask_gt`
  and the predicted mask `mask_pred`.

  Args:
    mask_gt: 3-dim Numpy array of type bool. The ground truth mask.
    mask_pred: 3-dim Numpy array of type bool. The predicted mask.

  Returns:
    the dice coeffcient as float. If both masks are empty, the result is NaN.
  """
  volume_sum = mask_gt.sum() + mask_pred.sum()
  if volume_sum == 0:
    return np.NaN
  volume_intersect = (mask_gt & mask_pred).sum()
  return 2*volume_intersect / volume_sum

################################################################################
# FILE: utils/train.py
# SIZE: 10364 bytes
################################################################################

import os
import sys
import glob
import time
import random
from dataclasses import dataclass
from typing import Optional, Dict, Any

import numpy as np
import torch
from torch import optim
import matplotlib.pyplot as plt
from natsort import natsorted


# ----------------------------- Logging ----------------------------- #
class Logger:
    """
    Mirrors stdout (and optionally stderr) to both console and a log file in log_dir.
    """
    def __init__(self, log_dir: str, filename: str = "logfile.log"):
        self.terminal = sys.stdout
        os.makedirs(log_dir, exist_ok=True)
        self.log_path = os.path.join(log_dir, filename)
        self.log = open(self.log_path, "a", encoding="utf-8", buffering=1)  # line-buffered

    def write(self, message: str):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()
        self.log.flush()

    def close(self):
        try:
            self.log.close()
        except Exception:
            pass


def attach_stdout_logger(log_dir: str, filename: str = "logfile.log", mirror_stderr: bool = True) -> Logger:
    """
    Redirect sys.stdout to Logger(log_dir). Optionally mirrors sys.stderr as well.
    Returns created logger (keep ref if you want to close it manually).
    """
    logger = Logger(log_dir, filename=filename)
    sys.stdout = logger
    if mirror_stderr:
        sys.stderr = logger
    return logger


# ----------------------------- Device ----------------------------- #
@dataclass(frozen=True)
class DeviceInfo:
    device: torch.device
    gpu_id: int
    gpu_name: Optional[str]


def setup_device(gpu_id: int = 0, seed: int = 0, deterministic: bool = False) -> torch.device:
    """
    Sets seeds + CUDA flags and returns torch.device.
    IMPORTANT: Must return torch.device (NOT DeviceInfo), so callers can do model.to(device).
    """
    # seeds (full set for reproducibility)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        if gpu_id < 0 or gpu_id >= gpu_count:
            raise ValueError(f"gpu_id={gpu_id} is out of range (available: 0..{gpu_count - 1})")

        torch.cuda.set_device(gpu_id)

        if deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            # not all versions support this, keep silent if not
            try:
                torch.use_deterministic_algorithms(True)
            except Exception:
                pass
        else:
            torch.backends.cudnn.benchmark = True

        print(f"Number of GPU: {gpu_count}")
        for i in range(gpu_count):
            print(f"     GPU #{i}: {torch.cuda.get_device_name(i)}")
        name = torch.cuda.get_device_name(gpu_id)
        print(f"Currently using: {name}")
        print("If the GPU is available? True")

        return torch.device(f"cuda:{gpu_id}")

    print("CUDA not available, using CPU.")
    return torch.device("cpu")


def get_device_info(gpu_id: int = 0) -> DeviceInfo:
    """
    Optional helper if you still want metadata in some scripts.
    Not used by training loops that call model.to(device).
    """
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        if gpu_id < 0 or gpu_id >= gpu_count:
            raise ValueError(f"gpu_id={gpu_id} is out of range (available: 0..{gpu_count - 1})")
        name = torch.cuda.get_device_name(gpu_id)
        return DeviceInfo(device=torch.device(f"cuda:{gpu_id}"), gpu_id=gpu_id, gpu_name=name)
    return DeviceInfo(device=torch.device("cpu"), gpu_id=-1, gpu_name=None)


# ----------------------------- Experiment paths ----------------------------- #
@dataclass(frozen=True)
class ExperimentPaths:
    exp_dir: str
    log_dir: str


def make_exp_dirs(exp_name: str) -> ExperimentPaths:
    exp_root = exp_name.rstrip("/\\")
    exp_dir = os.path.join("results", exp_root)
    log_dir = os.path.join("logs", exp_root)
    os.makedirs(exp_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    return ExperimentPaths(exp_dir=exp_dir, log_dir=log_dir)


# ----------------------------- LR schedule ----------------------------- #
def adjust_learning_rate_poly(
    optimizer: optim.Optimizer,
    epoch: int,
    max_epochs: int,
    init_lr: float,
    power: float = 0.9,
) -> float:
    """
    Polynomial LR schedule: lr = init_lr * (1 - epoch/max_epochs)^power
    """
    max_epochs = max(1, int(max_epochs))
    epoch = int(epoch)
    new_lr = float(np.round(init_lr * np.power(1 - (epoch / max_epochs), power), 8))
    for pg in optimizer.param_groups:
        pg["lr"] = new_lr
    return new_lr



def adjust_lr_ctcf_schedule(
    optimizer: optim.Optimizer,
    epoch: int,
    max_epochs: int,
    init_lr: float,
    *,
    power: float = 0.9,
    min_lr: float = 2e-5,
    warm_end_frac: float = 0.15,
) -> float:
    """
    Two-phase schedule:
      1) Hold LR = init_lr until warm is fully ON (epoch < warm_end)
      2) After that: polynomial decay on the remaining epochs
         lr = init_lr * (1 - t)^power, t in [0..1]
    """
    E = int(max_epochs)
    e = int(epoch)
    e1 = int(warm_end_frac * E)

    if e < e1:
        lr = float(init_lr)
    else:
        denom = float(max(1, E - e1))
        t = float(e - e1) / denom
        t = 0.0 if t < 0.0 else (1.0 if t > 1.0 else t)
        lr = float(init_lr * np.power(1.0 - t, power))

    lr = float(max(lr, min_lr))
    for pg in optimizer.param_groups:
        pg["lr"] = lr
    return lr


# ----------------------------- Checkpoints ----------------------------- #
def save_checkpoint(state: Dict[str, Any], save_dir: str, filename: str, max_model_num: int = 8) -> None:
    os.makedirs(save_dir, exist_ok=True)
    ckpt_path = os.path.join(save_dir, filename)
    torch.save(state, ckpt_path)

    # keep at most N newest (by natural sort)
    model_lists = natsorted(glob.glob(os.path.join(save_dir, "*")))
    while len(model_lists) > int(max_model_num):
        os.remove(model_lists[0])
        model_lists = natsorted(glob.glob(os.path.join(save_dir, "*")))


def load_checkpoint_if_exists(
    ckpt_path: str,
    model: torch.nn.Module,
    optimizer: Optional[optim.Optimizer] = None,
    map_location: str | torch.device = "cpu",
) -> Optional[Dict[str, Any]]:
    if not ckpt_path or not os.path.exists(ckpt_path):
        return None

    ckpt = torch.load(ckpt_path, map_location=map_location)

    # accept either full dict or raw state_dict
    state_dict = ckpt["state_dict"] if isinstance(ckpt, dict) and "state_dict" in ckpt else ckpt
    model.load_state_dict(state_dict, strict=True)

    if optimizer is not None and isinstance(ckpt, dict) and "optimizer" in ckpt:
        optimizer.load_state_dict(ckpt["optimizer"])

    return ckpt if isinstance(ckpt, dict) else {"state_dict": state_dict}


# ----------------------------- Perf helpers ----------------------------- #
@dataclass(frozen=True)
class PerfInfo:
    epoch_time_sec: float
    mean_iter_time_ms: float
    peak_gpu_mem_gib: Optional[float]


def perf_epoch_start() -> float:
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
    return time.perf_counter()


def perf_epoch_end(t0: float, iters: int, iter_time_sum: float) -> PerfInfo:
    epoch_time = time.perf_counter() - t0
    mean_iter_ms = (iter_time_sum / max(1, int(iters))) * 1000.0
    peak = None
    if torch.cuda.is_available():
        peak = torch.cuda.max_memory_reserved() / (1024 ** 3)
    return PerfInfo(epoch_time_sec=epoch_time, mean_iter_time_ms=mean_iter_ms, peak_gpu_mem_gib=peak)


# ----------------------------- Visualization helpers ----------------------------- #
def comput_fig(img: torch.Tensor) -> plt.Figure:
    """
    16 axial slices from [B, C, D, H, W], assumes B=1, C=1.
    """
    arr = img.detach().float().cpu().numpy()[0, 0]
    z0 = min(48, max(0, arr.shape[0] - 16))
    arr = arr[z0:z0 + 16]

    fig = plt.figure(figsize=(12, 12), dpi=180)
    for i in range(arr.shape[0]):
        plt.subplot(4, 4, i + 1)
        plt.axis("off")
        plt.imshow(arr[i, :, :], cmap="gray")
    fig.subplots_adjust(wspace=0, hspace=0)
    return fig


def mk_grid_img(flow: torch.Tensor, grid_step: int = 8, line_thickness: int = 1) -> torch.Tensor:
    """
    Create binary 3D grid [1,1,D,H,W] that matches flow spatial shape.
    flow: [B,3,D,H,W] or [B,D,H,W,3] (we only need D,H,W)
    """
    if flow.dim() == 5 and flow.shape[1] in (2, 3):
        d, h, w = map(int, flow.shape[-3:])
        device = flow.device
    elif flow.dim() == 5 and flow.shape[-1] in (2, 3):
        d, h, w = map(int, flow.shape[1:4])
        device = flow.device
    else:
        raise ValueError(f"Unsupported flow shape: {tuple(flow.shape)}")

    grid_step = max(1, int(grid_step))
    line_thickness = max(1, int(line_thickness))

    grid_img = torch.zeros((1, 1, d, h, w), dtype=torch.float32, device=device)

    # lines along H (y)
    for j in range(0, h, grid_step):
        j0 = j
        j1 = min(h, j + line_thickness)
        grid_img[:, :, :, j0:j1, :] = 1.0

    # lines along W (x)
    for i in range(0, w, grid_step):
        i0 = i
        i1 = min(w, i + line_thickness)
        grid_img[:, :, :, :, i0:i1] = 1.0

    return grid_img


def ctcf_schedule(epoch: int, max_epoch: int):
    max_epoch = max(1, int(max_epoch))
    e = int(epoch)

    def clamp01(x):
        return 0.0 if x <= 0.0 else 1.0 if x >= 1.0 else float(x)

    def ramp(p):
        p = clamp01(p)
        return p * p * (3.0 - 2.0 * p)  # smoothstep

    # unified schedule: 5% -> 15%
    s0 = max(1, int(0.05 * max_epoch))
    s1 = max(s0 + 1, int(0.15 * max_epoch))

    if e < s0:
        v = 0.0
    elif e >= s1:
        v = 1.0
    else:
        v = ramp((e - s0) / float(s1 - s0))

    alpha = float(v)
    warm = float(v)

    return alpha, alpha, warm
################################################################################
# FILE: utils/trans.py
# SIZE: 14948 bytes
################################################################################

# import math
import random
from collections.abc import Sequence
import numpy as np
import torch, random, math
from scipy import ndimage

from .rand import Constant
from scipy.ndimage import rotate
from skimage.transform import resize


class Base(object):
    def sample(self, *shape):
        return shape

    def tf(self, img, k=0):
        return img

    def __call__(self, img, dim=3, reuse=False): # class -> func()
        # image: nhwtc
        # shape: no first dims
        if not reuse:
            im = img if isinstance(img, np.ndarray) else img[0]
            shape = im.shape[1:dim+1]
            self.sample(*shape)

        if isinstance(img, Sequence):
            return [self.tf(x, k) for k, x in enumerate(img)] # img:k=0,label:k=1

        return self.tf(img)

    def __str__(self):
        return 'Identity()'


Identity = Base


# gemetric transformations, need a buffers
# first axis is N
class Rot90(Base):
    def __init__(self, axes=(0, 1)):
        self.axes = axes

        for a in self.axes:
            assert a > 0

    def sample(self, *shape):
        shape = list(shape)
        i, j = self.axes

        # shape: no first dim
        i, j = i-1, j-1
        shape[i], shape[j] = shape[j], shape[i]

        return shape

    def tf(self, img, k=0):
        return np.rot90(img, axes=self.axes)

    def __str__(self):
        return 'Rot90(axes=({}, {})'.format(*self.axes)


class RandomRotion(Base):
    def __init__(self,angle_spectrum=10):
        assert isinstance(angle_spectrum,int)
        axes = [(1, 0), (2, 1),(2, 0)]
        self.angle_spectrum = angle_spectrum
        self.axes = axes

    def sample(self,*shape):
        self.axes_buffer = self.axes[np.random.choice(list(range(len(self.axes))))] # choose the random direction
        self.angle_buffer = np.random.randint(-self.angle_spectrum, self.angle_spectrum) # choose the random direction
        return list(shape)

    def tf(self, img, k=0):
        """ Introduction: The rotation function supports the shape [H,W,D,C] or shape [H,W,D]
        :param img: if x, shape is [1,H,W,D,c]; if label, shape is [1,H,W,D]
        :param k: if x, k=0; if label, k=1
        """
        bsize = img.shape[0]

        for bs in range(bsize):
            if k == 0:
                channels = [rotate(img[bs,:,:,:,c], self.angle_buffer, axes=self.axes_buffer, reshape=False, order=0, mode='constant', cval=-1) for c in
                            range(img.shape[4])]
                img[bs,...] = np.stack(channels, axis=-1)

            if k == 1:
                img[bs,...] = rotate(img[bs,...], self.angle_buffer, axes=self.axes_buffer, reshape=False, order=0, mode='constant', cval=-1)

        return img

    def __str__(self):
        return 'RandomRotion(axes={},Angle:{}'.format(self.axes_buffer,self.angle_buffer)


class Flip(Base):
    def __init__(self, axis=0):
        self.axis = axis

    def tf(self, img, k=0):
        return np.flip(img, self.axis)

    def __str__(self):
        return 'Flip(axis={})'.format(self.axis)


class RandomFlip(Base):
    # mirror flip across all x,y,z
    def __init__(self,axis=0):
        # assert axis == (1,2,3) # For both data and label, it has to specify the axis.
        self.axis = (1,2,3)
        self.x_buffer = None
        self.y_buffer = None
        self.z_buffer = None

    def sample(self, *shape):
        self.x_buffer = np.random.choice([True,False])
        self.y_buffer = np.random.choice([True,False])
        self.z_buffer = np.random.choice([True,False])
        return list(shape) # the shape is not changed

    def tf(self,img,k=0): # img shape is (1, 240, 240, 155, 4)
        if self.x_buffer:
            img = np.flip(img,axis=self.axis[0])
        if self.y_buffer:
            img = np.flip(img,axis=self.axis[1])
        if self.z_buffer:
            img = np.flip(img,axis=self.axis[2])
        return img


class RandSelect(Base):
    def __init__(self, prob=0.5, tf=None):
        self.prob = prob
        self.ops  = tf if isinstance(tf, Sequence) else (tf, )
        self.buff = False

    def sample(self, *shape):
        self.buff = random.random() < self.prob

        if self.buff:
            for op in self.ops:
                shape = op.sample(*shape)

        return shape

    def tf(self, img, k=0):
        if self.buff:
            for op in self.ops:
                img = op.tf(img, k)
        return img

    def __str__(self):
        if len(self.ops) == 1:
            ops = str(self.ops[0])
        else:
            ops = '[{}]'.format(', '.join([str(op) for op in self.ops]))
        return 'RandSelect({}, {})'.format(self.prob, ops)


class CenterCrop(Base):
    def __init__(self, size):
        self.size = size
        self.buffer = None

    def sample(self, *shape):
        size = self.size
        start = [(s -size)//2 for s in shape]
        self.buffer = [slice(None)] + [slice(s, s+size) for s in start]
        return [size] * len(shape)

    def tf(self, img, k=0):
        return img[tuple(self.buffer)]

    def __str__(self):
        return 'CenterCrop({})'.format(self.size)


class CenterCropBySize(CenterCrop):
    def sample(self, *shape):
        assert len(self.size) == 3  # random crop [H,W,T] from img [240,240,155]
        if not isinstance(self.size, list):
            size = list(self.size)
        else:
            size = self.size
        start = [(s-i)//2 for i, s in zip(size, shape)]
        self.buffer = [slice(None)] + [slice(s, s+i) for i, s in zip(size, start)]
        return size

    def __str__(self):
        return 'CenterCropBySize({})'.format(self.size)


class RandCrop(CenterCrop):
    def sample(self, *shape):
        size = self.size
        start = [random.randint(0, s-size) for s in shape]
        self.buffer = [slice(None)] + [slice(s, s+size) for s in start]
        return [size]*len(shape)

    def __str__(self):
        return 'RandCrop({})'.format(self.size)


class RandCrop3D(CenterCrop):
    def sample(self, *shape): # shape : [240,240,155]
        assert len(self.size)==3 # random crop [H,W,T] from img [240,240,155]
        if not isinstance(self.size,list):
            size = list(self.size)
        else:
            size = self.size
        start = [random.randint(0, s-i) for i,s in zip(size,shape)]
        self.buffer = [slice(None)] + [slice(s, s+k) for s,k in zip(start,size)]
        return size

    def __str__(self):
        return 'RandCrop({})'.format(self.size)


# for data only
class RandomIntensityChange(Base):
    def __init__(self,factor):
        shift,scale = factor
        assert (shift >0) and (scale >0)
        self.shift = shift
        self.scale = scale

    def tf(self,img,k=0):
        if k==1:
            return img

        shift_factor = np.random.uniform(-self.shift,self.shift,size=[1,img.shape[1],1,1,img.shape[4]]) # [-0.1,+0.1]
        scale_factor = np.random.uniform(1.0 - self.scale, 1.0 + self.scale,size=[1,img.shape[1],1,1,img.shape[4]]) # [0.9,1.1)
        return img * scale_factor + shift_factor

    def __str__(self):
        return 'random intensity shift per channels on the input image, including'


class RandomGammaCorrection(Base):
    def __init__(self,factor):
        lower, upper = factor
        assert (lower >0) and (upper >0)
        self.lower = lower
        self.upper = upper

    def tf(self,img,k=0):
        if k==1:
            return img
        img = img + np.min(img)
        img_max = np.max(img)
        img = img/img_max
        factor = random.choice(np.arange(self.lower, self.upper, 0.1))
        gamma = random.choice([1, factor])
        if gamma == 1:
            return img
        img = img ** gamma * img_max
        img = (img - img.mean())/img.std()
        return img

    def __str__(self):
        return 'random intensity shift per channels on the input image, including'


class MinMax_norm(Base):
    def __init__(self, ):
        a = None

    def tf(self, img, k=0):
        if k == 1:
            return img
        img = (img - img.min()) / (img.max()-img.min())
        return img


class Seg_norm(Base):
    def __init__(self, ):
        a = None
        self.seg_table = np.array([0, 2, 3, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 24, 26,
                          28, 30, 31, 41, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 58, 60, 62,
                          63, 72, 77, 80, 85, 251, 252, 253, 254, 255])
    def tf(self, img, k=0):
        if k == 0:
            return img
        img_out = np.zeros_like(img)
        for i in range(len(self.seg_table)):
            img_out[img == self.seg_table[i]] = i
        return img_out


class Resize_img(Base):
    def __init__(self, shape):
        self.shape = shape

    def tf(self, img, k=0):
        if k == 1:
            img = resize(img, (img.shape[0], self.shape[0], self.shape[1], self.shape[2]),
                         anti_aliasing=False, order=0)
        else:
            img = resize(img, (img.shape[0], self.shape[0], self.shape[1], self.shape[2]),
                         anti_aliasing=False, order=3)
        return img


class Pad(Base):
    def __init__(self, pad): # [0,0,0,5,0]
        self.pad = pad
        self.px = tuple(zip([0]*len(pad), pad))

    def sample(self, *shape):

        shape = list(shape)

        # shape: no first dim
        for i in range(len(shape)):
            shape[i] += self.pad[i+1]

        return shape

    def tf(self, img, k=0):
        #nhwtc, nhwt
        dim = len(img.shape)
        return np.pad(img, self.px[:dim], mode='constant')

    def __str__(self):
        return 'Pad(({}, {}, {}))'.format(*self.pad)


class Pad3DIfNeeded(Base):
    def __init__(self, shape, value=0, mask_value=0): # [0,0,0,5,0]
        self.shape = shape
        self.value = value
        self.mask_value = mask_value

    def tf(self, img, k=0):
        pad = [(0,0)]
        if k==0:
            img_shape = img.shape[1:-1]
        else:
            img_shape = img.shape[1:]
        for i, t in zip(img_shape, self.shape):
            if i < t:
                diff = t-i
                pad.append((math.ceil(diff/2),math.floor(diff/2)))
            else:
                pad.append((0,0))
        if k == 0:
            pad.append((0,0))
        pad = tuple(pad)
        if k==0:
            return np.pad(img, pad, mode='constant', constant_values=img.min())
        else:
            return np.pad(img, pad, mode='constant', constant_values=self.mask_value)

    def __str__(self):
        return 'Pad(({}, {}, {}))'.format(*self.pad)


class Noise(Base):
    def __init__(self, dim, sigma=0.1, channel=True, num=-1):
        self.dim = dim
        self.sigma = sigma
        self.channel = channel
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img

        if self.channel:
            #nhwtc, hwtc, hwt
            shape = [1] if len(img.shape) < self.dim+2 else [img.shape[-1]]
        else:
            shape = img.shape
        return img * np.exp(self.sigma * torch.randn(shape, dtype=torch.float32).numpy())

    def __str__(self):
        return 'Noise()'


# dim could come from shape
class GaussianBlur(Base):
    def __init__(self, dim, sigma=Constant(1.5), app=-1):
        # 1.5 pixel
        self.dim = dim
        self.sigma = sigma
        self.eps   = 0.001
        self.app = app

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img

        # image is nhwtc
        for n in range(img.shape[0]):
            sig = self.sigma.sample()
            # sample each channel saperately to avoid correlations
            if sig > self.eps:
                if len(img.shape) == self.dim+2:
                    C = img.shape[-1]
                    for c in range(C):
                        img[n,..., c] = ndimage.gaussian_filter(img[n, ..., c], sig)
                elif len(img.shape) == self.dim+1:
                    img[n] = ndimage.gaussian_filter(img[n], sig)
                else:
                    raise ValueError('image shape is not supported')

        return img

    def __str__(self):
        return 'GaussianBlur()'


class ToNumpy(Base):
    def __init__(self, num=-1):
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        return img.numpy()

    def __str__(self):
        return 'ToNumpy()'


class ToTensor(Base):
    def __init__(self, num=-1):
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img

        return torch.from_numpy(img)

    def __str__(self):
        return 'ToTensor'


class TensorType(Base):
    def __init__(self, types, num=-1):
        self.types = types # ('torch.float32', 'torch.int64')
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        # make this work with both Tensor and Numpy
        return img.type(self.types[k])

    def __str__(self):
        s = ', '.join([str(s) for s in self.types])
        return 'TensorType(({}))'.format(s)


class NumpyType(Base):
    def __init__(self, types, num=-1):
        self.types = types # ('float32', 'int64')
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        # make this work with both Tensor and Numpy
        return img.astype(self.types[k])

    def __str__(self):
        s = ', '.join([str(s) for s in self.types])
        return 'NumpyType(({}))'.format(s)


class Normalize(Base):
    def __init__(self, mean=0.0, std=1.0, num=-1):
        self.mean = mean
        self.std = std
        self.num = num

    def tf(self, img, k=0):
        if self.num > 0 and k >= self.num:
            return img
        img -= self.mean
        img /= self.std
        return img

    def __str__(self):
        return 'Normalize()'


class Compose(Base):
    def __init__(self, ops):
        if not isinstance(ops, Sequence):
            ops = ops,
        self.ops = ops

    def sample(self, *shape):
        for op in self.ops:
            shape = op.sample(*shape)

    def tf(self, img, k=0):

        for op in self.ops:
            img = op.tf(img, k) # do not use op(img) here

        return img

    def __str__(self):
        ops = ', '.join([str(op) for op in self.ops])
        return 'Compose([{}])'.format(ops)
################################################################################
# FILE: utils/validation.py
# SIZE: 2660 bytes
################################################################################

from __future__ import annotations
from dataclasses import dataclass
from typing import Callable, Dict, Optional, Any
import torch

from utils import AverageMeter, jacobian_det


@dataclass
class ValResult:
    dsc: float
    fold_percent: float
    last_vis: Dict[str, Any]


@torch.no_grad()
def validate(
    *,
    model: torch.nn.Module,
    val_loader,
    device: torch.device,
    forward_flow_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
    dice_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
    register_model_cls,
    mk_grid_img_fn: Optional[Callable[..., torch.Tensor]] = None,
    grid_step: int = 8,
    line_thickness: int = 1,
) -> ValResult:
    """
    Universal OASIS validation:
      - computes DSC on VOI labels using warped segmentation (nearest)
      - computes folding percent (detJ <= 0)
      - optionally returns a deformed grid for visualization

    Contract:
      forward_flow_fn(x, y) -> flow_full of shape [B,3,D,H,W] on `device`
    """
    model.eval()

    reg_nearest = None
    reg_bilin = None

    dsc_meter = AverageMeter()
    fold_meter = AverageMeter()

    last_vis: Dict[str, Any] = {}

    for batch in val_loader:
        # expected: x, y, x_seg, y_seg
        x, y, x_seg, y_seg = [t.to(device, non_blocking=True) for t in batch]
        vol_shape = tuple(x.shape[2:])  # (D,H,W) from tensor

        if reg_nearest is None:
            reg_nearest = register_model_cls(vol_shape, mode="nearest").to(device)
            reg_bilin = register_model_cls(vol_shape, mode="bilinear").to(device)

        flow = forward_flow_fn(x, y)  # [B,3,D,H,W]

        # warp seg (nearest)
        def_seg = reg_nearest((x_seg.float(), flow.float()))
        dsc = dice_fn(def_seg.long(), y_seg.long())
        dsc_meter.update(float(dsc), x.size(0))

        # fold %
        detJ = jacobian_det(flow.float())  # [B,1,D,H,W]
        fold = (detJ <= 0.0).float().mean() * 100.0
        fold_meter.update(float(fold), x.size(0))

        # optional grid for visuals (only keep last batch)
        def_grid = None
        if mk_grid_img_fn is not None:
            grid_img = mk_grid_img_fn(flow, grid_step=grid_step, line_thickness=line_thickness)
            def_grid = reg_bilin((grid_img.float(), flow.float()))

        last_vis = {
            "x_seg": x_seg,
            "y_seg": y_seg,
            "def_seg": def_seg,
            "def_grid": def_grid,
            "flow": flow,
        }

    return ValResult(
        dsc=dsc_meter.avg,
        fold_percent=fold_meter.avg,
        last_vis=last_vis,
    )
